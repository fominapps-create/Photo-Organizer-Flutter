
pytorch	2.9.1+cpu:Ö¬5
È
image
/encoder.model.patch_embed.0.reparam_conv.weight
-encoder.model.patch_embed.0.reparam_conv.biasconv2dnode_conv2d"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†JÛ
	namespaceÂ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.0: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.0.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d: aten.conv2d.defaultJö
pkg.torch.onnx.class_hierarchy˜['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J¯
pkg.torch.onnx.fx_node›%conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_encoder_model_patch_embed_0_reparam_conv_weight, %p_encoder_model_patch_embed_0_reparam_conv_bias, [2, 2], [1, 1]), kwargs = {})J±
pkg.torch.onnx.name_scopesí['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.0', 'encoder.model.patch_embed.0.reparam_conv', 'conv2d']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
∑
conv2d
val_0val_1
node_Div_1"DivJÒ
	namespace„: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.0: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.0.activation: torch.nn.modules.activation.GELU/gelu: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÉ
pkg.torch.onnx.fx_nodei%gelu : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d,), kwargs = {})J≠
pkg.torch.onnx.name_scopesé['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.0', 'encoder.model.patch_embed.0.activation', 'gelu']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Ø
val_1val_2
node_Erf_2"ErfJÒ
	namespace„: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.0: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.0.activation: torch.nn.modules.activation.GELU/gelu: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÉ
pkg.torch.onnx.fx_nodei%gelu : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d,), kwargs = {})J≠
pkg.torch.onnx.name_scopesé['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.0', 'encoder.model.patch_embed.0.activation', 'gelu']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
∂
val_2
val_3val_4
node_Add_4"AddJÒ
	namespace„: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.0: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.0.activation: torch.nn.modules.activation.GELU/gelu: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÉ
pkg.torch.onnx.fx_nodei%gelu : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d,), kwargs = {})J≠
pkg.torch.onnx.name_scopesé['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.0', 'encoder.model.patch_embed.0.activation', 'gelu']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
∂
val_5
val_4val_6
node_Mul_6"MulJÒ
	namespace„: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.0: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.0.activation: torch.nn.modules.activation.GELU/gelu: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÉ
pkg.torch.onnx.fx_nodei%gelu : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d,), kwargs = {})J≠
pkg.torch.onnx.name_scopesé['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.0', 'encoder.model.patch_embed.0.activation', 'gelu']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
µ
conv2d
val_6gelu	node_gelu"MulJÒ
	namespace„: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.0: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.0.activation: torch.nn.modules.activation.GELU/gelu: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÉ
pkg.torch.onnx.fx_nodei%gelu : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d,), kwargs = {})J≠
pkg.torch.onnx.name_scopesé['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.0', 'encoder.model.patch_embed.0.activation', 'gelu']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Å
gelu
/encoder.model.patch_embed.1.reparam_conv.weight
-encoder.model.patch_embed.1.reparam_conv.biasconv2d_1node_conv2d_1"Conv*
group@†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jı
	namespaceÁ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.1.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_1: aten.conv2d.defaultJö
pkg.torch.onnx.class_hierarchy˜['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']Jâ
pkg.torch.onnx.fx_nodeÓ%conv2d_1 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%gelu, %p_encoder_model_patch_embed_1_reparam_conv_weight, %p_encoder_model_patch_embed_1_reparam_conv_bias, [2, 2], [1, 1], [1, 1], 64), kwargs = {})J≥
pkg.torch.onnx.name_scopesî['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.1', 'encoder.model.patch_embed.1.reparam_conv', 'conv2d_1']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
¡
conv2d_1
val_0val_8
node_Div_8"DivJÛ
	namespaceÂ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.1.activation: torch.nn.modules.activation.GELU/gelu_1: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_1 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_1,), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.1', 'encoder.model.patch_embed.1.activation', 'gelu_1']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
∑
val_8val_9
node_Erf_9"ErfJÛ
	namespaceÂ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.1.activation: torch.nn.modules.activation.GELU/gelu_1: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_1 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_1,), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.1', 'encoder.model.patch_embed.1.activation', 'gelu_1']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
¿
val_9
val_3val_11node_Add_11"AddJÛ
	namespaceÂ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.1.activation: torch.nn.modules.activation.GELU/gelu_1: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_1 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_1,), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.1', 'encoder.model.patch_embed.1.activation', 'gelu_1']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
¡
val_5
val_11val_13node_Mul_13"MulJÛ
	namespaceÂ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.1.activation: torch.nn.modules.activation.GELU/gelu_1: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_1 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_1,), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.1', 'encoder.model.patch_embed.1.activation', 'gelu_1']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ƒ
conv2d_1
val_13gelu_1node_gelu_1"MulJÛ
	namespaceÂ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.1.activation: torch.nn.modules.activation.GELU/gelu_1: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_1 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_1,), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.1', 'encoder.model.patch_embed.1.activation', 'gelu_1']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
È
gelu_1
/encoder.model.patch_embed.2.reparam_conv.weight
-encoder.model.patch_embed.2.reparam_conv.biasconv2d_2node_conv2d_2"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jı
	namespaceÁ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.2: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.2.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_2: aten.conv2d.defaultJö
pkg.torch.onnx.class_hierarchy˜['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÔ
pkg.torch.onnx.fx_node‘%conv2d_2 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%gelu_1, %p_encoder_model_patch_embed_2_reparam_conv_weight, %p_encoder_model_patch_embed_2_reparam_conv_bias), kwargs = {})J≥
pkg.torch.onnx.name_scopesî['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.2', 'encoder.model.patch_embed.2.reparam_conv', 'conv2d_2']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
√
conv2d_2
val_0val_15node_Div_15"DivJÛ
	namespaceÂ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.2: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.2.activation: torch.nn.modules.activation.GELU/gelu_2: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_2 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_2,), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.2', 'encoder.model.patch_embed.2.activation', 'gelu_2']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
∫
val_15val_16node_Erf_16"ErfJÛ
	namespaceÂ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.2: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.2.activation: torch.nn.modules.activation.GELU/gelu_2: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_2 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_2,), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.2', 'encoder.model.patch_embed.2.activation', 'gelu_2']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
¡
val_16
val_3val_18node_Add_18"AddJÛ
	namespaceÂ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.2: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.2.activation: torch.nn.modules.activation.GELU/gelu_2: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_2 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_2,), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.2', 'encoder.model.patch_embed.2.activation', 'gelu_2']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
¡
val_5
val_18val_20node_Mul_20"MulJÛ
	namespaceÂ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.2: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.2.activation: torch.nn.modules.activation.GELU/gelu_2: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_2 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_2,), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.2', 'encoder.model.patch_embed.2.activation', 'gelu_2']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ƒ
conv2d_2
val_20gelu_2node_gelu_2"MulJÛ
	namespaceÂ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.patch_embed: torch.nn.modules.container.Sequential/encoder.model.patch_embed.2: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.patch_embed.2.activation: torch.nn.modules.activation.GELU/gelu_2: aten.gelu.defaultJú
pkg.torch.onnx.class_hierarchy˘['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_2 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_2,), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.patch_embed', 'encoder.model.patch_embed.2', 'encoder.model.patch_embed.2.activation', 'gelu_2']Jƒ
pkg.torch.onnx.stack_trace•File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 849, in forward
    x = self.forward_embeddings(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
é
gelu_2
9encoder.model.network.0.0.token_mixer.reparam_conv.weight
7encoder.model.network.0.0.token_mixer.reparam_conv.biasconv2d_3node_conv2d_3"Conv*
group@†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≤
	namespace§: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.0.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.0.0.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_3: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']Jü
pkg.torch.onnx.fx_nodeÑ%conv2d_3 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%gelu_2, %p_encoder_model_network_0_0_token_mixer_reparam_conv_weight, %p_encoder_model_network_0_0_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 64), kwargs = {})J‚
pkg.torch.onnx.name_scopes√['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.0', 'encoder.model.network.0.0.token_mixer', 'encoder.model.network.0.0.token_mixer.reparam_conv', 'conv2d_3']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
å
conv2d_3
2encoder.model.network.0.0.convffn.conv.conv.weight
7encoder.model.network.0.0.convffn.conv.conv.weight_biasgetitemnode_Conv_557"Conv*
group@†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∂
	namespace®: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.0.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.0.0.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']JÆ
pkg.torch.onnx.fx_nodeì%_native_batch_norm_legit_no_training : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_4, %p_encoder_model_network_0_0_convffn_conv_bn_weight, %p_encoder_model_network_0_0_convffn_conv_bn_bias, %b_encoder_model_network_0_0_convffn_conv_bn_running_mean, %b_encoder_model_network_0_0_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jõ
pkg.torch.onnx.name_scopes¸['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.0', 'encoder.model.network.0.0.convffn', 'encoder.model.network.0.0.convffn.conv', 'encoder.model.network.0.0.convffn.conv.bn', '_native_batch_norm_legit_no_training']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
∂
getitem
,encoder.model.network.0.0.convffn.fc1.weight
*encoder.model.network.0.0.convffn.fc1.biasconv2d_5node_conv2d_5"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.0.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_5: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÍ
pkg.torch.onnx.fx_nodeœ%conv2d_5 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem, %p_encoder_model_network_0_0_convffn_fc1_weight, %p_encoder_model_network_0_0_convffn_fc1_bias), kwargs = {})J—
pkg.torch.onnx.name_scopes≤['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.0', 'encoder.model.network.0.0.convffn', 'encoder.model.network.0.0.convffn.fc1', 'conv2d_5']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
û
conv2d_5
val_0val_34node_Div_34"DivJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.0.convffn.act: torch.nn.modules.activation.GELU/gelu_3: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_3 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_5,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.0', 'encoder.model.network.0.0.convffn', 'encoder.model.network.0.0.convffn.act', 'gelu_3']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ï
val_34val_35node_Erf_35"ErfJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.0.convffn.act: torch.nn.modules.activation.GELU/gelu_3: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_3 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_5,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.0', 'encoder.model.network.0.0.convffn', 'encoder.model.network.0.0.convffn.act', 'gelu_3']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_35
val_3val_37node_Add_37"AddJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.0.convffn.act: torch.nn.modules.activation.GELU/gelu_3: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_3 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_5,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.0', 'encoder.model.network.0.0.convffn', 'encoder.model.network.0.0.convffn.act', 'gelu_3']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_5
val_37val_39node_Mul_39"MulJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.0.convffn.act: torch.nn.modules.activation.GELU/gelu_3: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_3 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_5,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.0', 'encoder.model.network.0.0.convffn', 'encoder.model.network.0.0.convffn.act', 'gelu_3']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ü
conv2d_5
val_39gelu_3node_gelu_3"MulJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.0.convffn.act: torch.nn.modules.activation.GELU/gelu_3: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_3 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_5,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.0', 'encoder.model.network.0.0.convffn', 'encoder.model.network.0.0.convffn.act', 'gelu_3']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
≥
gelu_3
,encoder.model.network.0.0.convffn.fc2.weight
*encoder.model.network.0.0.convffn.fc2.biasconv2d_6node_conv2d_6"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.0.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_6: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JË
pkg.torch.onnx.fx_nodeÕ%conv2d_6 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone, %p_encoder_model_network_0_0_convffn_fc2_weight, %p_encoder_model_network_0_0_convffn_fc2_bias), kwargs = {})J—
pkg.torch.onnx.name_scopes≤['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.0', 'encoder.model.network.0.0.convffn', 'encoder.model.network.0.0.convffn.fc2', 'conv2d_6']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
˘
%encoder.model.network.0.0.layer_scale
conv2d_6mulnode_mul"MulJí
	namespaceÑ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.0: mobileclip.models.mci.RepMixerBlock/mul: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']J´
pkg.torch.onnx.fx_nodeê%mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_0_0_layer_scale, %clone_1), kwargs = {})J}
pkg.torch.onnx.name_scopes_['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.0', 'mul']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
≥
conv2d_3
muladdnode_add"AddJí
	namespaceÑ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.0: mobileclip.models.mci.RepMixerBlock/add: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Já
pkg.torch.onnx.fx_nodem%add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_3, %mul), kwargs = {})J}
pkg.torch.onnx.name_scopes_['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.0', 'add']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
à
add
9encoder.model.network.0.1.token_mixer.reparam_conv.weight
7encoder.model.network.0.1.token_mixer.reparam_conv.biasconv2d_7node_conv2d_7"Conv*
group@†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≤
	namespace§: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.1.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.0.1.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_7: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']Jú
pkg.torch.onnx.fx_nodeÅ%conv2d_7 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add, %p_encoder_model_network_0_1_token_mixer_reparam_conv_weight, %p_encoder_model_network_0_1_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 64), kwargs = {})J‚
pkg.torch.onnx.name_scopes√['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.1', 'encoder.model.network.0.1.token_mixer', 'encoder.model.network.0.1.token_mixer.reparam_conv', 'conv2d_7']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
î
conv2d_7
2encoder.model.network.0.1.convffn.conv.conv.weight
7encoder.model.network.0.1.convffn.conv.conv.weight_bias	getitem_3node_Conv_559"Conv*
group@†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∏
	namespace™: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.1.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.0.1.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_1: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J∞
pkg.torch.onnx.fx_nodeï%_native_batch_norm_legit_no_training_1 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_8, %p_encoder_model_network_0_1_convffn_conv_bn_weight, %p_encoder_model_network_0_1_convffn_conv_bn_bias, %b_encoder_model_network_0_1_convffn_conv_bn_running_mean, %b_encoder_model_network_0_1_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jù
pkg.torch.onnx.name_scopes˛['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.1', 'encoder.model.network.0.1.convffn', 'encoder.model.network.0.1.convffn.conv', 'encoder.model.network.0.1.convffn.conv.bn', '_native_batch_norm_legit_no_training_1']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
∫
	getitem_3
,encoder.model.network.0.1.convffn.fc1.weight
*encoder.model.network.0.1.convffn.fc1.biasconv2d_9node_conv2d_9"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.1.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_9: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_9 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_3, %p_encoder_model_network_0_1_convffn_fc1_weight, %p_encoder_model_network_0_1_convffn_fc1_bias), kwargs = {})J—
pkg.torch.onnx.name_scopes≤['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.1', 'encoder.model.network.0.1.convffn', 'encoder.model.network.0.1.convffn.fc1', 'conv2d_9']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
û
conv2d_9
val_0val_50node_Div_50"DivJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.1.convffn.act: torch.nn.modules.activation.GELU/gelu_4: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_4 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_9,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.1', 'encoder.model.network.0.1.convffn', 'encoder.model.network.0.1.convffn.act', 'gelu_4']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ï
val_50val_51node_Erf_51"ErfJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.1.convffn.act: torch.nn.modules.activation.GELU/gelu_4: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_4 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_9,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.1', 'encoder.model.network.0.1.convffn', 'encoder.model.network.0.1.convffn.act', 'gelu_4']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_51
val_3val_53node_Add_53"AddJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.1.convffn.act: torch.nn.modules.activation.GELU/gelu_4: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_4 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_9,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.1', 'encoder.model.network.0.1.convffn', 'encoder.model.network.0.1.convffn.act', 'gelu_4']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_5
val_53val_55node_Mul_55"MulJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.1.convffn.act: torch.nn.modules.activation.GELU/gelu_4: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_4 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_9,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.1', 'encoder.model.network.0.1.convffn', 'encoder.model.network.0.1.convffn.act', 'gelu_4']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ü
conv2d_9
val_55gelu_4node_gelu_4"MulJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.1.convffn.act: torch.nn.modules.activation.GELU/gelu_4: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Já
pkg.torch.onnx.fx_nodem%gelu_4 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_9,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.1', 'encoder.model.network.0.1.convffn', 'encoder.model.network.0.1.convffn.act', 'gelu_4']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
∫
gelu_4
,encoder.model.network.0.1.convffn.fc2.weight
*encoder.model.network.0.1.convffn.fc2.bias	conv2d_10node_conv2d_10"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.0.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.0.1.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_10: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÎ
pkg.torch.onnx.fx_node–%conv2d_10 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_2, %p_encoder_model_network_0_1_convffn_fc2_weight, %p_encoder_model_network_0_1_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.1', 'encoder.model.network.0.1.convffn', 'encoder.model.network.0.1.convffn.fc2', 'conv2d_10']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
Ñ
%encoder.model.network.0.1.layer_scale
	conv2d_10mul_1
node_mul_1"MulJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.1: mobileclip.models.mci.RepMixerBlock/mul_1: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']J≠
pkg.torch.onnx.fx_nodeí%mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_0_1_layer_scale, %clone_3), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.1', 'mul_1']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
¡
conv2d_7
mul_1add_1
node_add_1"AddJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.0: torch.nn.modules.container.Sequential/encoder.model.network.0.1: mobileclip.models.mci.RepMixerBlock/add_1: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jã
pkg.torch.onnx.fx_nodeq%add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_7, %mul_1), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.0', 'encoder.model.network.0.1', 'add_1']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
å
add_1
1encoder.model.network.1.proj.0.lkb_reparam.weight
/encoder.model.network.1.proj.0.lkb_reparam.bias	conv2d_11node_conv2d_11"Conv*
group@†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jæ
	namespace∞: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.1: mobileclip.models.mci.PatchEmbed/encoder.model.network.1.proj: torch.nn.modules.container.Sequential/encoder.model.network.1.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.1.proj.0.lkb_reparam: torch.nn.modules.conv.Conv2d/conv2d_11: aten.conv2d.defaultJƒ
pkg.torch.onnx.class_hierarchy°['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']Jè
pkg.torch.onnx.fx_nodeÙ%conv2d_11 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_1, %p_encoder_model_network_1_proj_0_lkb_reparam_weight, %p_encoder_model_network_1_proj_0_lkb_reparam_bias, [2, 2], [3, 3], [1, 1], 64), kwargs = {})J◊
pkg.torch.onnx.name_scopes∏['', 'encoder', 'encoder.model', 'encoder.model.network.1', 'encoder.model.network.1.proj', 'encoder.model.network.1.proj.0', 'encoder.model.network.1.proj.0.lkb_reparam', 'conv2d_11']J´
pkg.torch.onnx.stack_traceåFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 92, in forward
    out = self.lkb_reparam(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
Œ
	conv2d_11
val_0val_57node_Div_57"DivJº
	namespaceÆ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.1: mobileclip.models.mci.PatchEmbed/encoder.model.network.1.proj: torch.nn.modules.container.Sequential/encoder.model.network.1.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.1.proj.0.activation: torch.nn.modules.activation.GELU/gelu_5: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_5 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_11,), kwargs = {})J”
pkg.torch.onnx.name_scopes¥['', 'encoder', 'encoder.model', 'encoder.model.network.1', 'encoder.model.network.1.proj', 'encoder.model.network.1.proj.0', 'encoder.model.network.1.proj.0.activation', 'gelu_5']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ƒ
val_57val_58node_Erf_58"ErfJº
	namespaceÆ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.1: mobileclip.models.mci.PatchEmbed/encoder.model.network.1.proj: torch.nn.modules.container.Sequential/encoder.model.network.1.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.1.proj.0.activation: torch.nn.modules.activation.GELU/gelu_5: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_5 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_11,), kwargs = {})J”
pkg.torch.onnx.name_scopes¥['', 'encoder', 'encoder.model', 'encoder.model.network.1', 'encoder.model.network.1.proj', 'encoder.model.network.1.proj.0', 'encoder.model.network.1.proj.0.activation', 'gelu_5']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
À
val_58
val_3val_60node_Add_60"AddJº
	namespaceÆ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.1: mobileclip.models.mci.PatchEmbed/encoder.model.network.1.proj: torch.nn.modules.container.Sequential/encoder.model.network.1.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.1.proj.0.activation: torch.nn.modules.activation.GELU/gelu_5: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_5 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_11,), kwargs = {})J”
pkg.torch.onnx.name_scopes¥['', 'encoder', 'encoder.model', 'encoder.model.network.1', 'encoder.model.network.1.proj', 'encoder.model.network.1.proj.0', 'encoder.model.network.1.proj.0.activation', 'gelu_5']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
À
val_5
val_60val_62node_Mul_62"MulJº
	namespaceÆ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.1: mobileclip.models.mci.PatchEmbed/encoder.model.network.1.proj: torch.nn.modules.container.Sequential/encoder.model.network.1.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.1.proj.0.activation: torch.nn.modules.activation.GELU/gelu_5: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_5 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_11,), kwargs = {})J”
pkg.torch.onnx.name_scopes¥['', 'encoder', 'encoder.model', 'encoder.model.network.1', 'encoder.model.network.1.proj', 'encoder.model.network.1.proj.0', 'encoder.model.network.1.proj.0.activation', 'gelu_5']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
œ
	conv2d_11
val_62gelu_5node_gelu_5"MulJº
	namespaceÆ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.1: mobileclip.models.mci.PatchEmbed/encoder.model.network.1.proj: torch.nn.modules.container.Sequential/encoder.model.network.1.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.1.proj.0.activation: torch.nn.modules.activation.GELU/gelu_5: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_5 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_11,), kwargs = {})J”
pkg.torch.onnx.name_scopes¥['', 'encoder', 'encoder.model', 'encoder.model.network.1', 'encoder.model.network.1.proj', 'encoder.model.network.1.proj.0', 'encoder.model.network.1.proj.0.activation', 'gelu_5']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ã
gelu_5
2encoder.model.network.1.proj.1.reparam_conv.weight
0encoder.model.network.1.proj.1.reparam_conv.bias	conv2d_12node_conv2d_12"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jπ
	namespace´: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.1: mobileclip.models.mci.PatchEmbed/encoder.model.network.1.proj: torch.nn.modules.container.Sequential/encoder.model.network.1.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.1.proj.1.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_12: aten.conv2d.defaultJæ
pkg.torch.onnx.class_hierarchyõ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']Jˆ
pkg.torch.onnx.fx_node€%conv2d_12 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%gelu_5, %p_encoder_model_network_1_proj_1_reparam_conv_weight, %p_encoder_model_network_1_proj_1_reparam_conv_bias), kwargs = {})Jÿ
pkg.torch.onnx.name_scopesπ['', 'encoder', 'encoder.model', 'encoder.model.network.1', 'encoder.model.network.1.proj', 'encoder.model.network.1.proj.1', 'encoder.model.network.1.proj.1.reparam_conv', 'conv2d_12']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
÷
	conv2d_12
val_0val_64node_Div_64"DivJ∂
	namespace®: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.1: mobileclip.models.mci.PatchEmbed/encoder.model.network.1.proj: torch.nn.modules.container.Sequential/encoder.model.network.1.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.1.proj.1.activation: torch.nn.modules.activation.GELU/gelu_6: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_6 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_12,), kwargs = {})J”
pkg.torch.onnx.name_scopes¥['', 'encoder', 'encoder.model', 'encoder.model.network.1', 'encoder.model.network.1.proj', 'encoder.model.network.1.proj.1', 'encoder.model.network.1.proj.1.activation', 'gelu_6']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Ã
val_64val_65node_Erf_65"ErfJ∂
	namespace®: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.1: mobileclip.models.mci.PatchEmbed/encoder.model.network.1.proj: torch.nn.modules.container.Sequential/encoder.model.network.1.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.1.proj.1.activation: torch.nn.modules.activation.GELU/gelu_6: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_6 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_12,), kwargs = {})J”
pkg.torch.onnx.name_scopes¥['', 'encoder', 'encoder.model', 'encoder.model.network.1', 'encoder.model.network.1.proj', 'encoder.model.network.1.proj.1', 'encoder.model.network.1.proj.1.activation', 'gelu_6']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
”
val_65
val_3val_67node_Add_67"AddJ∂
	namespace®: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.1: mobileclip.models.mci.PatchEmbed/encoder.model.network.1.proj: torch.nn.modules.container.Sequential/encoder.model.network.1.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.1.proj.1.activation: torch.nn.modules.activation.GELU/gelu_6: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_6 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_12,), kwargs = {})J”
pkg.torch.onnx.name_scopes¥['', 'encoder', 'encoder.model', 'encoder.model.network.1', 'encoder.model.network.1.proj', 'encoder.model.network.1.proj.1', 'encoder.model.network.1.proj.1.activation', 'gelu_6']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
”
val_5
val_67val_69node_Mul_69"MulJ∂
	namespace®: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.1: mobileclip.models.mci.PatchEmbed/encoder.model.network.1.proj: torch.nn.modules.container.Sequential/encoder.model.network.1.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.1.proj.1.activation: torch.nn.modules.activation.GELU/gelu_6: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_6 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_12,), kwargs = {})J”
pkg.torch.onnx.name_scopes¥['', 'encoder', 'encoder.model', 'encoder.model.network.1', 'encoder.model.network.1.proj', 'encoder.model.network.1.proj.1', 'encoder.model.network.1.proj.1.activation', 'gelu_6']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
◊
	conv2d_12
val_69gelu_6node_gelu_6"MulJ∂
	namespace®: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.1: mobileclip.models.mci.PatchEmbed/encoder.model.network.1.proj: torch.nn.modules.container.Sequential/encoder.model.network.1.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.1.proj.1.activation: torch.nn.modules.activation.GELU/gelu_6: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_6 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_12,), kwargs = {})J”
pkg.torch.onnx.name_scopes¥['', 'encoder', 'encoder.model', 'encoder.model.network.1', 'encoder.model.network.1.proj', 'encoder.model.network.1.proj.1', 'encoder.model.network.1.proj.1.activation', 'gelu_6']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ï
gelu_6
9encoder.model.network.2.0.token_mixer.reparam_conv.weight
7encoder.model.network.2.0.token_mixer.reparam_conv.bias	conv2d_13node_conv2d_13"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.0.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.2.0.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_13: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J°
pkg.torch.onnx.fx_nodeÜ%conv2d_13 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%gelu_6, %p_encoder_model_network_2_0_token_mixer_reparam_conv_weight, %p_encoder_model_network_2_0_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 128), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.0', 'encoder.model.network.2.0.token_mixer', 'encoder.model.network.2.0.token_mixer.reparam_conv', 'conv2d_13']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ó
	conv2d_13
2encoder.model.network.2.0.convffn.conv.conv.weight
7encoder.model.network.2.0.convffn.conv.conv.weight_bias	getitem_6node_Conv_561"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∏
	namespace™: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.0.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.2.0.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_2: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J±
pkg.torch.onnx.fx_nodeñ%_native_batch_norm_legit_no_training_2 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_14, %p_encoder_model_network_2_0_convffn_conv_bn_weight, %p_encoder_model_network_2_0_convffn_conv_bn_bias, %b_encoder_model_network_2_0_convffn_conv_bn_running_mean, %b_encoder_model_network_2_0_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jù
pkg.torch.onnx.name_scopes˛['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.0', 'encoder.model.network.2.0.convffn', 'encoder.model.network.2.0.convffn.conv', 'encoder.model.network.2.0.convffn.conv.bn', '_native_batch_norm_legit_no_training_2']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
ø
	getitem_6
,encoder.model.network.2.0.convffn.fc1.weight
*encoder.model.network.2.0.convffn.fc1.bias	conv2d_15node_conv2d_15"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.0.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_15: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÌ
pkg.torch.onnx.fx_node“%conv2d_15 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_6, %p_encoder_model_network_2_0_convffn_fc1_weight, %p_encoder_model_network_2_0_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.0', 'encoder.model.network.2.0.convffn', 'encoder.model.network.2.0.convffn.fc1', 'conv2d_15']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
†
	conv2d_15
val_0val_80node_Div_80"DivJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.0.convffn.act: torch.nn.modules.activation.GELU/gelu_7: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_7 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_15,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.0', 'encoder.model.network.2.0.convffn', 'encoder.model.network.2.0.convffn.act', 'gelu_7']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ñ
val_80val_81node_Erf_81"ErfJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.0.convffn.act: torch.nn.modules.activation.GELU/gelu_7: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_7 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_15,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.0', 'encoder.model.network.2.0.convffn', 'encoder.model.network.2.0.convffn.act', 'gelu_7']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ù
val_81
val_3val_83node_Add_83"AddJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.0.convffn.act: torch.nn.modules.activation.GELU/gelu_7: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_7 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_15,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.0', 'encoder.model.network.2.0.convffn', 'encoder.model.network.2.0.convffn.act', 'gelu_7']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ù
val_5
val_83val_85node_Mul_85"MulJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.0.convffn.act: torch.nn.modules.activation.GELU/gelu_7: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_7 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_15,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.0', 'encoder.model.network.2.0.convffn', 'encoder.model.network.2.0.convffn.act', 'gelu_7']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
°
	conv2d_15
val_85gelu_7node_gelu_7"MulJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.0.convffn.act: torch.nn.modules.activation.GELU/gelu_7: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_7 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_15,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.0', 'encoder.model.network.2.0.convffn', 'encoder.model.network.2.0.convffn.act', 'gelu_7']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
∫
gelu_7
,encoder.model.network.2.0.convffn.fc2.weight
*encoder.model.network.2.0.convffn.fc2.bias	conv2d_16node_conv2d_16"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.0.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_16: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÎ
pkg.torch.onnx.fx_node–%conv2d_16 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_4, %p_encoder_model_network_2_0_convffn_fc2_weight, %p_encoder_model_network_2_0_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.0', 'encoder.model.network.2.0.convffn', 'encoder.model.network.2.0.convffn.fc2', 'conv2d_16']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
Ñ
%encoder.model.network.2.0.layer_scale
	conv2d_16mul_2
node_mul_2"MulJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.0: mobileclip.models.mci.RepMixerBlock/mul_2: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']J≠
pkg.torch.onnx.fx_nodeí%mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_2_0_layer_scale, %clone_5), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.0', 'mul_2']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
√
	conv2d_13
mul_2add_2
node_add_2"AddJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.0: mobileclip.models.mci.RepMixerBlock/add_2: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jå
pkg.torch.onnx.fx_noder%add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_13, %mul_2), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.0', 'add_2']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ì
add_2
9encoder.model.network.2.1.token_mixer.reparam_conv.weight
7encoder.model.network.2.1.token_mixer.reparam_conv.bias	conv2d_17node_conv2d_17"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.1.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.2.1.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_17: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J†
pkg.torch.onnx.fx_nodeÖ%conv2d_17 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_2, %p_encoder_model_network_2_1_token_mixer_reparam_conv_weight, %p_encoder_model_network_2_1_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 128), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.1', 'encoder.model.network.2.1.token_mixer', 'encoder.model.network.2.1.token_mixer.reparam_conv', 'conv2d_17']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ó
	conv2d_17
2encoder.model.network.2.1.convffn.conv.conv.weight
7encoder.model.network.2.1.convffn.conv.conv.weight_bias	getitem_9node_Conv_563"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∏
	namespace™: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.1.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.2.1.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_3: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J±
pkg.torch.onnx.fx_nodeñ%_native_batch_norm_legit_no_training_3 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_18, %p_encoder_model_network_2_1_convffn_conv_bn_weight, %p_encoder_model_network_2_1_convffn_conv_bn_bias, %b_encoder_model_network_2_1_convffn_conv_bn_running_mean, %b_encoder_model_network_2_1_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jù
pkg.torch.onnx.name_scopes˛['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.1', 'encoder.model.network.2.1.convffn', 'encoder.model.network.2.1.convffn.conv', 'encoder.model.network.2.1.convffn.conv.bn', '_native_batch_norm_legit_no_training_3']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
ø
	getitem_9
,encoder.model.network.2.1.convffn.fc1.weight
*encoder.model.network.2.1.convffn.fc1.bias	conv2d_19node_conv2d_19"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.1.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_19: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÌ
pkg.torch.onnx.fx_node“%conv2d_19 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_9, %p_encoder_model_network_2_1_convffn_fc1_weight, %p_encoder_model_network_2_1_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.1', 'encoder.model.network.2.1.convffn', 'encoder.model.network.2.1.convffn.fc1', 'conv2d_19']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
†
	conv2d_19
val_0val_96node_Div_96"DivJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.1.convffn.act: torch.nn.modules.activation.GELU/gelu_8: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_8 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_19,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.1', 'encoder.model.network.2.1.convffn', 'encoder.model.network.2.1.convffn.act', 'gelu_8']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ñ
val_96val_97node_Erf_97"ErfJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.1.convffn.act: torch.nn.modules.activation.GELU/gelu_8: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_8 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_19,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.1', 'encoder.model.network.2.1.convffn', 'encoder.model.network.2.1.convffn.act', 'gelu_8']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ù
val_97
val_3val_99node_Add_99"AddJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.1.convffn.act: torch.nn.modules.activation.GELU/gelu_8: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_8 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_19,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.1', 'encoder.model.network.2.1.convffn', 'encoder.model.network.2.1.convffn.act', 'gelu_8']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ü
val_5
val_99val_101node_Mul_101"MulJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.1.convffn.act: torch.nn.modules.activation.GELU/gelu_8: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_8 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_19,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.1', 'encoder.model.network.2.1.convffn', 'encoder.model.network.2.1.convffn.act', 'gelu_8']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
¢
	conv2d_19
val_101gelu_8node_gelu_8"MulJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.1.convffn.act: torch.nn.modules.activation.GELU/gelu_8: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_8 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_19,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.1', 'encoder.model.network.2.1.convffn', 'encoder.model.network.2.1.convffn.act', 'gelu_8']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
∫
gelu_8
,encoder.model.network.2.1.convffn.fc2.weight
*encoder.model.network.2.1.convffn.fc2.bias	conv2d_20node_conv2d_20"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.1.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_20: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÎ
pkg.torch.onnx.fx_node–%conv2d_20 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_6, %p_encoder_model_network_2_1_convffn_fc2_weight, %p_encoder_model_network_2_1_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.1', 'encoder.model.network.2.1.convffn', 'encoder.model.network.2.1.convffn.fc2', 'conv2d_20']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
Ñ
%encoder.model.network.2.1.layer_scale
	conv2d_20mul_3
node_mul_3"MulJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.1: mobileclip.models.mci.RepMixerBlock/mul_3: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']J≠
pkg.torch.onnx.fx_nodeí%mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_2_1_layer_scale, %clone_7), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.1', 'mul_3']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
√
	conv2d_17
mul_3add_3
node_add_3"AddJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.1: mobileclip.models.mci.RepMixerBlock/add_3: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jå
pkg.torch.onnx.fx_noder%add_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_17, %mul_3), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.1', 'add_3']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ì
add_3
9encoder.model.network.2.2.token_mixer.reparam_conv.weight
7encoder.model.network.2.2.token_mixer.reparam_conv.bias	conv2d_21node_conv2d_21"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.2.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.2.2.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_21: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J†
pkg.torch.onnx.fx_nodeÖ%conv2d_21 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_3, %p_encoder_model_network_2_2_token_mixer_reparam_conv_weight, %p_encoder_model_network_2_2_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 128), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.2', 'encoder.model.network.2.2.token_mixer', 'encoder.model.network.2.2.token_mixer.reparam_conv', 'conv2d_21']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ò
	conv2d_21
2encoder.model.network.2.2.convffn.conv.conv.weight
7encoder.model.network.2.2.convffn.conv.conv.weight_bias
getitem_12node_Conv_565"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∏
	namespace™: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.2.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.2.2.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_4: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J±
pkg.torch.onnx.fx_nodeñ%_native_batch_norm_legit_no_training_4 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_22, %p_encoder_model_network_2_2_convffn_conv_bn_weight, %p_encoder_model_network_2_2_convffn_conv_bn_bias, %b_encoder_model_network_2_2_convffn_conv_bn_running_mean, %b_encoder_model_network_2_2_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jù
pkg.torch.onnx.name_scopes˛['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.2', 'encoder.model.network.2.2.convffn', 'encoder.model.network.2.2.convffn.conv', 'encoder.model.network.2.2.convffn.conv.bn', '_native_batch_norm_legit_no_training_4']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_12
,encoder.model.network.2.2.convffn.fc1.weight
*encoder.model.network.2.2.convffn.fc1.bias	conv2d_23node_conv2d_23"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.2.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_23: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_23 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_12, %p_encoder_model_network_2_2_convffn_fc1_weight, %p_encoder_model_network_2_2_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.2', 'encoder.model.network.2.2.convffn', 'encoder.model.network.2.2.convffn.fc1', 'conv2d_23']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
¢
	conv2d_23
val_0val_112node_Div_112"DivJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.2.convffn.act: torch.nn.modules.activation.GELU/gelu_9: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_9 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_23,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.2', 'encoder.model.network.2.2.convffn', 'encoder.model.network.2.2.convffn.act', 'gelu_9']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ô
val_112val_113node_Erf_113"ErfJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.2.convffn.act: torch.nn.modules.activation.GELU/gelu_9: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_9 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_23,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.2', 'encoder.model.network.2.2.convffn', 'encoder.model.network.2.2.convffn.act', 'gelu_9']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
†
val_113
val_3val_115node_Add_115"AddJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.2.convffn.act: torch.nn.modules.activation.GELU/gelu_9: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_9 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_23,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.2', 'encoder.model.network.2.2.convffn', 'encoder.model.network.2.2.convffn.act', 'gelu_9']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
†
val_5
val_115val_117node_Mul_117"MulJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.2.convffn.act: torch.nn.modules.activation.GELU/gelu_9: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_9 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_23,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.2', 'encoder.model.network.2.2.convffn', 'encoder.model.network.2.2.convffn.act', 'gelu_9']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
¢
	conv2d_23
val_117gelu_9node_gelu_9"MulJ†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.2.convffn.act: torch.nn.modules.activation.GELU/gelu_9: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jà
pkg.torch.onnx.fx_noden%gelu_9 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_23,), kwargs = {})Jœ
pkg.torch.onnx.name_scopes∞['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.2', 'encoder.model.network.2.2.convffn', 'encoder.model.network.2.2.convffn.act', 'gelu_9']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
∫
gelu_9
,encoder.model.network.2.2.convffn.fc2.weight
*encoder.model.network.2.2.convffn.fc2.bias	conv2d_24node_conv2d_24"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.2.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_24: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÎ
pkg.torch.onnx.fx_node–%conv2d_24 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_8, %p_encoder_model_network_2_2_convffn_fc2_weight, %p_encoder_model_network_2_2_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.2', 'encoder.model.network.2.2.convffn', 'encoder.model.network.2.2.convffn.fc2', 'conv2d_24']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
Ñ
%encoder.model.network.2.2.layer_scale
	conv2d_24mul_4
node_mul_4"MulJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.2: mobileclip.models.mci.RepMixerBlock/mul_4: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']J≠
pkg.torch.onnx.fx_nodeí%mul_4 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_2_2_layer_scale, %clone_9), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.2', 'mul_4']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
√
	conv2d_21
mul_4add_4
node_add_4"AddJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.2: mobileclip.models.mci.RepMixerBlock/add_4: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jå
pkg.torch.onnx.fx_noder%add_4 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_21, %mul_4), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.2', 'add_4']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ì
add_4
9encoder.model.network.2.3.token_mixer.reparam_conv.weight
7encoder.model.network.2.3.token_mixer.reparam_conv.bias	conv2d_25node_conv2d_25"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.3.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.2.3.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_25: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J†
pkg.torch.onnx.fx_nodeÖ%conv2d_25 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_4, %p_encoder_model_network_2_3_token_mixer_reparam_conv_weight, %p_encoder_model_network_2_3_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 128), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.3', 'encoder.model.network.2.3.token_mixer', 'encoder.model.network.2.3.token_mixer.reparam_conv', 'conv2d_25']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ò
	conv2d_25
2encoder.model.network.2.3.convffn.conv.conv.weight
7encoder.model.network.2.3.convffn.conv.conv.weight_bias
getitem_15node_Conv_567"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∏
	namespace™: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.3.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.2.3.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_5: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J±
pkg.torch.onnx.fx_nodeñ%_native_batch_norm_legit_no_training_5 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_26, %p_encoder_model_network_2_3_convffn_conv_bn_weight, %p_encoder_model_network_2_3_convffn_conv_bn_bias, %b_encoder_model_network_2_3_convffn_conv_bn_running_mean, %b_encoder_model_network_2_3_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jù
pkg.torch.onnx.name_scopes˛['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.3', 'encoder.model.network.2.3.convffn', 'encoder.model.network.2.3.convffn.conv', 'encoder.model.network.2.3.convffn.conv.bn', '_native_batch_norm_legit_no_training_5']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_15
,encoder.model.network.2.3.convffn.fc1.weight
*encoder.model.network.2.3.convffn.fc1.bias	conv2d_27node_conv2d_27"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.3.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_27: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_27 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_15, %p_encoder_model_network_2_3_convffn_fc1_weight, %p_encoder_model_network_2_3_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.3', 'encoder.model.network.2.3.convffn', 'encoder.model.network.2.3.convffn.fc1', 'conv2d_27']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_27
val_0val_128node_Div_128"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.3.convffn.act: torch.nn.modules.activation.GELU/gelu_10: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_10 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_27,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.3', 'encoder.model.network.2.3.convffn', 'encoder.model.network.2.3.convffn.act', 'gelu_10']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_128val_129node_Erf_129"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.3.convffn.act: torch.nn.modules.activation.GELU/gelu_10: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_10 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_27,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.3', 'encoder.model.network.2.3.convffn', 'encoder.model.network.2.3.convffn.act', 'gelu_10']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_129
val_3val_131node_Add_131"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.3.convffn.act: torch.nn.modules.activation.GELU/gelu_10: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_10 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_27,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.3', 'encoder.model.network.2.3.convffn', 'encoder.model.network.2.3.convffn.act', 'gelu_10']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_131val_133node_Mul_133"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.3.convffn.act: torch.nn.modules.activation.GELU/gelu_10: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_10 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_27,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.3', 'encoder.model.network.2.3.convffn', 'encoder.model.network.2.3.convffn.act', 'gelu_10']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_27
val_133gelu_10node_gelu_10"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.3.convffn.act: torch.nn.modules.activation.GELU/gelu_10: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_10 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_27,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.3', 'encoder.model.network.2.3.convffn', 'encoder.model.network.2.3.convffn.act', 'gelu_10']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_10
,encoder.model.network.2.3.convffn.fc2.weight
*encoder.model.network.2.3.convffn.fc2.bias	conv2d_28node_conv2d_28"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.3.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_28: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_28 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_10, %p_encoder_model_network_2_3_convffn_fc2_weight, %p_encoder_model_network_2_3_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.3', 'encoder.model.network.2.3.convffn', 'encoder.model.network.2.3.convffn.fc2', 'conv2d_28']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
Ö
%encoder.model.network.2.3.layer_scale
	conv2d_28mul_5
node_mul_5"MulJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.3: mobileclip.models.mci.RepMixerBlock/mul_5: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JÆ
pkg.torch.onnx.fx_nodeì%mul_5 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_2_3_layer_scale, %clone_11), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.3', 'mul_5']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
√
	conv2d_25
mul_5add_5
node_add_5"AddJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.3: mobileclip.models.mci.RepMixerBlock/add_5: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jå
pkg.torch.onnx.fx_noder%add_5 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_25, %mul_5), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.3', 'add_5']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ì
add_5
9encoder.model.network.2.4.token_mixer.reparam_conv.weight
7encoder.model.network.2.4.token_mixer.reparam_conv.bias	conv2d_29node_conv2d_29"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.4.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.2.4.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_29: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J†
pkg.torch.onnx.fx_nodeÖ%conv2d_29 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_5, %p_encoder_model_network_2_4_token_mixer_reparam_conv_weight, %p_encoder_model_network_2_4_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 128), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.4', 'encoder.model.network.2.4.token_mixer', 'encoder.model.network.2.4.token_mixer.reparam_conv', 'conv2d_29']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ò
	conv2d_29
2encoder.model.network.2.4.convffn.conv.conv.weight
7encoder.model.network.2.4.convffn.conv.conv.weight_bias
getitem_18node_Conv_569"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∏
	namespace™: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.4.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.2.4.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_6: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J±
pkg.torch.onnx.fx_nodeñ%_native_batch_norm_legit_no_training_6 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_30, %p_encoder_model_network_2_4_convffn_conv_bn_weight, %p_encoder_model_network_2_4_convffn_conv_bn_bias, %b_encoder_model_network_2_4_convffn_conv_bn_running_mean, %b_encoder_model_network_2_4_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jù
pkg.torch.onnx.name_scopes˛['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.4', 'encoder.model.network.2.4.convffn', 'encoder.model.network.2.4.convffn.conv', 'encoder.model.network.2.4.convffn.conv.bn', '_native_batch_norm_legit_no_training_6']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_18
,encoder.model.network.2.4.convffn.fc1.weight
*encoder.model.network.2.4.convffn.fc1.bias	conv2d_31node_conv2d_31"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.4.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_31: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_31 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_18, %p_encoder_model_network_2_4_convffn_fc1_weight, %p_encoder_model_network_2_4_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.4', 'encoder.model.network.2.4.convffn', 'encoder.model.network.2.4.convffn.fc1', 'conv2d_31']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_31
val_0val_144node_Div_144"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.4.convffn.act: torch.nn.modules.activation.GELU/gelu_11: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_11 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_31,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.4', 'encoder.model.network.2.4.convffn', 'encoder.model.network.2.4.convffn.act', 'gelu_11']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_144val_145node_Erf_145"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.4.convffn.act: torch.nn.modules.activation.GELU/gelu_11: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_11 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_31,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.4', 'encoder.model.network.2.4.convffn', 'encoder.model.network.2.4.convffn.act', 'gelu_11']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_145
val_3val_147node_Add_147"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.4.convffn.act: torch.nn.modules.activation.GELU/gelu_11: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_11 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_31,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.4', 'encoder.model.network.2.4.convffn', 'encoder.model.network.2.4.convffn.act', 'gelu_11']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_147val_149node_Mul_149"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.4.convffn.act: torch.nn.modules.activation.GELU/gelu_11: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_11 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_31,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.4', 'encoder.model.network.2.4.convffn', 'encoder.model.network.2.4.convffn.act', 'gelu_11']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_31
val_149gelu_11node_gelu_11"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.4.convffn.act: torch.nn.modules.activation.GELU/gelu_11: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_11 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_31,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.4', 'encoder.model.network.2.4.convffn', 'encoder.model.network.2.4.convffn.act', 'gelu_11']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_11
,encoder.model.network.2.4.convffn.fc2.weight
*encoder.model.network.2.4.convffn.fc2.bias	conv2d_32node_conv2d_32"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.4.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_32: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_32 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_12, %p_encoder_model_network_2_4_convffn_fc2_weight, %p_encoder_model_network_2_4_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.4', 'encoder.model.network.2.4.convffn', 'encoder.model.network.2.4.convffn.fc2', 'conv2d_32']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
Ö
%encoder.model.network.2.4.layer_scale
	conv2d_32mul_6
node_mul_6"MulJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.4: mobileclip.models.mci.RepMixerBlock/mul_6: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JÆ
pkg.torch.onnx.fx_nodeì%mul_6 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_2_4_layer_scale, %clone_13), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.4', 'mul_6']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
√
	conv2d_29
mul_6add_6
node_add_6"AddJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.4: mobileclip.models.mci.RepMixerBlock/add_6: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jå
pkg.torch.onnx.fx_noder%add_6 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_29, %mul_6), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.4', 'add_6']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ì
add_6
9encoder.model.network.2.5.token_mixer.reparam_conv.weight
7encoder.model.network.2.5.token_mixer.reparam_conv.bias	conv2d_33node_conv2d_33"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.5.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.2.5.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_33: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J†
pkg.torch.onnx.fx_nodeÖ%conv2d_33 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_6, %p_encoder_model_network_2_5_token_mixer_reparam_conv_weight, %p_encoder_model_network_2_5_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 128), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.5', 'encoder.model.network.2.5.token_mixer', 'encoder.model.network.2.5.token_mixer.reparam_conv', 'conv2d_33']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ò
	conv2d_33
2encoder.model.network.2.5.convffn.conv.conv.weight
7encoder.model.network.2.5.convffn.conv.conv.weight_bias
getitem_21node_Conv_571"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∏
	namespace™: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.5.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.2.5.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_7: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J±
pkg.torch.onnx.fx_nodeñ%_native_batch_norm_legit_no_training_7 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_34, %p_encoder_model_network_2_5_convffn_conv_bn_weight, %p_encoder_model_network_2_5_convffn_conv_bn_bias, %b_encoder_model_network_2_5_convffn_conv_bn_running_mean, %b_encoder_model_network_2_5_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jù
pkg.torch.onnx.name_scopes˛['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.5', 'encoder.model.network.2.5.convffn', 'encoder.model.network.2.5.convffn.conv', 'encoder.model.network.2.5.convffn.conv.bn', '_native_batch_norm_legit_no_training_7']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_21
,encoder.model.network.2.5.convffn.fc1.weight
*encoder.model.network.2.5.convffn.fc1.bias	conv2d_35node_conv2d_35"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.5.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_35: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_35 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_21, %p_encoder_model_network_2_5_convffn_fc1_weight, %p_encoder_model_network_2_5_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.5', 'encoder.model.network.2.5.convffn', 'encoder.model.network.2.5.convffn.fc1', 'conv2d_35']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_35
val_0val_160node_Div_160"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.5.convffn.act: torch.nn.modules.activation.GELU/gelu_12: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_12 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_35,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.5', 'encoder.model.network.2.5.convffn', 'encoder.model.network.2.5.convffn.act', 'gelu_12']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_160val_161node_Erf_161"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.5.convffn.act: torch.nn.modules.activation.GELU/gelu_12: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_12 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_35,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.5', 'encoder.model.network.2.5.convffn', 'encoder.model.network.2.5.convffn.act', 'gelu_12']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_161
val_3val_163node_Add_163"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.5.convffn.act: torch.nn.modules.activation.GELU/gelu_12: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_12 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_35,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.5', 'encoder.model.network.2.5.convffn', 'encoder.model.network.2.5.convffn.act', 'gelu_12']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_163val_165node_Mul_165"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.5.convffn.act: torch.nn.modules.activation.GELU/gelu_12: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_12 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_35,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.5', 'encoder.model.network.2.5.convffn', 'encoder.model.network.2.5.convffn.act', 'gelu_12']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_35
val_165gelu_12node_gelu_12"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.5.convffn.act: torch.nn.modules.activation.GELU/gelu_12: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_12 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_35,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.5', 'encoder.model.network.2.5.convffn', 'encoder.model.network.2.5.convffn.act', 'gelu_12']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_12
,encoder.model.network.2.5.convffn.fc2.weight
*encoder.model.network.2.5.convffn.fc2.bias	conv2d_36node_conv2d_36"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.2.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.2.5.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_36: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_36 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_14, %p_encoder_model_network_2_5_convffn_fc2_weight, %p_encoder_model_network_2_5_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.5', 'encoder.model.network.2.5.convffn', 'encoder.model.network.2.5.convffn.fc2', 'conv2d_36']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
Ö
%encoder.model.network.2.5.layer_scale
	conv2d_36mul_7
node_mul_7"MulJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.5: mobileclip.models.mci.RepMixerBlock/mul_7: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JÆ
pkg.torch.onnx.fx_nodeì%mul_7 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_2_5_layer_scale, %clone_15), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.5', 'mul_7']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
√
	conv2d_33
mul_7add_7
node_add_7"AddJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.2: torch.nn.modules.container.Sequential/encoder.model.network.2.5: mobileclip.models.mci.RepMixerBlock/add_7: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jå
pkg.torch.onnx.fx_noder%add_7 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_33, %mul_7), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.2', 'encoder.model.network.2.5', 'add_7']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
é
add_7
1encoder.model.network.3.proj.0.lkb_reparam.weight
/encoder.model.network.3.proj.0.lkb_reparam.bias	conv2d_37node_conv2d_37"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jæ
	namespace∞: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.3.proj.0.lkb_reparam: torch.nn.modules.conv.Conv2d/conv2d_37: aten.conv2d.defaultJƒ
pkg.torch.onnx.class_hierarchy°['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']Jê
pkg.torch.onnx.fx_nodeı%conv2d_37 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_7, %p_encoder_model_network_3_proj_0_lkb_reparam_weight, %p_encoder_model_network_3_proj_0_lkb_reparam_bias, [2, 2], [3, 3], [1, 1], 128), kwargs = {})J◊
pkg.torch.onnx.name_scopes∏['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.0', 'encoder.model.network.3.proj.0.lkb_reparam', 'conv2d_37']J´
pkg.torch.onnx.stack_traceåFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 92, in forward
    out = self.lkb_reparam(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
·
	conv2d_37
val_168mean	node_mean"
ReduceMean*
keepdims†*
noop_with_empty_axes †J±
	namespace£: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.3.proj.0.se: timm.layers.squeeze_excite.SEModule/mean: aten.mean.dimJ≈
pkg.torch.onnx.class_hierarchy¢['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'timm.layers.squeeze_excite.SEModule', 'aten.mean.dim']Jè
pkg.torch.onnx.fx_nodeu%mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%conv2d_37, [2, 3], True), kwargs = {})J…
pkg.torch.onnx.name_scopes™['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.0', 'encoder.model.network.3.proj.0.se', 'mean']J•
pkg.torch.onnx.stack_traceÜFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\squeeze_excite.py", line 56, in forward
    x_se = x.mean((2, 3), keepdim=True)
Å
mean
,encoder.model.network.3.proj.0.se.fc1.weight
*encoder.model.network.3.proj.0.se.fc1.bias	conv2d_38node_conv2d_38"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÄ
	namespaceÚ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.3.proj.0.se: timm.layers.squeeze_excite.SEModule/encoder.model.network.3.proj.0.se.fc1: torch.nn.modules.conv.Conv2d/conv2d_38: aten.conv2d.defaultJÎ
pkg.torch.onnx.class_hierarchy»['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'timm.layers.squeeze_excite.SEModule', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JË
pkg.torch.onnx.fx_nodeÕ%conv2d_38 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%mean, %p_encoder_model_network_3_proj_0_se_fc1_weight, %p_encoder_model_network_3_proj_0_se_fc1_bias), kwargs = {})J˜
pkg.torch.onnx.name_scopesÿ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.0', 'encoder.model.network.3.proj.0.se', 'encoder.model.network.3.proj.0.se.fc1', 'conv2d_38']J…
pkg.torch.onnx.stack_trace™File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\squeeze_excite.py", line 60, in forward
    x_se = self.fc1(x_se)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
›
	conv2d_38relu	node_relu"ReluJ˝
	namespaceÔ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.3.proj.0.se: timm.layers.squeeze_excite.SEModule/encoder.model.network.3.proj.0.se.act: torch.nn.modules.activation.ReLU/relu: aten.relu.defaultJÌ
pkg.torch.onnx.class_hierarchy ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'timm.layers.squeeze_excite.SEModule', 'torch.nn.modules.activation.ReLU', 'aten.relu.default']JÜ
pkg.torch.onnx.fx_nodel%relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%conv2d_38,), kwargs = {})JÚ
pkg.torch.onnx.name_scopes”['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.0', 'encoder.model.network.3.proj.0.se', 'encoder.model.network.3.proj.0.se.act', 'relu']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\squeeze_excite.py", line 61, in forward
    x_se = self.act(self.bn(x_se))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 144, in forward
    return F.relu(input, inplace=self.inplace)
Å
relu
,encoder.model.network.3.proj.0.se.fc2.weight
*encoder.model.network.3.proj.0.se.fc2.bias	conv2d_39node_conv2d_39"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÄ
	namespaceÚ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.3.proj.0.se: timm.layers.squeeze_excite.SEModule/encoder.model.network.3.proj.0.se.fc2: torch.nn.modules.conv.Conv2d/conv2d_39: aten.conv2d.defaultJÎ
pkg.torch.onnx.class_hierarchy»['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'timm.layers.squeeze_excite.SEModule', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JË
pkg.torch.onnx.fx_nodeÕ%conv2d_39 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%relu, %p_encoder_model_network_3_proj_0_se_fc2_weight, %p_encoder_model_network_3_proj_0_se_fc2_bias), kwargs = {})J˜
pkg.torch.onnx.name_scopesÿ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.0', 'encoder.model.network.3.proj.0.se', 'encoder.model.network.3.proj.0.se.fc2', 'conv2d_39']J…
pkg.torch.onnx.stack_trace™File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\squeeze_excite.py", line 62, in forward
    x_se = self.fc2(x_se)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
˘
	conv2d_39sigmoidnode_sigmoid"SigmoidJÉ
	namespaceı: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.3.proj.0.se: timm.layers.squeeze_excite.SEModule/encoder.model.network.3.proj.0.se.gate: timm.layers.activations.Sigmoid/sigmoid: aten.sigmoid.defaultJÔ
pkg.torch.onnx.class_hierarchyÃ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'timm.layers.squeeze_excite.SEModule', 'timm.layers.activations.Sigmoid', 'aten.sigmoid.default']Jå
pkg.torch.onnx.fx_noder%sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%conv2d_39,), kwargs = {})Jˆ
pkg.torch.onnx.name_scopes◊['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.0', 'encoder.model.network.3.proj.0.se', 'encoder.model.network.3.proj.0.se.gate', 'sigmoid']JÀ
pkg.torch.onnx.stack_trace¨File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\squeeze_excite.py", line 63, in forward
    return x * self.gate(x_se)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\activations.py", line 57, in forward
    return x.sigmoid_() if self.inplace else x.sigmoid()
™
	conv2d_37
sigmoidmul_8
node_mul_8"MulJ¥
	namespace¶: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.3.proj.0.se: timm.layers.squeeze_excite.SEModule/mul_8: aten.mul.TensorJ«
pkg.torch.onnx.class_hierarchy§['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'timm.layers.squeeze_excite.SEModule', 'aten.mul.Tensor']Jé
pkg.torch.onnx.fx_nodet%mul_8 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%conv2d_37, %sigmoid), kwargs = {})J 
pkg.torch.onnx.name_scopes´['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.0', 'encoder.model.network.3.proj.0.se', 'mul_8']Jú
pkg.torch.onnx.stack_trace˝File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\squeeze_excite.py", line 63, in forward
    return x * self.gate(x_se)
À
mul_8
val_0val_170node_Div_170"DivJΩ
	namespaceØ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.3.proj.0.activation: torch.nn.modules.activation.GELU/gelu_13: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÖ
pkg.torch.onnx.fx_nodek%gelu_13 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_8,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.0', 'encoder.model.network.3.proj.0.activation', 'gelu_13']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
∆
val_170val_171node_Erf_171"ErfJΩ
	namespaceØ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.3.proj.0.activation: torch.nn.modules.activation.GELU/gelu_13: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÖ
pkg.torch.onnx.fx_nodek%gelu_13 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_8,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.0', 'encoder.model.network.3.proj.0.activation', 'gelu_13']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Õ
val_171
val_3val_173node_Add_173"AddJΩ
	namespaceØ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.3.proj.0.activation: torch.nn.modules.activation.GELU/gelu_13: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÖ
pkg.torch.onnx.fx_nodek%gelu_13 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_8,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.0', 'encoder.model.network.3.proj.0.activation', 'gelu_13']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Õ
val_5
val_173val_175node_Mul_175"MulJΩ
	namespaceØ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.3.proj.0.activation: torch.nn.modules.activation.GELU/gelu_13: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÖ
pkg.torch.onnx.fx_nodek%gelu_13 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_8,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.0', 'encoder.model.network.3.proj.0.activation', 'gelu_13']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Õ
mul_8
val_175gelu_13node_gelu_13"MulJΩ
	namespaceØ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.3.proj.0.activation: torch.nn.modules.activation.GELU/gelu_13: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÖ
pkg.torch.onnx.fx_nodek%gelu_13 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_8,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.0', 'encoder.model.network.3.proj.0.activation', 'gelu_13']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ç
gelu_13
2encoder.model.network.3.proj.1.reparam_conv.weight
0encoder.model.network.3.proj.1.reparam_conv.bias	conv2d_40node_conv2d_40"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jπ
	namespace´: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.3.proj.1.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_40: aten.conv2d.defaultJæ
pkg.torch.onnx.class_hierarchyõ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J˜
pkg.torch.onnx.fx_node‹%conv2d_40 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%gelu_13, %p_encoder_model_network_3_proj_1_reparam_conv_weight, %p_encoder_model_network_3_proj_1_reparam_conv_bias), kwargs = {})Jÿ
pkg.torch.onnx.name_scopesπ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.1', 'encoder.model.network.3.proj.1.reparam_conv', 'conv2d_40']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
€
	conv2d_40
val_0val_177node_Div_177"DivJ∑
	namespace©: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.3.proj.1.activation: torch.nn.modules.activation.GELU/gelu_14: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_14 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_40,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.1', 'encoder.model.network.3.proj.1.activation', 'gelu_14']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
“
val_177val_178node_Erf_178"ErfJ∑
	namespace©: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.3.proj.1.activation: torch.nn.modules.activation.GELU/gelu_14: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_14 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_40,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.1', 'encoder.model.network.3.proj.1.activation', 'gelu_14']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Ÿ
val_178
val_3val_180node_Add_180"AddJ∑
	namespace©: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.3.proj.1.activation: torch.nn.modules.activation.GELU/gelu_14: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_14 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_40,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.1', 'encoder.model.network.3.proj.1.activation', 'gelu_14']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Ÿ
val_5
val_180val_182node_Mul_182"MulJ∑
	namespace©: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.3.proj.1.activation: torch.nn.modules.activation.GELU/gelu_14: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_14 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_40,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.1', 'encoder.model.network.3.proj.1.activation', 'gelu_14']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
›
	conv2d_40
val_182gelu_14node_gelu_14"MulJ∑
	namespace©: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.3: mobileclip.models.mci.PatchEmbed/encoder.model.network.3.proj: torch.nn.modules.container.Sequential/encoder.model.network.3.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.3.proj.1.activation: torch.nn.modules.activation.GELU/gelu_14: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_14 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_40,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.3', 'encoder.model.network.3.proj', 'encoder.model.network.3.proj.1', 'encoder.model.network.3.proj.1.activation', 'gelu_14']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ó
gelu_14
9encoder.model.network.4.0.token_mixer.reparam_conv.weight
7encoder.model.network.4.0.token_mixer.reparam_conv.bias	conv2d_41node_conv2d_41"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.0.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.4.0.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_41: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J¢
pkg.torch.onnx.fx_nodeá%conv2d_41 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%gelu_14, %p_encoder_model_network_4_0_token_mixer_reparam_conv_weight, %p_encoder_model_network_4_0_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 256), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.0', 'encoder.model.network.4.0.token_mixer', 'encoder.model.network.4.0.token_mixer.reparam_conv', 'conv2d_41']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ò
	conv2d_41
2encoder.model.network.4.0.convffn.conv.conv.weight
7encoder.model.network.4.0.convffn.conv.conv.weight_bias
getitem_24node_Conv_573"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∏
	namespace™: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.0.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.4.0.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_8: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J±
pkg.torch.onnx.fx_nodeñ%_native_batch_norm_legit_no_training_8 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_42, %p_encoder_model_network_4_0_convffn_conv_bn_weight, %p_encoder_model_network_4_0_convffn_conv_bn_bias, %b_encoder_model_network_4_0_convffn_conv_bn_running_mean, %b_encoder_model_network_4_0_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jù
pkg.torch.onnx.name_scopes˛['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.0', 'encoder.model.network.4.0.convffn', 'encoder.model.network.4.0.convffn.conv', 'encoder.model.network.4.0.convffn.conv.bn', '_native_batch_norm_legit_no_training_8']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_24
,encoder.model.network.4.0.convffn.fc1.weight
*encoder.model.network.4.0.convffn.fc1.bias	conv2d_43node_conv2d_43"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.0.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_43: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_43 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_24, %p_encoder_model_network_4_0_convffn_fc1_weight, %p_encoder_model_network_4_0_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.0', 'encoder.model.network.4.0.convffn', 'encoder.model.network.4.0.convffn.fc1', 'conv2d_43']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_43
val_0val_193node_Div_193"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.0.convffn.act: torch.nn.modules.activation.GELU/gelu_15: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_15 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_43,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.0', 'encoder.model.network.4.0.convffn', 'encoder.model.network.4.0.convffn.act', 'gelu_15']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_193val_194node_Erf_194"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.0.convffn.act: torch.nn.modules.activation.GELU/gelu_15: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_15 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_43,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.0', 'encoder.model.network.4.0.convffn', 'encoder.model.network.4.0.convffn.act', 'gelu_15']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_194
val_3val_196node_Add_196"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.0.convffn.act: torch.nn.modules.activation.GELU/gelu_15: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_15 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_43,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.0', 'encoder.model.network.4.0.convffn', 'encoder.model.network.4.0.convffn.act', 'gelu_15']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_196val_198node_Mul_198"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.0.convffn.act: torch.nn.modules.activation.GELU/gelu_15: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_15 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_43,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.0', 'encoder.model.network.4.0.convffn', 'encoder.model.network.4.0.convffn.act', 'gelu_15']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_43
val_198gelu_15node_gelu_15"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.0.convffn.act: torch.nn.modules.activation.GELU/gelu_15: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_15 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_43,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.0', 'encoder.model.network.4.0.convffn', 'encoder.model.network.4.0.convffn.act', 'gelu_15']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_15
,encoder.model.network.4.0.convffn.fc2.weight
*encoder.model.network.4.0.convffn.fc2.bias	conv2d_44node_conv2d_44"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.0: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.0.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_44: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_44 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_16, %p_encoder_model_network_4_0_convffn_fc2_weight, %p_encoder_model_network_4_0_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.0', 'encoder.model.network.4.0.convffn', 'encoder.model.network.4.0.convffn.fc2', 'conv2d_44']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
Ö
%encoder.model.network.4.0.layer_scale
	conv2d_44mul_9
node_mul_9"MulJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.0: mobileclip.models.mci.RepMixerBlock/mul_9: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JÆ
pkg.torch.onnx.fx_nodeì%mul_9 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_4_0_layer_scale, %clone_17), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.0', 'mul_9']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
√
	conv2d_41
mul_9add_8
node_add_8"AddJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.0: mobileclip.models.mci.RepMixerBlock/add_8: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jå
pkg.torch.onnx.fx_noder%add_8 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_41, %mul_9), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.0', 'add_8']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ì
add_8
9encoder.model.network.4.1.token_mixer.reparam_conv.weight
7encoder.model.network.4.1.token_mixer.reparam_conv.bias	conv2d_45node_conv2d_45"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.1.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.4.1.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_45: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J†
pkg.torch.onnx.fx_nodeÖ%conv2d_45 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_8, %p_encoder_model_network_4_1_token_mixer_reparam_conv_weight, %p_encoder_model_network_4_1_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 256), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.1', 'encoder.model.network.4.1.token_mixer', 'encoder.model.network.4.1.token_mixer.reparam_conv', 'conv2d_45']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ò
	conv2d_45
2encoder.model.network.4.1.convffn.conv.conv.weight
7encoder.model.network.4.1.convffn.conv.conv.weight_bias
getitem_27node_Conv_575"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∏
	namespace™: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.1.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.4.1.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_9: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J±
pkg.torch.onnx.fx_nodeñ%_native_batch_norm_legit_no_training_9 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_46, %p_encoder_model_network_4_1_convffn_conv_bn_weight, %p_encoder_model_network_4_1_convffn_conv_bn_bias, %b_encoder_model_network_4_1_convffn_conv_bn_running_mean, %b_encoder_model_network_4_1_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jù
pkg.torch.onnx.name_scopes˛['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.1', 'encoder.model.network.4.1.convffn', 'encoder.model.network.4.1.convffn.conv', 'encoder.model.network.4.1.convffn.conv.bn', '_native_batch_norm_legit_no_training_9']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_27
,encoder.model.network.4.1.convffn.fc1.weight
*encoder.model.network.4.1.convffn.fc1.bias	conv2d_47node_conv2d_47"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.1.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_47: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_47 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_27, %p_encoder_model_network_4_1_convffn_fc1_weight, %p_encoder_model_network_4_1_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.1', 'encoder.model.network.4.1.convffn', 'encoder.model.network.4.1.convffn.fc1', 'conv2d_47']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_47
val_0val_209node_Div_209"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.1.convffn.act: torch.nn.modules.activation.GELU/gelu_16: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_16 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_47,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.1', 'encoder.model.network.4.1.convffn', 'encoder.model.network.4.1.convffn.act', 'gelu_16']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_209val_210node_Erf_210"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.1.convffn.act: torch.nn.modules.activation.GELU/gelu_16: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_16 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_47,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.1', 'encoder.model.network.4.1.convffn', 'encoder.model.network.4.1.convffn.act', 'gelu_16']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_210
val_3val_212node_Add_212"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.1.convffn.act: torch.nn.modules.activation.GELU/gelu_16: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_16 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_47,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.1', 'encoder.model.network.4.1.convffn', 'encoder.model.network.4.1.convffn.act', 'gelu_16']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_212val_214node_Mul_214"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.1.convffn.act: torch.nn.modules.activation.GELU/gelu_16: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_16 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_47,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.1', 'encoder.model.network.4.1.convffn', 'encoder.model.network.4.1.convffn.act', 'gelu_16']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_47
val_214gelu_16node_gelu_16"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.1.convffn.act: torch.nn.modules.activation.GELU/gelu_16: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_16 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_47,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.1', 'encoder.model.network.4.1.convffn', 'encoder.model.network.4.1.convffn.act', 'gelu_16']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_16
,encoder.model.network.4.1.convffn.fc2.weight
*encoder.model.network.4.1.convffn.fc2.bias	conv2d_48node_conv2d_48"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.1: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.1.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_48: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_48 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_18, %p_encoder_model_network_4_1_convffn_fc2_weight, %p_encoder_model_network_4_1_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.1', 'encoder.model.network.4.1.convffn', 'encoder.model.network.4.1.convffn.fc2', 'conv2d_48']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ã
%encoder.model.network.4.1.layer_scale
	conv2d_48mul_10node_mul_10"MulJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.1: mobileclip.models.mci.RepMixerBlock/mul_10: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JØ
pkg.torch.onnx.fx_nodeî%mul_10 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_4_1_layer_scale, %clone_19), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.1', 'mul_10']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
≈
	conv2d_45
mul_10add_9
node_add_9"AddJî
	namespaceÜ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.1: mobileclip.models.mci.RepMixerBlock/add_9: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jç
pkg.torch.onnx.fx_nodes%add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_45, %mul_10), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.1', 'add_9']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ì
add_9
9encoder.model.network.4.2.token_mixer.reparam_conv.weight
7encoder.model.network.4.2.token_mixer.reparam_conv.bias	conv2d_49node_conv2d_49"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.2.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.4.2.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_49: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J†
pkg.torch.onnx.fx_nodeÖ%conv2d_49 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_9, %p_encoder_model_network_4_2_token_mixer_reparam_conv_weight, %p_encoder_model_network_4_2_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 256), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.2', 'encoder.model.network.4.2.token_mixer', 'encoder.model.network.4.2.token_mixer.reparam_conv', 'conv2d_49']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
õ
	conv2d_49
2encoder.model.network.4.2.convffn.conv.conv.weight
7encoder.model.network.4.2.convffn.conv.conv.weight_bias
getitem_30node_Conv_577"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jπ
	namespace´: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.2.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.4.2.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_10: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J≤
pkg.torch.onnx.fx_nodeó%_native_batch_norm_legit_no_training_10 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_50, %p_encoder_model_network_4_2_convffn_conv_bn_weight, %p_encoder_model_network_4_2_convffn_conv_bn_bias, %b_encoder_model_network_4_2_convffn_conv_bn_running_mean, %b_encoder_model_network_4_2_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jû
pkg.torch.onnx.name_scopesˇ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.2', 'encoder.model.network.4.2.convffn', 'encoder.model.network.4.2.convffn.conv', 'encoder.model.network.4.2.convffn.conv.bn', '_native_batch_norm_legit_no_training_10']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_30
,encoder.model.network.4.2.convffn.fc1.weight
*encoder.model.network.4.2.convffn.fc1.bias	conv2d_51node_conv2d_51"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.2.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_51: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_51 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_30, %p_encoder_model_network_4_2_convffn_fc1_weight, %p_encoder_model_network_4_2_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.2', 'encoder.model.network.4.2.convffn', 'encoder.model.network.4.2.convffn.fc1', 'conv2d_51']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_51
val_0val_225node_Div_225"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.2.convffn.act: torch.nn.modules.activation.GELU/gelu_17: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_17 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_51,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.2', 'encoder.model.network.4.2.convffn', 'encoder.model.network.4.2.convffn.act', 'gelu_17']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_225val_226node_Erf_226"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.2.convffn.act: torch.nn.modules.activation.GELU/gelu_17: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_17 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_51,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.2', 'encoder.model.network.4.2.convffn', 'encoder.model.network.4.2.convffn.act', 'gelu_17']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_226
val_3val_228node_Add_228"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.2.convffn.act: torch.nn.modules.activation.GELU/gelu_17: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_17 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_51,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.2', 'encoder.model.network.4.2.convffn', 'encoder.model.network.4.2.convffn.act', 'gelu_17']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_228val_230node_Mul_230"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.2.convffn.act: torch.nn.modules.activation.GELU/gelu_17: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_17 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_51,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.2', 'encoder.model.network.4.2.convffn', 'encoder.model.network.4.2.convffn.act', 'gelu_17']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_51
val_230gelu_17node_gelu_17"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.2.convffn.act: torch.nn.modules.activation.GELU/gelu_17: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_17 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_51,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.2', 'encoder.model.network.4.2.convffn', 'encoder.model.network.4.2.convffn.act', 'gelu_17']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_17
,encoder.model.network.4.2.convffn.fc2.weight
*encoder.model.network.4.2.convffn.fc2.bias	conv2d_52node_conv2d_52"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.2: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.2.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.2.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_52: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_52 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_20, %p_encoder_model_network_4_2_convffn_fc2_weight, %p_encoder_model_network_4_2_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.2', 'encoder.model.network.4.2.convffn', 'encoder.model.network.4.2.convffn.fc2', 'conv2d_52']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ã
%encoder.model.network.4.2.layer_scale
	conv2d_52mul_11node_mul_11"MulJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.2: mobileclip.models.mci.RepMixerBlock/mul_11: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JØ
pkg.torch.onnx.fx_nodeî%mul_11 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_4_2_layer_scale, %clone_21), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.2', 'mul_11']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
À
	conv2d_49
mul_11add_10node_add_10"AddJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.2: mobileclip.models.mci.RepMixerBlock/add_10: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_10 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_49, %mul_11), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.2', 'add_10']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ï
add_10
9encoder.model.network.4.3.token_mixer.reparam_conv.weight
7encoder.model.network.4.3.token_mixer.reparam_conv.bias	conv2d_53node_conv2d_53"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.3.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.4.3.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_53: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J°
pkg.torch.onnx.fx_nodeÜ%conv2d_53 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_10, %p_encoder_model_network_4_3_token_mixer_reparam_conv_weight, %p_encoder_model_network_4_3_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 256), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.3', 'encoder.model.network.4.3.token_mixer', 'encoder.model.network.4.3.token_mixer.reparam_conv', 'conv2d_53']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
õ
	conv2d_53
2encoder.model.network.4.3.convffn.conv.conv.weight
7encoder.model.network.4.3.convffn.conv.conv.weight_bias
getitem_33node_Conv_579"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jπ
	namespace´: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.3.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.4.3.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_11: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J≤
pkg.torch.onnx.fx_nodeó%_native_batch_norm_legit_no_training_11 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_54, %p_encoder_model_network_4_3_convffn_conv_bn_weight, %p_encoder_model_network_4_3_convffn_conv_bn_bias, %b_encoder_model_network_4_3_convffn_conv_bn_running_mean, %b_encoder_model_network_4_3_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jû
pkg.torch.onnx.name_scopesˇ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.3', 'encoder.model.network.4.3.convffn', 'encoder.model.network.4.3.convffn.conv', 'encoder.model.network.4.3.convffn.conv.bn', '_native_batch_norm_legit_no_training_11']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_33
,encoder.model.network.4.3.convffn.fc1.weight
*encoder.model.network.4.3.convffn.fc1.bias	conv2d_55node_conv2d_55"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.3.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_55: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_55 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_33, %p_encoder_model_network_4_3_convffn_fc1_weight, %p_encoder_model_network_4_3_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.3', 'encoder.model.network.4.3.convffn', 'encoder.model.network.4.3.convffn.fc1', 'conv2d_55']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_55
val_0val_241node_Div_241"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.3.convffn.act: torch.nn.modules.activation.GELU/gelu_18: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_18 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_55,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.3', 'encoder.model.network.4.3.convffn', 'encoder.model.network.4.3.convffn.act', 'gelu_18']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_241val_242node_Erf_242"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.3.convffn.act: torch.nn.modules.activation.GELU/gelu_18: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_18 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_55,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.3', 'encoder.model.network.4.3.convffn', 'encoder.model.network.4.3.convffn.act', 'gelu_18']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_242
val_3val_244node_Add_244"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.3.convffn.act: torch.nn.modules.activation.GELU/gelu_18: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_18 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_55,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.3', 'encoder.model.network.4.3.convffn', 'encoder.model.network.4.3.convffn.act', 'gelu_18']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_244val_246node_Mul_246"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.3.convffn.act: torch.nn.modules.activation.GELU/gelu_18: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_18 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_55,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.3', 'encoder.model.network.4.3.convffn', 'encoder.model.network.4.3.convffn.act', 'gelu_18']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_55
val_246gelu_18node_gelu_18"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.3.convffn.act: torch.nn.modules.activation.GELU/gelu_18: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_18 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_55,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.3', 'encoder.model.network.4.3.convffn', 'encoder.model.network.4.3.convffn.act', 'gelu_18']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_18
,encoder.model.network.4.3.convffn.fc2.weight
*encoder.model.network.4.3.convffn.fc2.bias	conv2d_56node_conv2d_56"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.3: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.3.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.3.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_56: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_56 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_22, %p_encoder_model_network_4_3_convffn_fc2_weight, %p_encoder_model_network_4_3_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.3', 'encoder.model.network.4.3.convffn', 'encoder.model.network.4.3.convffn.fc2', 'conv2d_56']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ã
%encoder.model.network.4.3.layer_scale
	conv2d_56mul_12node_mul_12"MulJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.3: mobileclip.models.mci.RepMixerBlock/mul_12: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JØ
pkg.torch.onnx.fx_nodeî%mul_12 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_4_3_layer_scale, %clone_23), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.3', 'mul_12']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
À
	conv2d_53
mul_12add_11node_add_11"AddJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.3: mobileclip.models.mci.RepMixerBlock/add_11: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_11 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_53, %mul_12), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.3', 'add_11']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ï
add_11
9encoder.model.network.4.4.token_mixer.reparam_conv.weight
7encoder.model.network.4.4.token_mixer.reparam_conv.bias	conv2d_57node_conv2d_57"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.4.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.4.4.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_57: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J°
pkg.torch.onnx.fx_nodeÜ%conv2d_57 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_11, %p_encoder_model_network_4_4_token_mixer_reparam_conv_weight, %p_encoder_model_network_4_4_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 256), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.4', 'encoder.model.network.4.4.token_mixer', 'encoder.model.network.4.4.token_mixer.reparam_conv', 'conv2d_57']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
õ
	conv2d_57
2encoder.model.network.4.4.convffn.conv.conv.weight
7encoder.model.network.4.4.convffn.conv.conv.weight_bias
getitem_36node_Conv_581"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jπ
	namespace´: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.4.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.4.4.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_12: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J≤
pkg.torch.onnx.fx_nodeó%_native_batch_norm_legit_no_training_12 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_58, %p_encoder_model_network_4_4_convffn_conv_bn_weight, %p_encoder_model_network_4_4_convffn_conv_bn_bias, %b_encoder_model_network_4_4_convffn_conv_bn_running_mean, %b_encoder_model_network_4_4_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jû
pkg.torch.onnx.name_scopesˇ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.4', 'encoder.model.network.4.4.convffn', 'encoder.model.network.4.4.convffn.conv', 'encoder.model.network.4.4.convffn.conv.bn', '_native_batch_norm_legit_no_training_12']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_36
,encoder.model.network.4.4.convffn.fc1.weight
*encoder.model.network.4.4.convffn.fc1.bias	conv2d_59node_conv2d_59"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.4.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_59: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_59 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_36, %p_encoder_model_network_4_4_convffn_fc1_weight, %p_encoder_model_network_4_4_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.4', 'encoder.model.network.4.4.convffn', 'encoder.model.network.4.4.convffn.fc1', 'conv2d_59']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_59
val_0val_257node_Div_257"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.4.convffn.act: torch.nn.modules.activation.GELU/gelu_19: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_19 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_59,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.4', 'encoder.model.network.4.4.convffn', 'encoder.model.network.4.4.convffn.act', 'gelu_19']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_257val_258node_Erf_258"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.4.convffn.act: torch.nn.modules.activation.GELU/gelu_19: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_19 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_59,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.4', 'encoder.model.network.4.4.convffn', 'encoder.model.network.4.4.convffn.act', 'gelu_19']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_258
val_3val_260node_Add_260"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.4.convffn.act: torch.nn.modules.activation.GELU/gelu_19: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_19 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_59,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.4', 'encoder.model.network.4.4.convffn', 'encoder.model.network.4.4.convffn.act', 'gelu_19']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_260val_262node_Mul_262"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.4.convffn.act: torch.nn.modules.activation.GELU/gelu_19: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_19 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_59,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.4', 'encoder.model.network.4.4.convffn', 'encoder.model.network.4.4.convffn.act', 'gelu_19']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_59
val_262gelu_19node_gelu_19"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.4.convffn.act: torch.nn.modules.activation.GELU/gelu_19: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_19 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_59,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.4', 'encoder.model.network.4.4.convffn', 'encoder.model.network.4.4.convffn.act', 'gelu_19']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_19
,encoder.model.network.4.4.convffn.fc2.weight
*encoder.model.network.4.4.convffn.fc2.bias	conv2d_60node_conv2d_60"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.4: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.4.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.4.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_60: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_60 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_24, %p_encoder_model_network_4_4_convffn_fc2_weight, %p_encoder_model_network_4_4_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.4', 'encoder.model.network.4.4.convffn', 'encoder.model.network.4.4.convffn.fc2', 'conv2d_60']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ã
%encoder.model.network.4.4.layer_scale
	conv2d_60mul_13node_mul_13"MulJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.4: mobileclip.models.mci.RepMixerBlock/mul_13: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JØ
pkg.torch.onnx.fx_nodeî%mul_13 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_4_4_layer_scale, %clone_25), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.4', 'mul_13']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
À
	conv2d_57
mul_13add_12node_add_12"AddJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.4: mobileclip.models.mci.RepMixerBlock/add_12: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_12 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_57, %mul_13), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.4', 'add_12']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ï
add_12
9encoder.model.network.4.5.token_mixer.reparam_conv.weight
7encoder.model.network.4.5.token_mixer.reparam_conv.bias	conv2d_61node_conv2d_61"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.5.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.4.5.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_61: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J°
pkg.torch.onnx.fx_nodeÜ%conv2d_61 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_12, %p_encoder_model_network_4_5_token_mixer_reparam_conv_weight, %p_encoder_model_network_4_5_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 256), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.5', 'encoder.model.network.4.5.token_mixer', 'encoder.model.network.4.5.token_mixer.reparam_conv', 'conv2d_61']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
õ
	conv2d_61
2encoder.model.network.4.5.convffn.conv.conv.weight
7encoder.model.network.4.5.convffn.conv.conv.weight_bias
getitem_39node_Conv_583"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jπ
	namespace´: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.5.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.4.5.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_13: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J≤
pkg.torch.onnx.fx_nodeó%_native_batch_norm_legit_no_training_13 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_62, %p_encoder_model_network_4_5_convffn_conv_bn_weight, %p_encoder_model_network_4_5_convffn_conv_bn_bias, %b_encoder_model_network_4_5_convffn_conv_bn_running_mean, %b_encoder_model_network_4_5_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jû
pkg.torch.onnx.name_scopesˇ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.5', 'encoder.model.network.4.5.convffn', 'encoder.model.network.4.5.convffn.conv', 'encoder.model.network.4.5.convffn.conv.bn', '_native_batch_norm_legit_no_training_13']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_39
,encoder.model.network.4.5.convffn.fc1.weight
*encoder.model.network.4.5.convffn.fc1.bias	conv2d_63node_conv2d_63"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.5.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_63: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_63 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_39, %p_encoder_model_network_4_5_convffn_fc1_weight, %p_encoder_model_network_4_5_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.5', 'encoder.model.network.4.5.convffn', 'encoder.model.network.4.5.convffn.fc1', 'conv2d_63']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_63
val_0val_273node_Div_273"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.5.convffn.act: torch.nn.modules.activation.GELU/gelu_20: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_20 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_63,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.5', 'encoder.model.network.4.5.convffn', 'encoder.model.network.4.5.convffn.act', 'gelu_20']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_273val_274node_Erf_274"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.5.convffn.act: torch.nn.modules.activation.GELU/gelu_20: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_20 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_63,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.5', 'encoder.model.network.4.5.convffn', 'encoder.model.network.4.5.convffn.act', 'gelu_20']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_274
val_3val_276node_Add_276"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.5.convffn.act: torch.nn.modules.activation.GELU/gelu_20: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_20 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_63,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.5', 'encoder.model.network.4.5.convffn', 'encoder.model.network.4.5.convffn.act', 'gelu_20']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_276val_278node_Mul_278"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.5.convffn.act: torch.nn.modules.activation.GELU/gelu_20: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_20 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_63,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.5', 'encoder.model.network.4.5.convffn', 'encoder.model.network.4.5.convffn.act', 'gelu_20']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_63
val_278gelu_20node_gelu_20"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.5.convffn.act: torch.nn.modules.activation.GELU/gelu_20: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_20 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_63,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.5', 'encoder.model.network.4.5.convffn', 'encoder.model.network.4.5.convffn.act', 'gelu_20']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_20
,encoder.model.network.4.5.convffn.fc2.weight
*encoder.model.network.4.5.convffn.fc2.bias	conv2d_64node_conv2d_64"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.5: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.5.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.5.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_64: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_64 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_26, %p_encoder_model_network_4_5_convffn_fc2_weight, %p_encoder_model_network_4_5_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.5', 'encoder.model.network.4.5.convffn', 'encoder.model.network.4.5.convffn.fc2', 'conv2d_64']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ã
%encoder.model.network.4.5.layer_scale
	conv2d_64mul_14node_mul_14"MulJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.5: mobileclip.models.mci.RepMixerBlock/mul_14: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JØ
pkg.torch.onnx.fx_nodeî%mul_14 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_4_5_layer_scale, %clone_27), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.5', 'mul_14']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
À
	conv2d_61
mul_14add_13node_add_13"AddJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.5: mobileclip.models.mci.RepMixerBlock/add_13: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_13 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_61, %mul_14), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.5', 'add_13']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ï
add_13
9encoder.model.network.4.6.token_mixer.reparam_conv.weight
7encoder.model.network.4.6.token_mixer.reparam_conv.bias	conv2d_65node_conv2d_65"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.6: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.6.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.4.6.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_65: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J°
pkg.torch.onnx.fx_nodeÜ%conv2d_65 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_13, %p_encoder_model_network_4_6_token_mixer_reparam_conv_weight, %p_encoder_model_network_4_6_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 256), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.6', 'encoder.model.network.4.6.token_mixer', 'encoder.model.network.4.6.token_mixer.reparam_conv', 'conv2d_65']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
õ
	conv2d_65
2encoder.model.network.4.6.convffn.conv.conv.weight
7encoder.model.network.4.6.convffn.conv.conv.weight_bias
getitem_42node_Conv_585"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jπ
	namespace´: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.6: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.6.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.6.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.4.6.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_14: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J≤
pkg.torch.onnx.fx_nodeó%_native_batch_norm_legit_no_training_14 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_66, %p_encoder_model_network_4_6_convffn_conv_bn_weight, %p_encoder_model_network_4_6_convffn_conv_bn_bias, %b_encoder_model_network_4_6_convffn_conv_bn_running_mean, %b_encoder_model_network_4_6_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jû
pkg.torch.onnx.name_scopesˇ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.6', 'encoder.model.network.4.6.convffn', 'encoder.model.network.4.6.convffn.conv', 'encoder.model.network.4.6.convffn.conv.bn', '_native_batch_norm_legit_no_training_14']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_42
,encoder.model.network.4.6.convffn.fc1.weight
*encoder.model.network.4.6.convffn.fc1.bias	conv2d_67node_conv2d_67"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.6: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.6.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.6.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_67: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_67 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_42, %p_encoder_model_network_4_6_convffn_fc1_weight, %p_encoder_model_network_4_6_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.6', 'encoder.model.network.4.6.convffn', 'encoder.model.network.4.6.convffn.fc1', 'conv2d_67']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_67
val_0val_289node_Div_289"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.6: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.6.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.6.convffn.act: torch.nn.modules.activation.GELU/gelu_21: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_21 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_67,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.6', 'encoder.model.network.4.6.convffn', 'encoder.model.network.4.6.convffn.act', 'gelu_21']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_289val_290node_Erf_290"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.6: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.6.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.6.convffn.act: torch.nn.modules.activation.GELU/gelu_21: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_21 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_67,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.6', 'encoder.model.network.4.6.convffn', 'encoder.model.network.4.6.convffn.act', 'gelu_21']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_290
val_3val_292node_Add_292"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.6: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.6.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.6.convffn.act: torch.nn.modules.activation.GELU/gelu_21: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_21 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_67,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.6', 'encoder.model.network.4.6.convffn', 'encoder.model.network.4.6.convffn.act', 'gelu_21']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_292val_294node_Mul_294"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.6: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.6.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.6.convffn.act: torch.nn.modules.activation.GELU/gelu_21: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_21 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_67,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.6', 'encoder.model.network.4.6.convffn', 'encoder.model.network.4.6.convffn.act', 'gelu_21']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_67
val_294gelu_21node_gelu_21"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.6: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.6.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.6.convffn.act: torch.nn.modules.activation.GELU/gelu_21: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_21 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_67,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.6', 'encoder.model.network.4.6.convffn', 'encoder.model.network.4.6.convffn.act', 'gelu_21']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_21
,encoder.model.network.4.6.convffn.fc2.weight
*encoder.model.network.4.6.convffn.fc2.bias	conv2d_68node_conv2d_68"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.6: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.6.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.6.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_68: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_68 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_28, %p_encoder_model_network_4_6_convffn_fc2_weight, %p_encoder_model_network_4_6_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.6', 'encoder.model.network.4.6.convffn', 'encoder.model.network.4.6.convffn.fc2', 'conv2d_68']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ã
%encoder.model.network.4.6.layer_scale
	conv2d_68mul_15node_mul_15"MulJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.6: mobileclip.models.mci.RepMixerBlock/mul_15: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JØ
pkg.torch.onnx.fx_nodeî%mul_15 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_4_6_layer_scale, %clone_29), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.6', 'mul_15']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
À
	conv2d_65
mul_15add_14node_add_14"AddJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.6: mobileclip.models.mci.RepMixerBlock/add_14: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_14 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_65, %mul_15), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.6', 'add_14']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ï
add_14
9encoder.model.network.4.7.token_mixer.reparam_conv.weight
7encoder.model.network.4.7.token_mixer.reparam_conv.bias	conv2d_69node_conv2d_69"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.7: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.7.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.4.7.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_69: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J°
pkg.torch.onnx.fx_nodeÜ%conv2d_69 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_14, %p_encoder_model_network_4_7_token_mixer_reparam_conv_weight, %p_encoder_model_network_4_7_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 256), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.7', 'encoder.model.network.4.7.token_mixer', 'encoder.model.network.4.7.token_mixer.reparam_conv', 'conv2d_69']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
õ
	conv2d_69
2encoder.model.network.4.7.convffn.conv.conv.weight
7encoder.model.network.4.7.convffn.conv.conv.weight_bias
getitem_45node_Conv_587"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jπ
	namespace´: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.7: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.7.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.7.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.4.7.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_15: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J≤
pkg.torch.onnx.fx_nodeó%_native_batch_norm_legit_no_training_15 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_70, %p_encoder_model_network_4_7_convffn_conv_bn_weight, %p_encoder_model_network_4_7_convffn_conv_bn_bias, %b_encoder_model_network_4_7_convffn_conv_bn_running_mean, %b_encoder_model_network_4_7_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jû
pkg.torch.onnx.name_scopesˇ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.7', 'encoder.model.network.4.7.convffn', 'encoder.model.network.4.7.convffn.conv', 'encoder.model.network.4.7.convffn.conv.bn', '_native_batch_norm_legit_no_training_15']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_45
,encoder.model.network.4.7.convffn.fc1.weight
*encoder.model.network.4.7.convffn.fc1.bias	conv2d_71node_conv2d_71"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.7: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.7.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.7.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_71: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_71 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_45, %p_encoder_model_network_4_7_convffn_fc1_weight, %p_encoder_model_network_4_7_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.7', 'encoder.model.network.4.7.convffn', 'encoder.model.network.4.7.convffn.fc1', 'conv2d_71']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_71
val_0val_305node_Div_305"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.7: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.7.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.7.convffn.act: torch.nn.modules.activation.GELU/gelu_22: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_22 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_71,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.7', 'encoder.model.network.4.7.convffn', 'encoder.model.network.4.7.convffn.act', 'gelu_22']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_305val_306node_Erf_306"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.7: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.7.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.7.convffn.act: torch.nn.modules.activation.GELU/gelu_22: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_22 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_71,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.7', 'encoder.model.network.4.7.convffn', 'encoder.model.network.4.7.convffn.act', 'gelu_22']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_306
val_3val_308node_Add_308"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.7: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.7.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.7.convffn.act: torch.nn.modules.activation.GELU/gelu_22: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_22 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_71,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.7', 'encoder.model.network.4.7.convffn', 'encoder.model.network.4.7.convffn.act', 'gelu_22']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_308val_310node_Mul_310"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.7: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.7.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.7.convffn.act: torch.nn.modules.activation.GELU/gelu_22: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_22 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_71,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.7', 'encoder.model.network.4.7.convffn', 'encoder.model.network.4.7.convffn.act', 'gelu_22']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_71
val_310gelu_22node_gelu_22"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.7: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.7.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.7.convffn.act: torch.nn.modules.activation.GELU/gelu_22: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_22 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_71,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.7', 'encoder.model.network.4.7.convffn', 'encoder.model.network.4.7.convffn.act', 'gelu_22']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_22
,encoder.model.network.4.7.convffn.fc2.weight
*encoder.model.network.4.7.convffn.fc2.bias	conv2d_72node_conv2d_72"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.7: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.7.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.7.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_72: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_72 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_30, %p_encoder_model_network_4_7_convffn_fc2_weight, %p_encoder_model_network_4_7_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.7', 'encoder.model.network.4.7.convffn', 'encoder.model.network.4.7.convffn.fc2', 'conv2d_72']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ã
%encoder.model.network.4.7.layer_scale
	conv2d_72mul_16node_mul_16"MulJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.7: mobileclip.models.mci.RepMixerBlock/mul_16: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JØ
pkg.torch.onnx.fx_nodeî%mul_16 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_4_7_layer_scale, %clone_31), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.7', 'mul_16']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
À
	conv2d_69
mul_16add_15node_add_15"AddJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.7: mobileclip.models.mci.RepMixerBlock/add_15: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_15 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_69, %mul_16), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.7', 'add_15']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ï
add_15
9encoder.model.network.4.8.token_mixer.reparam_conv.weight
7encoder.model.network.4.8.token_mixer.reparam_conv.bias	conv2d_73node_conv2d_73"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.8: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.8.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.4.8.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_73: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J°
pkg.torch.onnx.fx_nodeÜ%conv2d_73 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_15, %p_encoder_model_network_4_8_token_mixer_reparam_conv_weight, %p_encoder_model_network_4_8_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 256), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.8', 'encoder.model.network.4.8.token_mixer', 'encoder.model.network.4.8.token_mixer.reparam_conv', 'conv2d_73']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
õ
	conv2d_73
2encoder.model.network.4.8.convffn.conv.conv.weight
7encoder.model.network.4.8.convffn.conv.conv.weight_bias
getitem_48node_Conv_589"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jπ
	namespace´: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.8: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.8.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.8.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.4.8.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_16: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J≤
pkg.torch.onnx.fx_nodeó%_native_batch_norm_legit_no_training_16 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_74, %p_encoder_model_network_4_8_convffn_conv_bn_weight, %p_encoder_model_network_4_8_convffn_conv_bn_bias, %b_encoder_model_network_4_8_convffn_conv_bn_running_mean, %b_encoder_model_network_4_8_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jû
pkg.torch.onnx.name_scopesˇ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.8', 'encoder.model.network.4.8.convffn', 'encoder.model.network.4.8.convffn.conv', 'encoder.model.network.4.8.convffn.conv.bn', '_native_batch_norm_legit_no_training_16']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_48
,encoder.model.network.4.8.convffn.fc1.weight
*encoder.model.network.4.8.convffn.fc1.bias	conv2d_75node_conv2d_75"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.8: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.8.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.8.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_75: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_75 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_48, %p_encoder_model_network_4_8_convffn_fc1_weight, %p_encoder_model_network_4_8_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.8', 'encoder.model.network.4.8.convffn', 'encoder.model.network.4.8.convffn.fc1', 'conv2d_75']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_75
val_0val_321node_Div_321"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.8: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.8.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.8.convffn.act: torch.nn.modules.activation.GELU/gelu_23: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_23 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_75,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.8', 'encoder.model.network.4.8.convffn', 'encoder.model.network.4.8.convffn.act', 'gelu_23']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_321val_322node_Erf_322"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.8: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.8.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.8.convffn.act: torch.nn.modules.activation.GELU/gelu_23: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_23 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_75,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.8', 'encoder.model.network.4.8.convffn', 'encoder.model.network.4.8.convffn.act', 'gelu_23']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_322
val_3val_324node_Add_324"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.8: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.8.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.8.convffn.act: torch.nn.modules.activation.GELU/gelu_23: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_23 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_75,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.8', 'encoder.model.network.4.8.convffn', 'encoder.model.network.4.8.convffn.act', 'gelu_23']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_324val_326node_Mul_326"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.8: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.8.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.8.convffn.act: torch.nn.modules.activation.GELU/gelu_23: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_23 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_75,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.8', 'encoder.model.network.4.8.convffn', 'encoder.model.network.4.8.convffn.act', 'gelu_23']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_75
val_326gelu_23node_gelu_23"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.8: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.8.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.8.convffn.act: torch.nn.modules.activation.GELU/gelu_23: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_23 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_75,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.8', 'encoder.model.network.4.8.convffn', 'encoder.model.network.4.8.convffn.act', 'gelu_23']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_23
,encoder.model.network.4.8.convffn.fc2.weight
*encoder.model.network.4.8.convffn.fc2.bias	conv2d_76node_conv2d_76"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.8: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.8.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.8.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_76: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_76 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_32, %p_encoder_model_network_4_8_convffn_fc2_weight, %p_encoder_model_network_4_8_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.8', 'encoder.model.network.4.8.convffn', 'encoder.model.network.4.8.convffn.fc2', 'conv2d_76']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ã
%encoder.model.network.4.8.layer_scale
	conv2d_76mul_17node_mul_17"MulJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.8: mobileclip.models.mci.RepMixerBlock/mul_17: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JØ
pkg.torch.onnx.fx_nodeî%mul_17 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_4_8_layer_scale, %clone_33), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.8', 'mul_17']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
À
	conv2d_73
mul_17add_16node_add_16"AddJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.8: mobileclip.models.mci.RepMixerBlock/add_16: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_16 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_73, %mul_17), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.8', 'add_16']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ï
add_16
9encoder.model.network.4.9.token_mixer.reparam_conv.weight
7encoder.model.network.4.9.token_mixer.reparam_conv.bias	conv2d_77node_conv2d_77"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.9: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.9.token_mixer: mobileclip.models.mci.RepMixer/encoder.model.network.4.9.token_mixer.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_77: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.RepMixer', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J°
pkg.torch.onnx.fx_nodeÜ%conv2d_77 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_16, %p_encoder_model_network_4_9_token_mixer_reparam_conv_weight, %p_encoder_model_network_4_9_token_mixer_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 256), kwargs = {})J„
pkg.torch.onnx.name_scopesƒ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.9', 'encoder.model.network.4.9.token_mixer', 'encoder.model.network.4.9.token_mixer.reparam_conv', 'conv2d_77']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 568, in forward
    x = self.token_mixer(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 276, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
õ
	conv2d_77
2encoder.model.network.4.9.convffn.conv.conv.weight
7encoder.model.network.4.9.convffn.conv.conv.weight_bias
getitem_51node_Conv_591"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jπ
	namespace´: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.9: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.9.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.9.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.4.9.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_17: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˝
pkg.torch.onnx.class_hierarchy⁄['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J≤
pkg.torch.onnx.fx_nodeó%_native_batch_norm_legit_no_training_17 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_78, %p_encoder_model_network_4_9_convffn_conv_bn_weight, %p_encoder_model_network_4_9_convffn_conv_bn_bias, %b_encoder_model_network_4_9_convffn_conv_bn_running_mean, %b_encoder_model_network_4_9_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jû
pkg.torch.onnx.name_scopesˇ['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.9', 'encoder.model.network.4.9.convffn', 'encoder.model.network.4.9.convffn.conv', 'encoder.model.network.4.9.convffn.conv.bn', '_native_batch_norm_legit_no_training_17']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
¡

getitem_51
,encoder.model.network.4.9.convffn.fc1.weight
*encoder.model.network.4.9.convffn.fc1.bias	conv2d_79node_conv2d_79"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.9: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.9.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.9.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_79: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_79 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_51, %p_encoder_model_network_4_9_convffn_fc1_weight, %p_encoder_model_network_4_9_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.9', 'encoder.model.network.4.9.convffn', 'encoder.model.network.4.9.convffn.fc1', 'conv2d_79']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
•
	conv2d_79
val_0val_337node_Div_337"DivJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.9: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.9.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.9.convffn.act: torch.nn.modules.activation.GELU/gelu_24: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_24 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_79,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.9', 'encoder.model.network.4.9.convffn', 'encoder.model.network.4.9.convffn.act', 'gelu_24']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ú
val_337val_338node_Erf_338"ErfJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.9: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.9.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.9.convffn.act: torch.nn.modules.activation.GELU/gelu_24: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_24 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_79,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.9', 'encoder.model.network.4.9.convffn', 'encoder.model.network.4.9.convffn.act', 'gelu_24']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_338
val_3val_340node_Add_340"AddJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.9: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.9.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.9.convffn.act: torch.nn.modules.activation.GELU/gelu_24: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_24 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_79,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.9', 'encoder.model.network.4.9.convffn', 'encoder.model.network.4.9.convffn.act', 'gelu_24']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
£
val_5
val_340val_342node_Mul_342"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.9: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.9.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.9.convffn.act: torch.nn.modules.activation.GELU/gelu_24: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_24 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_79,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.9', 'encoder.model.network.4.9.convffn', 'encoder.model.network.4.9.convffn.act', 'gelu_24']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
	conv2d_79
val_342gelu_24node_gelu_24"MulJ°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.9: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.9.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.9.convffn.act: torch.nn.modules.activation.GELU/gelu_24: aten.gelu.defaultJÆ
pkg.torch.onnx.class_hierarchyã['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_24 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_79,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.9', 'encoder.model.network.4.9.convffn', 'encoder.model.network.4.9.convffn.act', 'gelu_24']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
º
gelu_24
,encoder.model.network.4.9.convffn.fc2.weight
*encoder.model.network.4.9.convffn.fc2.bias	conv2d_80node_conv2d_80"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J°
	namespaceì: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.9: mobileclip.models.mci.RepMixerBlock/encoder.model.network.4.9.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.4.9.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_80: aten.conv2d.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_80 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_34, %p_encoder_model_network_4_9_convffn_fc2_weight, %p_encoder_model_network_4_9_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.9', 'encoder.model.network.4.9.convffn', 'encoder.model.network.4.9.convffn.fc2', 'conv2d_80']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ã
%encoder.model.network.4.9.layer_scale
	conv2d_80mul_18node_mul_18"MulJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.9: mobileclip.models.mci.RepMixerBlock/mul_18: aten.mul.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.mul.Tensor']JØ
pkg.torch.onnx.fx_nodeî%mul_18 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_4_9_layer_scale, %clone_35), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.9', 'mul_18']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
À
	conv2d_77
mul_18add_17node_add_17"AddJï
	namespaceá: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.4: torch.nn.modules.container.Sequential/encoder.model.network.4.9: mobileclip.models.mci.RepMixerBlock/add_17: aten.add.TensorJÁ
pkg.torch.onnx.class_hierarchyƒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.RepMixerBlock', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_17 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_77, %mul_18), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.4', 'encoder.model.network.4.9', 'add_17']JÖ
pkg.torch.onnx.stack_traceÊFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 569, in forward
    x = x + self.drop_path(self.layer_scale * self.convffn(x))
ê
add_17
1encoder.model.network.5.proj.0.lkb_reparam.weight
/encoder.model.network.5.proj.0.lkb_reparam.bias	conv2d_81node_conv2d_81"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jæ
	namespace∞: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.5.proj.0.lkb_reparam: torch.nn.modules.conv.Conv2d/conv2d_81: aten.conv2d.defaultJƒ
pkg.torch.onnx.class_hierarchy°['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']Jë
pkg.torch.onnx.fx_nodeˆ%conv2d_81 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_17, %p_encoder_model_network_5_proj_0_lkb_reparam_weight, %p_encoder_model_network_5_proj_0_lkb_reparam_bias, [2, 2], [3, 3], [1, 1], 256), kwargs = {})J◊
pkg.torch.onnx.name_scopes∏['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.0', 'encoder.model.network.5.proj.0.lkb_reparam', 'conv2d_81']J´
pkg.torch.onnx.stack_traceåFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 92, in forward
    out = self.lkb_reparam(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
Î
	conv2d_81
val_168mean_1node_mean_1"
ReduceMean*
keepdims†*
noop_with_empty_axes †J≥
	namespace•: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.5.proj.0.se: timm.layers.squeeze_excite.SEModule/mean_1: aten.mean.dimJ≈
pkg.torch.onnx.class_hierarchy¢['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'timm.layers.squeeze_excite.SEModule', 'aten.mean.dim']Jë
pkg.torch.onnx.fx_nodew%mean_1 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%conv2d_81, [2, 3], True), kwargs = {})JÀ
pkg.torch.onnx.name_scopes¨['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.0', 'encoder.model.network.5.proj.0.se', 'mean_1']J•
pkg.torch.onnx.stack_traceÜFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\squeeze_excite.py", line 56, in forward
    x_se = x.mean((2, 3), keepdim=True)
Ö
mean_1
,encoder.model.network.5.proj.0.se.fc1.weight
*encoder.model.network.5.proj.0.se.fc1.bias	conv2d_82node_conv2d_82"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÄ
	namespaceÚ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.5.proj.0.se: timm.layers.squeeze_excite.SEModule/encoder.model.network.5.proj.0.se.fc1: torch.nn.modules.conv.Conv2d/conv2d_82: aten.conv2d.defaultJÎ
pkg.torch.onnx.class_hierarchy»['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'timm.layers.squeeze_excite.SEModule', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÍ
pkg.torch.onnx.fx_nodeœ%conv2d_82 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%mean_1, %p_encoder_model_network_5_proj_0_se_fc1_weight, %p_encoder_model_network_5_proj_0_se_fc1_bias), kwargs = {})J˜
pkg.torch.onnx.name_scopesÿ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.0', 'encoder.model.network.5.proj.0.se', 'encoder.model.network.5.proj.0.se.fc1', 'conv2d_82']J…
pkg.torch.onnx.stack_trace™File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\squeeze_excite.py", line 60, in forward
    x_se = self.fc1(x_se)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
Á
	conv2d_82relu_1node_relu_1"ReluJˇ
	namespaceÒ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.5.proj.0.se: timm.layers.squeeze_excite.SEModule/encoder.model.network.5.proj.0.se.act: torch.nn.modules.activation.ReLU/relu_1: aten.relu.defaultJÌ
pkg.torch.onnx.class_hierarchy ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'timm.layers.squeeze_excite.SEModule', 'torch.nn.modules.activation.ReLU', 'aten.relu.default']Jà
pkg.torch.onnx.fx_noden%relu_1 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%conv2d_82,), kwargs = {})JÙ
pkg.torch.onnx.name_scopes’['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.0', 'encoder.model.network.5.proj.0.se', 'encoder.model.network.5.proj.0.se.act', 'relu_1']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\squeeze_excite.py", line 61, in forward
    x_se = self.act(self.bn(x_se))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 144, in forward
    return F.relu(input, inplace=self.inplace)
Ö
relu_1
,encoder.model.network.5.proj.0.se.fc2.weight
*encoder.model.network.5.proj.0.se.fc2.bias	conv2d_83node_conv2d_83"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÄ
	namespaceÚ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.5.proj.0.se: timm.layers.squeeze_excite.SEModule/encoder.model.network.5.proj.0.se.fc2: torch.nn.modules.conv.Conv2d/conv2d_83: aten.conv2d.defaultJÎ
pkg.torch.onnx.class_hierarchy»['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'timm.layers.squeeze_excite.SEModule', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÍ
pkg.torch.onnx.fx_nodeœ%conv2d_83 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%relu_1, %p_encoder_model_network_5_proj_0_se_fc2_weight, %p_encoder_model_network_5_proj_0_se_fc2_bias), kwargs = {})J˜
pkg.torch.onnx.name_scopesÿ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.0', 'encoder.model.network.5.proj.0.se', 'encoder.model.network.5.proj.0.se.fc2', 'conv2d_83']J…
pkg.torch.onnx.stack_trace™File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\squeeze_excite.py", line 62, in forward
    x_se = self.fc2(x_se)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
É
	conv2d_83	sigmoid_1node_sigmoid_1"SigmoidJÖ
	namespace˜: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.5.proj.0.se: timm.layers.squeeze_excite.SEModule/encoder.model.network.5.proj.0.se.gate: timm.layers.activations.Sigmoid/sigmoid_1: aten.sigmoid.defaultJÔ
pkg.torch.onnx.class_hierarchyÃ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'timm.layers.squeeze_excite.SEModule', 'timm.layers.activations.Sigmoid', 'aten.sigmoid.default']Jé
pkg.torch.onnx.fx_nodet%sigmoid_1 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%conv2d_83,), kwargs = {})J¯
pkg.torch.onnx.name_scopesŸ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.0', 'encoder.model.network.5.proj.0.se', 'encoder.model.network.5.proj.0.se.gate', 'sigmoid_1']JÀ
pkg.torch.onnx.stack_trace¨File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\squeeze_excite.py", line 63, in forward
    return x * self.gate(x_se)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\activations.py", line 57, in forward
    return x.sigmoid_() if self.inplace else x.sigmoid()
≥
	conv2d_81
	sigmoid_1mul_19node_mul_19"MulJµ
	namespaceß: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.5.proj.0.se: timm.layers.squeeze_excite.SEModule/mul_19: aten.mul.TensorJ«
pkg.torch.onnx.class_hierarchy§['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'timm.layers.squeeze_excite.SEModule', 'aten.mul.Tensor']Jë
pkg.torch.onnx.fx_nodew%mul_19 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%conv2d_81, %sigmoid_1), kwargs = {})JÀ
pkg.torch.onnx.name_scopes¨['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.0', 'encoder.model.network.5.proj.0.se', 'mul_19']Jú
pkg.torch.onnx.stack_trace˝File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\timm\layers\squeeze_excite.py", line 63, in forward
    return x * self.gate(x_se)
Õ
mul_19
val_0val_346node_Div_346"DivJΩ
	namespaceØ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.5.proj.0.activation: torch.nn.modules.activation.GELU/gelu_25: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÜ
pkg.torch.onnx.fx_nodel%gelu_25 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_19,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.0', 'encoder.model.network.5.proj.0.activation', 'gelu_25']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
«
val_346val_347node_Erf_347"ErfJΩ
	namespaceØ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.5.proj.0.activation: torch.nn.modules.activation.GELU/gelu_25: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÜ
pkg.torch.onnx.fx_nodel%gelu_25 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_19,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.0', 'encoder.model.network.5.proj.0.activation', 'gelu_25']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Œ
val_347
val_3val_349node_Add_349"AddJΩ
	namespaceØ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.5.proj.0.activation: torch.nn.modules.activation.GELU/gelu_25: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÜ
pkg.torch.onnx.fx_nodel%gelu_25 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_19,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.0', 'encoder.model.network.5.proj.0.activation', 'gelu_25']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Œ
val_5
val_349val_351node_Mul_351"MulJΩ
	namespaceØ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.5.proj.0.activation: torch.nn.modules.activation.GELU/gelu_25: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÜ
pkg.torch.onnx.fx_nodel%gelu_25 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_19,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.0', 'encoder.model.network.5.proj.0.activation', 'gelu_25']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
œ
mul_19
val_351gelu_25node_gelu_25"MulJΩ
	namespaceØ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.0: mobileclip.modules.image.replknet.ReparamLargeKernelConv/encoder.model.network.5.proj.0.activation: torch.nn.modules.activation.GELU/gelu_25: aten.gelu.defaultJ∆
pkg.torch.onnx.class_hierarchy£['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.image.replknet.ReparamLargeKernelConv', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÜ
pkg.torch.onnx.fx_nodel%gelu_25 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_19,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.0', 'encoder.model.network.5.proj.0.activation', 'gelu_25']J∂
pkg.torch.onnx.stack_traceóFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\replknet.py", line 98, in forward
    return self.activation(self.se(out))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ç
gelu_25
2encoder.model.network.5.proj.1.reparam_conv.weight
0encoder.model.network.5.proj.1.reparam_conv.bias	conv2d_84node_conv2d_84"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †Jπ
	namespace´: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.5.proj.1.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_84: aten.conv2d.defaultJæ
pkg.torch.onnx.class_hierarchyõ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J˜
pkg.torch.onnx.fx_node‹%conv2d_84 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%gelu_25, %p_encoder_model_network_5_proj_1_reparam_conv_weight, %p_encoder_model_network_5_proj_1_reparam_conv_bias), kwargs = {})Jÿ
pkg.torch.onnx.name_scopesπ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.1', 'encoder.model.network.5.proj.1.reparam_conv', 'conv2d_84']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
€
	conv2d_84
val_0val_353node_Div_353"DivJ∑
	namespace©: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.5.proj.1.activation: torch.nn.modules.activation.GELU/gelu_26: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_26 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_84,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.1', 'encoder.model.network.5.proj.1.activation', 'gelu_26']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
“
val_353val_354node_Erf_354"ErfJ∑
	namespace©: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.5.proj.1.activation: torch.nn.modules.activation.GELU/gelu_26: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_26 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_84,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.1', 'encoder.model.network.5.proj.1.activation', 'gelu_26']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Ÿ
val_354
val_3val_356node_Add_356"AddJ∑
	namespace©: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.5.proj.1.activation: torch.nn.modules.activation.GELU/gelu_26: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_26 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_84,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.1', 'encoder.model.network.5.proj.1.activation', 'gelu_26']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Ÿ
val_5
val_356val_358node_Mul_358"MulJ∑
	namespace©: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.5.proj.1.activation: torch.nn.modules.activation.GELU/gelu_26: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_26 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_84,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.1', 'encoder.model.network.5.proj.1.activation', 'gelu_26']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
›
	conv2d_84
val_358gelu_26node_gelu_26"MulJ∑
	namespace©: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.5: mobileclip.models.mci.PatchEmbed/encoder.model.network.5.proj: torch.nn.modules.container.Sequential/encoder.model.network.5.proj.1: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.network.5.proj.1.activation: torch.nn.modules.activation.GELU/gelu_26: aten.gelu.defaultJ¿
pkg.torch.onnx.class_hierarchyù['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.PatchEmbed', 'torch.nn.modules.container.Sequential', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_26 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_84,), kwargs = {})J‘
pkg.torch.onnx.name_scopesµ['', 'encoder', 'encoder.model', 'encoder.model.network.5', 'encoder.model.network.5.proj', 'encoder.model.network.5.proj.1', 'encoder.model.network.5.proj.1.activation', 'gelu_26']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 206, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
˜
gelu_26
+encoder.model.network.6.reparam_conv.weight
)encoder.model.network.6.reparam_conv.bias	conv2d_85node_conv2d_85"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†Jó
	namespaceâ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.6: mobileclip.models.mci.RepCPE/encoder.model.network.6.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_85: aten.conv2d.defaultJ€
pkg.torch.onnx.class_hierarchy∏['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.models.mci.RepCPE', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÜ
pkg.torch.onnx.fx_nodeÎ%conv2d_85 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%gelu_26, %p_encoder_model_network_6_reparam_conv_weight, %p_encoder_model_network_6_reparam_conv_bias, [1, 1], [3, 3], [1, 1], 512), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'encoder', 'encoder.model', 'encoder.model.network.6', 'encoder.model.network.6.reparam_conv', 'conv2d_85']JÅ
pkg.torch.onnx.stack_trace‚File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 452, in forward
    x = self.reparam_conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
˛
	conv2d_85
%encoder.model.network.7.0.norm.weight
#encoder.model.network.7.0.norm.bias
+encoder.model.network.7.0.norm.running_mean
*encoder.model.network.7.0.norm.running_var
getitem_54/node__native_batch_norm_legit_no_training_18__0"BatchNormalization*
epsilon¨≈'7†*
momentumfff?†J†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.norm: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_18: aten._native_batch_norm_legit_no_training.defaultJ¥
pkg.torch.onnx.class_hierarchyë['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']JÜ
pkg.torch.onnx.fx_nodeÎ%_native_batch_norm_legit_no_training_18 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_85, %p_encoder_model_network_7_0_norm_weight, %p_encoder_model_network_7_0_norm_bias, %b_encoder_model_network_7_0_norm_running_mean, %b_encoder_model_network_7_0_norm_running_var, 0.1, 1e-05), kwargs = {})Jƒ
pkg.torch.onnx.name_scopes•['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.norm', '_native_batch_norm_legit_no_training_18']J©
pkg.torch.onnx.stack_traceäFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
º

getitem_54
val_367view	node_view"Reshape*
	allowzero†Jÿ
	namespace : __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/view: aten.view.defaultJà
pkg.torch.onnx.class_hierarchyÂ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.view.default']Jî
pkg.torch.onnx.fx_nodez%view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%getitem_54, [1, 512, 64]), kwargs = {})J®
pkg.torch.onnx.name_scopesâ['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'view']J–
pkg.torch.onnx.stack_trace±File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 132, in forward
    x = torch.flatten(x, start_dim=2).transpose(-2, -1)  # (B, N, C)
æ
view	transposenode_transpose"	Transpose*
perm@ @@†Jﬁ
	namespace–: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/transpose: aten.transpose.intJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.transpose.int']Jé
pkg.torch.onnx.fx_nodet%transpose : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view, -2, -1), kwargs = {})J≠
pkg.torch.onnx.name_scopesé['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'transpose']J–
pkg.torch.onnx.stack_trace±File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 132, in forward
    x = torch.flatten(x, start_dim=2).transpose(-2, -1)  # (B, N, C)
Ì
	transpose
val_368linearnode_linear"MatMulJ¶
	namespaceò: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/encoder.model.network.7.0.token_mixer.qkv: torch.nn.modules.linear.Linear/linear: aten.linear.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'torch.nn.modules.linear.Linear', 'aten.linear.default']Jø
pkg.torch.onnx.fx_node§%linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose, %p_encoder_model_network_7_0_token_mixer_qkv_weight), kwargs = {})J◊
pkg.torch.onnx.name_scopes∏['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'encoder.model.network.7.0.token_mixer.qkv', 'linear']J≈
pkg.torch.onnx.stack_trace¶File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 134, in forward
    self.qkv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
¥
linear
val_375view_1node_view_1"Reshape*
	allowzero†J⁄
	namespaceÃ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/view_1: aten.view.defaultJà
pkg.torch.onnx.class_hierarchyÂ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.view.default']Jò
pkg.torch.onnx.fx_node~%view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear, [1, 64, 3, 16, 32]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'view_1']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 135, in forward
    .reshape(B, N, 3, self.num_heads, self.head_dim)
¢
view_1permutenode_permute"	Transpose*
perm@@ @@@†Jﬁ
	namespace–: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/permute: aten.permute.defaultJã
pkg.torch.onnx.class_hierarchyË['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.permute.default']Jô
pkg.torch.onnx.fx_node%permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_1, [2, 0, 3, 1, 4]), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'permute']Jß
pkg.torch.onnx.stack_traceàFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 136, in forward
    .permute(2, 0, 3, 1, 4)
Ω
permute
val_376
val_377
val_376val_379node_Slice_379"SliceJÿ
	namespace : __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/unbind: aten.unbind.intJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.unbind.int']JÑ
pkg.torch.onnx.fx_nodej%unbind : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute,), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'unbind']Jﬁ
pkg.torch.onnx.stack_traceøFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 138, in forward
    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
∞
val_379
val_376
getitem_57node_unbind__0"SqueezeJÿ
	namespace : __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/unbind: aten.unbind.intJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.unbind.int']JÑ
pkg.torch.onnx.fx_nodej%unbind : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute,), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'unbind']Jﬁ
pkg.torch.onnx.stack_traceøFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 138, in forward
    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
Ω
permute
val_377
val_382
val_376val_384node_Slice_384"SliceJÿ
	namespace : __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/unbind: aten.unbind.intJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.unbind.int']JÑ
pkg.torch.onnx.fx_nodej%unbind : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute,), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'unbind']Jﬁ
pkg.torch.onnx.stack_traceøFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 138, in forward
    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
∞
val_384
val_376
getitem_58node_unbind__1"SqueezeJÿ
	namespace : __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/unbind: aten.unbind.intJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.unbind.int']JÑ
pkg.torch.onnx.fx_nodej%unbind : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute,), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'unbind']Jﬁ
pkg.torch.onnx.stack_traceøFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 138, in forward
    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
Ω
permute
val_382
val_386
val_376val_388node_Slice_388"SliceJÿ
	namespace : __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/unbind: aten.unbind.intJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.unbind.int']JÑ
pkg.torch.onnx.fx_nodej%unbind : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute,), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'unbind']Jﬁ
pkg.torch.onnx.stack_traceøFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 138, in forward
    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
∞
val_388
val_376
getitem_59node_unbind__2"SqueezeJÿ
	namespace : __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/unbind: aten.unbind.intJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.unbind.int']JÑ
pkg.torch.onnx.fx_nodej%unbind : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute,), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'unbind']Jﬁ
pkg.torch.onnx.stack_traceøFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 138, in forward
    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
û

getitem_57
val_389mul_20node_mul_20"MulJÿ
	namespace : __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/mul_20: aten.mul.TensorJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.mul.Tensor']Jõ
pkg.torch.onnx.fx_nodeÄ%mul_20 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_57, 0.1767766952966369), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'mul_20']JΩ
pkg.torch.onnx.stack_traceûFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 141, in forward
    attn = (q * self.scale) @ k.transpose(-2, -1)
√

getitem_58transpose_1node_transpose_1"	Transpose*
perm@ @@@†J‡
	namespace“: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/transpose_1: aten.transpose.intJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.transpose.int']Jñ
pkg.torch.onnx.fx_node|%transpose_1 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%getitem_58, -2, -1), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'transpose_1']JΩ
pkg.torch.onnx.stack_traceûFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 141, in forward
    attn = (q * self.scale) @ k.transpose(-2, -1)
¢
mul_20
transpose_1matmulnode_matmul"MatMulJ‹
	namespaceŒ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/matmul: aten.matmul.defaultJä
pkg.torch.onnx.class_hierarchyÁ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.matmul.default']Jî
pkg.torch.onnx.fx_nodez%matmul : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%mul_20, %transpose_1), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'matmul']JΩ
pkg.torch.onnx.stack_traceûFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 141, in forward
    attn = (q * self.scale) @ k.transpose(-2, -1)
å
matmulsoftmaxnode_softmax"Softmax*
axisˇˇˇˇˇˇˇˇˇ†J⁄
	namespaceÃ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/softmax: aten.softmax.intJá
pkg.torch.onnx.class_hierarchy‰['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.softmax.int']Jà
pkg.torch.onnx.fx_noden%softmax : [num_users=1] = call_function[target=torch.ops.aten.softmax.int](args = (%matmul, -1), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'softmax']J´
pkg.torch.onnx.stack_traceåFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 142, in forward
    attn = attn.softmax(dim=-1)
Ø
softmax

getitem_59matmul_1node_matmul_1"MatMulJﬁ
	namespace–: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/matmul_1: aten.matmul.defaultJä
pkg.torch.onnx.class_hierarchyÁ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.matmul.default']Jó
pkg.torch.onnx.fx_node}%matmul_1 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%clone_36, %getitem_59), kwargs = {})J¨
pkg.torch.onnx.name_scopesç['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'matmul_1']Jø
pkg.torch.onnx.stack_trace†File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 145, in forward
    x = (attn @ v).transpose(1, 2).reshape(B, N, C)
ø
matmul_1transpose_2node_transpose_2"	Transpose*
perm@ @@@†J‡
	namespace“: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/transpose_2: aten.transpose.intJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_2 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%matmul_1, 1, 2), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'transpose_2']Jø
pkg.torch.onnx.stack_trace†File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 145, in forward
    x = (attn @ v).transpose(1, 2).reshape(B, N, C)
Î
transpose_2
val_394_unsafe_viewnode__unsafe_view"Reshape*
	allowzero†JË
	namespace⁄: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/_unsafe_view: aten._unsafe_view.defaultJê
pkg.torch.onnx.class_hierarchyÌ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten._unsafe_view.default']J£
pkg.torch.onnx.fx_nodeà%_unsafe_view : [num_users=1] = call_function[target=torch.ops.aten._unsafe_view.default](args = (%clone_37, [1, 64, 512]), kwargs = {})J∞
pkg.torch.onnx.name_scopesë['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', '_unsafe_view']Jø
pkg.torch.onnx.stack_trace†File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 145, in forward
    x = (attn @ v).transpose(1, 2).reshape(B, N, C)
∫
_unsafe_view
val_395val_396node_MatMul_396"MatMulJ©
	namespaceõ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/encoder.model.network.7.0.token_mixer.proj: torch.nn.modules.linear.Linear/linear_1: aten.linear.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J˘
pkg.torch.onnx.fx_nodeﬁ%linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_unsafe_view, %p_encoder_model_network_7_0_token_mixer_proj_weight, %p_encoder_model_network_7_0_token_mixer_proj_bias), kwargs = {})J⁄
pkg.torch.onnx.name_scopesª['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'encoder.model.network.7.0.token_mixer.proj', 'linear_1']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 146, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
Ÿ
val_396
/encoder.model.network.7.0.token_mixer.proj.biaslinear_1node_linear_1"AddJ©
	namespaceõ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/encoder.model.network.7.0.token_mixer.proj: torch.nn.modules.linear.Linear/linear_1: aten.linear.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J˘
pkg.torch.onnx.fx_nodeﬁ%linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_unsafe_view, %p_encoder_model_network_7_0_token_mixer_proj_weight, %p_encoder_model_network_7_0_token_mixer_proj_bias), kwargs = {})J⁄
pkg.torch.onnx.name_scopesª['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'encoder.model.network.7.0.token_mixer.proj', 'linear_1']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 146, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
ª
linear_1transpose_3node_transpose_3"	Transpose*
perm@ @@†J‡
	namespace“: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/transpose_3: aten.transpose.intJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.transpose.int']Jî
pkg.torch.onnx.fx_nodez%transpose_3 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%clone_38, -2, -1), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'transpose_3']Jª
pkg.torch.onnx.stack_traceúFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 149, in forward
    x = x.transpose(-2, -1).reshape(B, C, H, W)
µ
transpose_3
val_402view_2node_view_2"Reshape*
	allowzero†J⁄
	namespaceÃ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.token_mixer: mobileclip.models.mci.MHSA/view_2: aten.view.defaultJà
pkg.torch.onnx.class_hierarchyÂ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.view.default']Jô
pkg.torch.onnx.fx_node%view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_3, [1, 512, 8, 8]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.token_mixer', 'view_2']Jª
pkg.torch.onnx.stack_traceúFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 149, in forward
    x = x.transpose(-2, -1).reshape(B, C, H, W)
ù
'encoder.model.network.7.0.layer_scale_1
view_2mul_21node_mul_21"MulJñ
	namespaceà: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/mul_21: aten.mul.TensorJË
pkg.torch.onnx.class_hierarchy≈['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'aten.mul.Tensor']JØ
pkg.torch.onnx.fx_nodeî%mul_21 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_7_0_layer_scale_1, %view_2), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'mul_21']Jñ
pkg.torch.onnx.stack_trace˜File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
ﬁ
	conv2d_85
mul_21add_18node_add_18"AddJñ
	namespaceà: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/add_18: aten.add.TensorJË
pkg.torch.onnx.class_hierarchy≈['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_18 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_85, %mul_21), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'add_18']Jñ
pkg.torch.onnx.stack_trace˜File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
ú
add_18
2encoder.model.network.7.0.convffn.conv.conv.weight
7encoder.model.network.7.0.convffn.conv.conv.weight_bias
getitem_60node_Conv_593"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∫
	namespace¨: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.0.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.7.0.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_19: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˛
pkg.torch.onnx.class_hierarchy€['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J≤
pkg.torch.onnx.fx_nodeó%_native_batch_norm_legit_no_training_19 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_86, %p_encoder_model_network_7_0_convffn_conv_bn_weight, %p_encoder_model_network_7_0_convffn_conv_bn_bias, %b_encoder_model_network_7_0_convffn_conv_bn_running_mean, %b_encoder_model_network_7_0_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jû
pkg.torch.onnx.name_scopesˇ['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.convffn', 'encoder.model.network.7.0.convffn.conv', 'encoder.model.network.7.0.convffn.conv.bn', '_native_batch_norm_legit_no_training_19']J∏
pkg.torch.onnx.stack_traceôFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
≈

getitem_60
,encoder.model.network.7.0.convffn.fc1.weight
*encoder.model.network.7.0.convffn.fc1.bias	conv2d_87node_conv2d_87"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.0.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_87: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_87 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_60, %p_encoder_model_network_7_0_convffn_fc1_weight, %p_encoder_model_network_7_0_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.convffn', 'encoder.model.network.7.0.convffn.fc1', 'conv2d_87']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
©
	conv2d_87
val_0val_413node_Div_413"DivJ¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.0.convffn.act: torch.nn.modules.activation.GELU/gelu_27: aten.gelu.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_27 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_87,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.convffn', 'encoder.model.network.7.0.convffn.act', 'gelu_27']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
†
val_413val_414node_Erf_414"ErfJ¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.0.convffn.act: torch.nn.modules.activation.GELU/gelu_27: aten.gelu.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_27 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_87,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.convffn', 'encoder.model.network.7.0.convffn.act', 'gelu_27']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
val_414
val_3val_416node_Add_416"AddJ¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.0.convffn.act: torch.nn.modules.activation.GELU/gelu_27: aten.gelu.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_27 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_87,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.convffn', 'encoder.model.network.7.0.convffn.act', 'gelu_27']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
val_5
val_416val_418node_Mul_418"MulJ¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.0.convffn.act: torch.nn.modules.activation.GELU/gelu_27: aten.gelu.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_27 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_87,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.convffn', 'encoder.model.network.7.0.convffn.act', 'gelu_27']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
´
	conv2d_87
val_418gelu_27node_gelu_27"MulJ¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.0.convffn.act: torch.nn.modules.activation.GELU/gelu_27: aten.gelu.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_27 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_87,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.convffn', 'encoder.model.network.7.0.convffn.act', 'gelu_27']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
¿
gelu_27
,encoder.model.network.7.0.convffn.fc2.weight
*encoder.model.network.7.0.convffn.fc2.bias	conv2d_88node_conv2d_88"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.0.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.0.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_88: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_88 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_39, %p_encoder_model_network_7_0_convffn_fc2_weight, %p_encoder_model_network_7_0_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'encoder.model.network.7.0.convffn', 'encoder.model.network.7.0.convffn.fc2', 'conv2d_88']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ì
'encoder.model.network.7.0.layer_scale_2
	conv2d_88mul_22node_mul_22"MulJñ
	namespaceà: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/mul_22: aten.mul.TensorJË
pkg.torch.onnx.class_hierarchy≈['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'aten.mul.Tensor']J±
pkg.torch.onnx.fx_nodeñ%mul_22 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_7_0_layer_scale_2, %clone_40), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'mul_22']Já
pkg.torch.onnx.stack_traceËFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
…
add_18
mul_22add_19node_add_19"AddJñ
	namespaceà: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.0: mobileclip.models.mci.AttentionBlock/add_19: aten.add.TensorJË
pkg.torch.onnx.class_hierarchy≈['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'aten.add.Tensor']Jã
pkg.torch.onnx.fx_nodeq%add_19 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_18, %mul_22), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.0', 'add_19']Já
pkg.torch.onnx.stack_traceËFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
¯
add_19
%encoder.model.network.7.1.norm.weight
#encoder.model.network.7.1.norm.bias
+encoder.model.network.7.1.norm.running_mean
*encoder.model.network.7.1.norm.running_var
getitem_63/node__native_batch_norm_legit_no_training_20__0"BatchNormalization*
epsilon¨≈'7†*
momentumfff?†J†
	namespaceí: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.norm: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_20: aten._native_batch_norm_legit_no_training.defaultJ¥
pkg.torch.onnx.class_hierarchyë['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']JÉ
pkg.torch.onnx.fx_nodeË%_native_batch_norm_legit_no_training_20 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%add_19, %p_encoder_model_network_7_1_norm_weight, %p_encoder_model_network_7_1_norm_bias, %b_encoder_model_network_7_1_norm_running_mean, %b_encoder_model_network_7_1_norm_running_var, 0.1, 1e-05), kwargs = {})Jƒ
pkg.torch.onnx.name_scopes•['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.norm', '_native_batch_norm_legit_no_training_20']J©
pkg.torch.onnx.stack_traceäFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
∆

getitem_63
val_367view_3node_view_3"Reshape*
	allowzero†J⁄
	namespaceÃ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/view_3: aten.view.defaultJà
pkg.torch.onnx.class_hierarchyÂ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.view.default']Jñ
pkg.torch.onnx.fx_node|%view_3 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%getitem_63, [1, 512, 64]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'view_3']J–
pkg.torch.onnx.stack_trace±File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 132, in forward
    x = torch.flatten(x, start_dim=2).transpose(-2, -1)  # (B, N, C)
Ã
view_3transpose_4node_transpose_4"	Transpose*
perm@ @@†J‡
	namespace“: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/transpose_4: aten.transpose.intJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_4 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_3, -2, -1), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'transpose_4']J–
pkg.torch.onnx.stack_trace±File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 132, in forward
    x = torch.flatten(x, start_dim=2).transpose(-2, -1)  # (B, N, C)
˚
transpose_4
val_428linear_2node_linear_2"MatMulJ®
	namespaceö: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/encoder.model.network.7.1.token_mixer.qkv: torch.nn.modules.linear.Linear/linear_2: aten.linear.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J√
pkg.torch.onnx.fx_node®%linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_4, %p_encoder_model_network_7_1_token_mixer_qkv_weight), kwargs = {})JŸ
pkg.torch.onnx.name_scopes∫['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'encoder.model.network.7.1.token_mixer.qkv', 'linear_2']J≈
pkg.torch.onnx.stack_trace¶File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 134, in forward
    self.qkv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
π
linear_2
val_375view_4node_view_4"Reshape*
	allowzero†J⁄
	namespaceÃ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/view_4: aten.view.defaultJà
pkg.torch.onnx.class_hierarchyÂ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.view.default']Jõ
pkg.torch.onnx.fx_nodeÄ%view_4 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_2, [1, 64, 3, 16, 32]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'view_4']J¿
pkg.torch.onnx.stack_trace°File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 135, in forward
    .reshape(B, N, 3, self.num_heads, self.head_dim)
≠
view_4	permute_1node_permute_1"	Transpose*
perm@@ @@@†J‡
	namespace“: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/permute_1: aten.permute.defaultJã
pkg.torch.onnx.class_hierarchyË['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.permute.default']Jú
pkg.torch.onnx.fx_nodeÅ%permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_4, [2, 0, 3, 1, 4]), kwargs = {})J≠
pkg.torch.onnx.name_scopesé['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'permute_1']Jß
pkg.torch.onnx.stack_traceàFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 136, in forward
    .permute(2, 0, 3, 1, 4)
«
	permute_1
val_376
val_377
val_376val_439node_Slice_439"SliceJ⁄
	namespaceÃ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/unbind_1: aten.unbind.intJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.unbind.int']Jà
pkg.torch.onnx.fx_noden%unbind_1 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_1,), kwargs = {})J¨
pkg.torch.onnx.name_scopesç['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'unbind_1']Jﬁ
pkg.torch.onnx.stack_traceøFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 138, in forward
    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
∫
val_439
val_376
getitem_66node_unbind_1__0"SqueezeJ⁄
	namespaceÃ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/unbind_1: aten.unbind.intJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.unbind.int']Jà
pkg.torch.onnx.fx_noden%unbind_1 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_1,), kwargs = {})J¨
pkg.torch.onnx.name_scopesç['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'unbind_1']Jﬁ
pkg.torch.onnx.stack_traceøFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 138, in forward
    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
«
	permute_1
val_377
val_382
val_376val_443node_Slice_443"SliceJ⁄
	namespaceÃ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/unbind_1: aten.unbind.intJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.unbind.int']Jà
pkg.torch.onnx.fx_noden%unbind_1 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_1,), kwargs = {})J¨
pkg.torch.onnx.name_scopesç['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'unbind_1']Jﬁ
pkg.torch.onnx.stack_traceøFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 138, in forward
    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
∫
val_443
val_376
getitem_67node_unbind_1__1"SqueezeJ⁄
	namespaceÃ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/unbind_1: aten.unbind.intJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.unbind.int']Jà
pkg.torch.onnx.fx_noden%unbind_1 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_1,), kwargs = {})J¨
pkg.torch.onnx.name_scopesç['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'unbind_1']Jﬁ
pkg.torch.onnx.stack_traceøFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 138, in forward
    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
«
	permute_1
val_382
val_386
val_376val_447node_Slice_447"SliceJ⁄
	namespaceÃ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/unbind_1: aten.unbind.intJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.unbind.int']Jà
pkg.torch.onnx.fx_noden%unbind_1 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_1,), kwargs = {})J¨
pkg.torch.onnx.name_scopesç['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'unbind_1']Jﬁ
pkg.torch.onnx.stack_traceøFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 138, in forward
    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
∫
val_447
val_376
getitem_68node_unbind_1__2"SqueezeJ⁄
	namespaceÃ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/unbind_1: aten.unbind.intJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.unbind.int']Jà
pkg.torch.onnx.fx_noden%unbind_1 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_1,), kwargs = {})J¨
pkg.torch.onnx.name_scopesç['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'unbind_1']Jﬁ
pkg.torch.onnx.stack_traceøFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 138, in forward
    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)
û

getitem_66
val_389mul_23node_mul_23"MulJÿ
	namespace : __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/mul_23: aten.mul.TensorJÜ
pkg.torch.onnx.class_hierarchy„['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.mul.Tensor']Jõ
pkg.torch.onnx.fx_nodeÄ%mul_23 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_66, 0.1767766952966369), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'mul_23']JΩ
pkg.torch.onnx.stack_traceûFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 141, in forward
    attn = (q * self.scale) @ k.transpose(-2, -1)
√

getitem_67transpose_5node_transpose_5"	Transpose*
perm@ @@@†J‡
	namespace“: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/transpose_5: aten.transpose.intJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.transpose.int']Jñ
pkg.torch.onnx.fx_node|%transpose_5 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%getitem_67, -2, -1), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'transpose_5']JΩ
pkg.torch.onnx.stack_traceûFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 141, in forward
    attn = (q * self.scale) @ k.transpose(-2, -1)
¨
mul_23
transpose_5matmul_2node_matmul_2"MatMulJﬁ
	namespace–: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/matmul_2: aten.matmul.defaultJä
pkg.torch.onnx.class_hierarchyÁ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.matmul.default']Jñ
pkg.torch.onnx.fx_node|%matmul_2 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%mul_23, %transpose_5), kwargs = {})J¨
pkg.torch.onnx.name_scopesç['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'matmul_2']JΩ
pkg.torch.onnx.stack_traceûFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 141, in forward
    attn = (q * self.scale) @ k.transpose(-2, -1)
ö
matmul_2	softmax_1node_softmax_1"Softmax*
axisˇˇˇˇˇˇˇˇˇ†J‹
	namespaceŒ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/softmax_1: aten.softmax.intJá
pkg.torch.onnx.class_hierarchy‰['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.softmax.int']Jå
pkg.torch.onnx.fx_noder%softmax_1 : [num_users=1] = call_function[target=torch.ops.aten.softmax.int](args = (%matmul_2, -1), kwargs = {})J≠
pkg.torch.onnx.name_scopesé['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'softmax_1']J´
pkg.torch.onnx.stack_traceåFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 142, in forward
    attn = attn.softmax(dim=-1)
±
	softmax_1

getitem_68matmul_3node_matmul_3"MatMulJﬁ
	namespace–: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/matmul_3: aten.matmul.defaultJä
pkg.torch.onnx.class_hierarchyÁ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.matmul.default']Jó
pkg.torch.onnx.fx_node}%matmul_3 : [num_users=1] = call_function[target=torch.ops.aten.matmul.default](args = (%clone_41, %getitem_68), kwargs = {})J¨
pkg.torch.onnx.name_scopesç['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'matmul_3']Jø
pkg.torch.onnx.stack_trace†File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 145, in forward
    x = (attn @ v).transpose(1, 2).reshape(B, N, C)
ø
matmul_3transpose_6node_transpose_6"	Transpose*
perm@ @@@†J‡
	namespace“: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/transpose_6: aten.transpose.intJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_6 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%matmul_3, 1, 2), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'transpose_6']Jø
pkg.torch.onnx.stack_trace†File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 145, in forward
    x = (attn @ v).transpose(1, 2).reshape(B, N, C)
ı
transpose_6
val_394_unsafe_view_1node__unsafe_view_1"Reshape*
	allowzero†JÍ
	namespace‹: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/_unsafe_view_1: aten._unsafe_view.defaultJê
pkg.torch.onnx.class_hierarchyÌ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten._unsafe_view.default']J•
pkg.torch.onnx.fx_nodeä%_unsafe_view_1 : [num_users=1] = call_function[target=torch.ops.aten._unsafe_view.default](args = (%clone_42, [1, 64, 512]), kwargs = {})J≤
pkg.torch.onnx.name_scopesì['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', '_unsafe_view_1']Jø
pkg.torch.onnx.stack_trace†File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 145, in forward
    x = (attn @ v).transpose(1, 2).reshape(B, N, C)
æ
_unsafe_view_1
val_453val_454node_MatMul_454"MatMulJ©
	namespaceõ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/encoder.model.network.7.1.token_mixer.proj: torch.nn.modules.linear.Linear/linear_3: aten.linear.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J˚
pkg.torch.onnx.fx_node‡%linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_unsafe_view_1, %p_encoder_model_network_7_1_token_mixer_proj_weight, %p_encoder_model_network_7_1_token_mixer_proj_bias), kwargs = {})J⁄
pkg.torch.onnx.name_scopesª['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'encoder.model.network.7.1.token_mixer.proj', 'linear_3']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 146, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
€
val_454
/encoder.model.network.7.1.token_mixer.proj.biaslinear_3node_linear_3"AddJ©
	namespaceõ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/encoder.model.network.7.1.token_mixer.proj: torch.nn.modules.linear.Linear/linear_3: aten.linear.defaultJ¨
pkg.torch.onnx.class_hierarchyâ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J˚
pkg.torch.onnx.fx_node‡%linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_unsafe_view_1, %p_encoder_model_network_7_1_token_mixer_proj_weight, %p_encoder_model_network_7_1_token_mixer_proj_bias), kwargs = {})J⁄
pkg.torch.onnx.name_scopesª['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'encoder.model.network.7.1.token_mixer.proj', 'linear_3']J 
pkg.torch.onnx.stack_trace´File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 146, in forward
    x = self.proj(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
ª
linear_3transpose_7node_transpose_7"	Transpose*
perm@ @@†J‡
	namespace“: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/transpose_7: aten.transpose.intJâ
pkg.torch.onnx.class_hierarchyÊ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.transpose.int']Jî
pkg.torch.onnx.fx_nodez%transpose_7 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%clone_43, -2, -1), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'transpose_7']Jª
pkg.torch.onnx.stack_traceúFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 149, in forward
    x = x.transpose(-2, -1).reshape(B, C, H, W)
µ
transpose_7
val_402view_5node_view_5"Reshape*
	allowzero†J⁄
	namespaceÃ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.token_mixer: mobileclip.models.mci.MHSA/view_5: aten.view.defaultJà
pkg.torch.onnx.class_hierarchyÂ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.MHSA', 'aten.view.default']Jô
pkg.torch.onnx.fx_node%view_5 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_7, [1, 512, 8, 8]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.token_mixer', 'view_5']Jª
pkg.torch.onnx.stack_traceúFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 149, in forward
    x = x.transpose(-2, -1).reshape(B, C, H, W)
ù
'encoder.model.network.7.1.layer_scale_1
view_5mul_24node_mul_24"MulJñ
	namespaceà: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/mul_24: aten.mul.TensorJË
pkg.torch.onnx.class_hierarchy≈['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'aten.mul.Tensor']JØ
pkg.torch.onnx.fx_nodeî%mul_24 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_7_1_layer_scale_1, %view_5), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'mul_24']Jñ
pkg.torch.onnx.stack_trace˜File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
ÿ
add_19
mul_24add_20node_add_20"AddJñ
	namespaceà: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/add_20: aten.add.TensorJË
pkg.torch.onnx.class_hierarchy≈['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'aten.add.Tensor']Jã
pkg.torch.onnx.fx_nodeq%add_20 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_19, %mul_24), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'add_20']Jñ
pkg.torch.onnx.stack_trace˜File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 638, in forward
    x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(self.norm(x)))
ú
add_20
2encoder.model.network.7.1.convffn.conv.conv.weight
7encoder.model.network.7.1.convffn.conv.conv.weight_bias
getitem_69node_Conv_595"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J∫
	namespace¨: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.1.convffn.conv: torch.nn.modules.container.Sequential/encoder.model.network.7.1.convffn.conv.bn: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_21: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ˛
pkg.torch.onnx.class_hierarchy€['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J≤
pkg.torch.onnx.fx_nodeó%_native_batch_norm_legit_no_training_21 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_89, %p_encoder_model_network_7_1_convffn_conv_bn_weight, %p_encoder_model_network_7_1_convffn_conv_bn_bias, %b_encoder_model_network_7_1_convffn_conv_bn_running_mean, %b_encoder_model_network_7_1_convffn_conv_bn_running_var, 0.1, 1e-05), kwargs = {})Jû
pkg.torch.onnx.name_scopesˇ['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.convffn', 'encoder.model.network.7.1.convffn.conv', 'encoder.model.network.7.1.convffn.conv.bn', '_native_batch_norm_legit_no_training_21']J∏
pkg.torch.onnx.stack_traceôFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 379, in forward
    x = self.conv(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\batchnorm.py", line 193, in forward
    return F.batch_norm(
≈

getitem_69
,encoder.model.network.7.1.convffn.fc1.weight
*encoder.model.network.7.1.convffn.fc1.bias	conv2d_90node_conv2d_90"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.1.convffn.fc1: torch.nn.modules.conv.Conv2d/conv2d_90: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÓ
pkg.torch.onnx.fx_node”%conv2d_90 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem_69, %p_encoder_model_network_7_1_convffn_fc1_weight, %p_encoder_model_network_7_1_convffn_fc1_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.convffn', 'encoder.model.network.7.1.convffn.fc1', 'conv2d_90']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 380, in forward
    x = self.fc1(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
©
	conv2d_90
val_0val_471node_Div_471"DivJ¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.1.convffn.act: torch.nn.modules.activation.GELU/gelu_28: aten.gelu.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_28 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_90,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.convffn', 'encoder.model.network.7.1.convffn.act', 'gelu_28']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
†
val_471val_472node_Erf_472"ErfJ¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.1.convffn.act: torch.nn.modules.activation.GELU/gelu_28: aten.gelu.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_28 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_90,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.convffn', 'encoder.model.network.7.1.convffn.act', 'gelu_28']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
val_472
val_3val_474node_Add_474"AddJ¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.1.convffn.act: torch.nn.modules.activation.GELU/gelu_28: aten.gelu.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_28 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_90,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.convffn', 'encoder.model.network.7.1.convffn.act', 'gelu_28']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ß
val_5
val_474val_476node_Mul_476"MulJ¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.1.convffn.act: torch.nn.modules.activation.GELU/gelu_28: aten.gelu.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_28 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_90,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.convffn', 'encoder.model.network.7.1.convffn.act', 'gelu_28']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
´
	conv2d_90
val_476gelu_28node_gelu_28"MulJ¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.1.convffn.act: torch.nn.modules.activation.GELU/gelu_28: aten.gelu.defaultJØ
pkg.torch.onnx.class_hierarchyå['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']Jâ
pkg.torch.onnx.fx_nodeo%gelu_28 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%conv2d_90,), kwargs = {})J–
pkg.torch.onnx.name_scopes±['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.convffn', 'encoder.model.network.7.1.convffn.act', 'gelu_28']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 381, in forward
    x = self.act(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
¿
gelu_28
,encoder.model.network.7.1.convffn.fc2.weight
*encoder.model.network.7.1.convffn.fc2.bias	conv2d_91node_conv2d_91"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †J¢
	namespaceî: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/encoder.model.network.7.1.convffn: mobileclip.models.mci.ConvFFN/encoder.model.network.7.1.convffn.fc2: torch.nn.modules.conv.Conv2d/conv2d_91: aten.conv2d.defaultJ≠
pkg.torch.onnx.class_hierarchyä['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'mobileclip.models.mci.ConvFFN', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÏ
pkg.torch.onnx.fx_node—%conv2d_91 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone_44, %p_encoder_model_network_7_1_convffn_fc2_weight, %p_encoder_model_network_7_1_convffn_fc2_bias), kwargs = {})J“
pkg.torch.onnx.name_scopes≥['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'encoder.model.network.7.1.convffn', 'encoder.model.network.7.1.convffn.fc2', 'conv2d_91']J¬
pkg.torch.onnx.stack_trace£File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 383, in forward
    x = self.fc2(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ì
'encoder.model.network.7.1.layer_scale_2
	conv2d_91mul_25node_mul_25"MulJñ
	namespaceà: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/mul_25: aten.mul.TensorJË
pkg.torch.onnx.class_hierarchy≈['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'aten.mul.Tensor']J±
pkg.torch.onnx.fx_nodeñ%mul_25 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%p_encoder_model_network_7_1_layer_scale_2, %clone_45), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'mul_25']Já
pkg.torch.onnx.stack_traceËFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
…
add_20
mul_25add_21node_add_21"AddJñ
	namespaceà: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.network.7: torch.nn.modules.container.Sequential/encoder.model.network.7.1: mobileclip.models.mci.AttentionBlock/add_21: aten.add.TensorJË
pkg.torch.onnx.class_hierarchy≈['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'torch.nn.modules.container.Sequential', 'mobileclip.models.mci.AttentionBlock', 'aten.add.Tensor']Jã
pkg.torch.onnx.fx_nodeq%add_21 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_20, %mul_25), kwargs = {})JÄ
pkg.torch.onnx.name_scopesb['', 'encoder', 'encoder.model', 'encoder.model.network.7', 'encoder.model.network.7.1', 'add_21']Já
pkg.torch.onnx.stack_traceËFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 851, in forward
    x = self.forward_tokens(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 639, in forward
    x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
æ
add_21
*encoder.model.conv_exp.reparam_conv.weight
(encoder.model.conv_exp.reparam_conv.bias	conv2d_92node_conv2d_92"Conv*
groupÄ†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@@@@†J´
	namespaceù: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.reparam_conv: torch.nn.modules.conv.Conv2d/conv2d_92: aten.conv2d.defaultJÒ
pkg.torch.onnx.class_hierarchyŒ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']JÉ
pkg.torch.onnx.fx_nodeË%conv2d_92 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_21, %p_encoder_model_conv_exp_reparam_conv_weight, %p_encoder_model_conv_exp_reparam_conv_bias, [1, 1], [1, 1], [1, 1], 512), kwargs = {})Jå
pkg.torch.onnx.name_scopesn['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.reparam_conv', 'conv2d_92']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
ø
	conv2d_92
avg_pool2dnode_avg_pool2d"AveragePool*
kernel_shape@@†*
count_include_pad†*
strides@@†*
auto_pad"NOTSET†*
pads@ @ @ @ †*
	ceil_mode †Jµ
	namespaceß: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.se: mobileclip.modules.common.mobileone.SEBlock/avg_pool2d: aten.avg_pool2d.defaultJÑ
pkg.torch.onnx.class_hierarchy·['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'mobileclip.modules.common.mobileone.SEBlock', 'aten.avg_pool2d.default']Jô
pkg.torch.onnx.fx_node%avg_pool2d : [num_users=1] = call_function[target=torch.ops.aten.avg_pool2d.default](args = (%conv2d_92, [8, 8]), kwargs = {})JÉ
pkg.torch.onnx.name_scopese['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.se', 'avg_pool2d']Jß
pkg.torch.onnx.stack_traceàFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 48, in forward
    x = F.avg_pool2d(inputs, kernel_size=[h, w])
√

avg_pool2d
'encoder.model.conv_exp.se.reduce.weight
%encoder.model.conv_exp.se.reduce.bias	conv2d_93node_conv2d_93"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÔ
	namespace·: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.se: mobileclip.modules.common.mobileone.SEBlock/encoder.model.conv_exp.se.reduce: torch.nn.modules.conv.Conv2d/conv2d_93: aten.conv2d.defaultJ†
pkg.torch.onnx.class_hierarchy˝['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'mobileclip.modules.common.mobileone.SEBlock', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J‰
pkg.torch.onnx.fx_node…%conv2d_93 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%avg_pool2d, %p_encoder_model_conv_exp_se_reduce_weight, %p_encoder_model_conv_exp_se_reduce_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.se', 'encoder.model.conv_exp.se.reduce', 'conv2d_93']Jø
pkg.torch.onnx.stack_trace†File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 49, in forward
    x = self.reduce(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
Ï
	conv2d_93relu_2node_relu_2"ReluJ´
	namespaceù: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.se: mobileclip.modules.common.mobileone.SEBlock/relu_2: aten.relu.defaultJ˛
pkg.torch.onnx.class_hierarchy€['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'mobileclip.modules.common.mobileone.SEBlock', 'aten.relu.default']Jà
pkg.torch.onnx.fx_noden%relu_2 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%conv2d_93,), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.se', 'relu_2']Jà
pkg.torch.onnx.stack_traceÈFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 50, in forward
    x = F.relu(x)
ª
relu_2
'encoder.model.conv_exp.se.expand.weight
%encoder.model.conv_exp.se.expand.bias	conv2d_94node_conv2d_94"Conv*
group†*
auto_pad"NOTSET†*
	dilations@@†*
strides@@†*
pads@ @ @ @ †JÔ
	namespace·: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.se: mobileclip.modules.common.mobileone.SEBlock/encoder.model.conv_exp.se.expand: torch.nn.modules.conv.Conv2d/conv2d_94: aten.conv2d.defaultJ†
pkg.torch.onnx.class_hierarchy˝['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'mobileclip.modules.common.mobileone.SEBlock', 'torch.nn.modules.conv.Conv2d', 'aten.conv2d.default']J‡
pkg.torch.onnx.fx_node≈%conv2d_94 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%relu_2, %p_encoder_model_conv_exp_se_expand_weight, %p_encoder_model_conv_exp_se_expand_bias), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.se', 'encoder.model.conv_exp.se.expand', 'conv2d_94']Jø
pkg.torch.onnx.stack_trace†File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 51, in forward
    x = self.expand(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
è
	conv2d_94	sigmoid_2node_sigmoid_2"SigmoidJ±
	namespace£: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.se: mobileclip.modules.common.mobileone.SEBlock/sigmoid_2: aten.sigmoid.defaultJÅ
pkg.torch.onnx.class_hierarchyﬁ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'mobileclip.modules.common.mobileone.SEBlock', 'aten.sigmoid.default']Jé
pkg.torch.onnx.fx_nodet%sigmoid_2 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%conv2d_94,), kwargs = {})JÇ
pkg.torch.onnx.name_scopesd['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.se', 'sigmoid_2']Jè
pkg.torch.onnx.stack_traceFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 52, in forward
    x = torch.sigmoid(x)
•
	sigmoid_2
val_482view_6node_view_6"Reshape*
	allowzero†J´
	namespaceù: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.se: mobileclip.modules.common.mobileone.SEBlock/view_6: aten.view.defaultJ˛
pkg.torch.onnx.class_hierarchy€['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'mobileclip.modules.common.mobileone.SEBlock', 'aten.view.default']Jô
pkg.torch.onnx.fx_node%view_6 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%sigmoid_2, [-1, 1024, 1, 1]), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.se', 'view_6']Jí
pkg.torch.onnx.stack_traceÛFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 53, in forward
    x = x.view(-1, c, 1, 1)
˘
	conv2d_92
view_6mul_26node_mul_26"MulJ©
	namespaceõ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.se: mobileclip.modules.common.mobileone.SEBlock/mul_26: aten.mul.TensorJ¸
pkg.torch.onnx.class_hierarchyŸ['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'mobileclip.modules.common.mobileone.SEBlock', 'aten.mul.Tensor']Jé
pkg.torch.onnx.fx_nodet%mul_26 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%conv2d_92, %view_6), kwargs = {})J
pkg.torch.onnx.name_scopesa['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.se', 'mul_26']Jå
pkg.torch.onnx.stack_traceÌFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 54, in forward
    return inputs * x
ä
mul_26
val_0val_484node_Div_484"DivJ©
	namespaceõ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.activation: torch.nn.modules.activation.GELU/gelu_29: aten.gelu.defaultJÛ
pkg.torch.onnx.class_hierarchy–['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÜ
pkg.torch.onnx.fx_nodel%gelu_29 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_26,), kwargs = {})Jà
pkg.torch.onnx.name_scopesj['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.activation', 'gelu_29']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
Ñ
val_484val_485node_Erf_485"ErfJ©
	namespaceõ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.activation: torch.nn.modules.activation.GELU/gelu_29: aten.gelu.defaultJÛ
pkg.torch.onnx.class_hierarchy–['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÜ
pkg.torch.onnx.fx_nodel%gelu_29 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_26,), kwargs = {})Jà
pkg.torch.onnx.name_scopesj['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.activation', 'gelu_29']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ã
val_485
val_3val_487node_Add_487"AddJ©
	namespaceõ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.activation: torch.nn.modules.activation.GELU/gelu_29: aten.gelu.defaultJÛ
pkg.torch.onnx.class_hierarchy–['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÜ
pkg.torch.onnx.fx_nodel%gelu_29 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_26,), kwargs = {})Jà
pkg.torch.onnx.name_scopesj['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.activation', 'gelu_29']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
ã
val_5
val_487val_489node_Mul_489"MulJ©
	namespaceõ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.activation: torch.nn.modules.activation.GELU/gelu_29: aten.gelu.defaultJÛ
pkg.torch.onnx.class_hierarchy–['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÜ
pkg.torch.onnx.fx_nodel%gelu_29 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_26,), kwargs = {})Jà
pkg.torch.onnx.name_scopesj['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.activation', 'gelu_29']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
å
mul_26
val_489gelu_29node_gelu_29"MulJ©
	namespaceõ: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.conv_exp: mobileclip.modules.common.mobileone.MobileOneBlock/encoder.model.conv_exp.activation: torch.nn.modules.activation.GELU/gelu_29: aten.gelu.defaultJÛ
pkg.torch.onnx.class_hierarchy–['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.common.mobileone.MobileOneBlock', 'torch.nn.modules.activation.GELU', 'aten.gelu.default']JÜ
pkg.torch.onnx.fx_nodel%gelu_29 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%mul_26,), kwargs = {})Jà
pkg.torch.onnx.name_scopesj['', 'encoder', 'encoder.model', 'encoder.model.conv_exp', 'encoder.model.conv_exp.activation', 'gelu_29']J¶
pkg.torch.onnx.stack_traceáFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 853, in forward
    x = self.conv_exp(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\common\mobileone.py", line 162, in forward
    return self.activation(self.se(self.reparam_conv(x)))
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\torch\nn\modules\activation.py", line 816, in forward
    return F.gelu(input, approximate=self.approximate)
±
gelu_29
val_492mean_2node_mean_2"
ReduceMean*
keepdims †*
noop_with_empty_axes †JÆ
	namespace†: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.head: mobileclip.modules.image.image_projection.GlobalPool2D/encoder.model.head.pool: mobileclip.modules.image.image_projection.GlobalPool/mean_2: aten.mean.dimJá
pkg.torch.onnx.class_hierarchy‰['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.image.image_projection.GlobalPool2D', 'mobileclip.modules.image.image_projection.GlobalPool', 'aten.mean.dim']Jã
pkg.torch.onnx.fx_nodeq%mean_2 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%gelu_29, [-2, -1]), kwargs = {})Jy
pkg.torch.onnx.name_scopes[['', 'encoder', 'encoder.model', 'encoder.model.head', 'encoder.model.head.pool', 'mean_2']Jâ
pkg.torch.onnx.stack_traceÍFile "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 854, in forward
    cls_out = self.head(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\image_projection.py", line 89, in forward
    x = self.pool(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\image_projection.py", line 66, in forward
    return self._global_pool(x, dims=dims)
È	
mean_2
encoder.model.head.projmatmul_4node_matmul_4"MatMulJË
	namespace⁄: __main__.ImageEncoderWrapper/encoder: mobileclip.image_encoder.MCi/encoder.model: mobileclip.models.mci.FastViT/encoder.model.head: mobileclip.modules.image.image_projection.GlobalPool2D/matmul_4: aten.matmul.defaultJ’
pkg.torch.onnx.class_hierarchy≤['__main__.ImageEncoderWrapper', 'mobileclip.image_encoder.MCi', 'mobileclip.models.mci.FastViT', 'mobileclip.modules.image.image_projection.GlobalPool2D', 'aten.matmul.default']J•
pkg.torch.onnx.fx_nodeä%matmul_4 : [num_users=2] = call_function[target=torch.ops.aten.matmul.default](args = (%mean_2, %p_encoder_model_head_proj), kwargs = {})J`
pkg.torch.onnx.name_scopesB['', 'encoder', 'encoder.model', 'encoder.model.head', 'matmul_4']J◊
pkg.torch.onnx.stack_trace∏File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\image_encoder.py", line 37, in forward
    x = self.model(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\models\mci.py", line 854, in forward
    cls_out = self.head(x)
  File "C:\Users\MIKE\AppData\Roaming\Python\Python311\site-packages\mobileclip\modules\image\image_projection.py", line 91, in forward
    x = x @ self.proj
Õ
matmul_4
val_495linalg_vector_normnode_linalg_vector_norm"ReduceL2*
keepdims†*
noop_with_empty_axes †J_
	namespaceR: __main__.ImageEncoderWrapper/linalg_vector_norm: aten.linalg_vector_norm.defaultJe
pkg.torch.onnx.class_hierarchyC['__main__.ImageEncoderWrapper', 'aten.linalg_vector_norm.default']J∞
pkg.torch.onnx.fx_nodeï%linalg_vector_norm : [num_users=1] = call_function[target=torch.ops.aten.linalg_vector_norm.default](args = (%matmul_4, 2, [-1], True), kwargs = {})J8
pkg.torch.onnx.name_scopes['', 'linalg_vector_norm']J
pkg.torch.onnx.stack_trace 
µ
matmul_4
linalg_vector_norm	embeddingnode_div"DivJ@
	namespace3: __main__.ImageEncoderWrapper/div: aten.div.TensorJU
pkg.torch.onnx.class_hierarchy3['__main__.ImageEncoderWrapper', 'aten.div.Tensor']Jñ
pkg.torch.onnx.fx_node|%div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%matmul_4, %linalg_vector_norm), kwargs = {})J)
pkg.torch.onnx.name_scopes['', 'div']J
pkg.torch.onnx.stack_trace 
main_graph*∂@B-encoder.model.patch_embed.0.reparam_conv.biasJÄÏ@ûNèAk+õ?˜¯?Ë_ß>æ7Æ?îR¥?§ﬂË?ù?@∂!å¿b)™?6Àaæ¿*æƒ◊Ñ?X÷ ø œÓ; ôΩ$Æõ? ªæºêÚö?EŸÁ? }:Ωˇ{H¿€Ù›?ÓÌ(¡∆√}?í–—? &º¥¸ø®„?¿’·ºÅÿ"?E÷¿®$%>¡Eé?Äc¶?DïÊ>´Î¿ÙL ?ˆì”?S–Ω?ÓÚ≤?eòt?4È
?b- ¡©Cæ? ∂o?C9B‘°±?a=`?hCø≈Â?∞1æ.îç?p÷læ ’1ºµìA≠p>¡3Pﬂ?dÁ∆@í£¿?àÆö?n∂æ¨?ë?*ì@B/encoder.model.patch_embed.1.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset113664j
length2304p*∂@B-encoder.model.patch_embed.1.reparam_conv.biasJÄ »Pª lw?Y˘.AXÓøL*'øH¬>?§-™@Åø®÷@†ﬂ-@Ép?
áìAZQ´?ﬁH¸?§ê£ø|u∞@@m¶?Ã”N>∂@≠Aÿô æ d≠>$·çAÆ-D@¿!Öæ6|¿†ûe>ûO%¿¨Üô?£ô∏?írˆ?÷§A‡≠ ?*'øÙå9@–ø$xzø?‚™@Ã«øa@AÄ-≠Ω¿∂äæ˝ÿ@ ‹C>‡V ?®¯ñøX‹A¿Q∞@"˚Œ?bâõ?›∞@+E‘¿¢kÇ¿(¨?*	í?5)™A =≥A0BÖ?ππ¿ê˚0¿®¨—?– î?ö∑>|ºØ?èÛî?*∂@B-encoder.model.patch_embed.2.reparam_conv.biasJÄãsøFﬁ»=Hê>5ﬂ=€–ëæmåê?{à:>~5>9¡ø ’æV®Å¿{ñ5¿Ç[˘Ωá,øéø⁄X	ø ∫QΩjı≥<hqë=~?–8øõ9Ω≤R’=ÏÖ∑<[êÑ¡∑‡æù0>±ÓIæéüæV–äæ›≈>xò∆<ûŸ1øjSV>ˆ4ø@¯ˇ=*ìP?òâ?\Å?ö«ø±ïø3q™=0·Åæ’ôë>0µ·æ£´>Xyø\˙ÛøQÚP?≤–ΩT¬A9ææ }FøfΩ»æ⁄Yø√T¿2oÕøôUøZSá>–võΩéñH¿§Ñêæh€‡ø„3>*≤@B%encoder.model.network.0.0.layer_scaleJÄ¶íæÍßn>i-ƒ>Æ”ˇΩµ7">◊_)>œ*íæ`Ô‹æ⁄ˇªæ<µù>N
èæÂw>+%z>‚r>ˆ‰`æ¡j2>R,f>¶Ø†>Hì1>+@cæﬂ” >”q.>ôítæ…xTæ5Üæ:_Sæ–0æ∫ˆ'>≥>ß√æ´hÜ>§ﬂW>∆ºﬁæ€ÒÙΩ√ˇ>lk>÷.'æ^Í⁄æÉ’k>ü¢dæÑ.4>„”læ•+oæ‹[æ>«SÎ=ƒOB>ı[q>YÆΩ>›föæà=Wu∞>=€u>=Dm>¬åO>2äôæÆ˜æc©™æçl1>#>ﬂ<É>«ëuæ=æ?æ¢ü]>Ï˛h>*ù@B9encoder.model.network.0.0.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset115968j
length2304p*¿@B7encoder.model.network.0.0.token_mixer.reparam_conv.biasJÄ¡lŸªD≤:ü#ΩU4ãª^œ¡=&
`;ı@YΩ-;?DÏº™D"=∑ç/º?º$Ω»åÙº —P<"õ3:†≠πgxØ;aERºaµâº∏n>”<%=ª!Êª(‘Ωº∫gºfáU<ƒø};˙[º…-=áΩÛº—G§;áë∏º™Ø=â∫‹ª,rº87èª¸·ºe‰≥<k0=M>¨r<Ínº¡•∂ª¯°Ω@=Lwè=Ÿä^>§“=íT!ΩyÃxºƒq≤ºW8<ö5‡=∂∂<˘ö¬<ﬁöo=î|ß<à†ñª›&∂=_≈Óª Ñº¯/±<…ã∏º=∫==Ólº*á¿B*encoder.model.network.0.0.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset13824j
length768p*≥@B*encoder.model.network.0.0.convffn.fc2.biasJÄ¸Ê=Ó‹7=90⁄=6ùòΩëÜ¥>ù%$æÚÔ3æ◊æ©m*=ﬁïæx]Ωp8 =ß\_Ω±H>bí(º¶.r>tÜΩuY¬=O,ƒæ¥tŸ?ﬁ¿›=Ωˆ=R&sΩÃßæŒ5n>≥≤>∫˝Ø>hîæIÃk> 
æ«R æ%¶j>˚G>ëÄ∏=iD>àﬂ™<úæ¶=∆∫.Ω»L<¿ˇÕO=V± æ—¿aΩ6®>r0`=lŸ‚>R)æøßdæ¿Tg=ÿ˘º∏˜π>:∆IΩÅàræ¢C=òóêæH≤Qæ`¥eæ±îÚ=2_Ë=£‰{æ≈Î£ΩlŸ>[ãæœU^æõ7›æ*≤@B%encoder.model.network.0.1.layer_scaleJÄïTæ/èæÃ:ø»√Y>¢i!?¡“> ﬁ> HgørJßæQÑâ>9ÿ#?¸ö«æn°Ÿ>ÄÓÊ>†‹
øL‡Ì=s–»>∆_Õ>pQ¯>´ >÷äææ~Ó>MÁÌæ‘ˆ\ø{≠ì>‡(0ø_>ı>mº?	 ø„πOø¢‘6æü©™æ-≠>IÔæÑõ∞>¢Ãæ÷5ø| æ÷⁄âæÊ3æÌÜæÆOø™=?„8æ[sÑ>sâAæî¨äæ£qÜ>‹9æ[1˛>¶eMæn]ø_Œ%>±Møwêáæˆû˝>æI>›Ö >∏_ŒæÖ6¿>/¬∂æ¶˚*øU>*ù@B9encoder.model.network.0.1.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset118272j
length2304p*¿@B7encoder.model.network.0.1.token_mixer.reparam_conv.biasJÄj.`;‘ro;rVÏ;|e¬<s}Mπ‡ËΩ0Sjº„˝˚;¨N<QO;”ı’:≥ <<Cª≤ñ;ˆ#{:∫&<›áq;ùäºGÅ<Zîºa¢l;«¢	<Rpw:H…^ªØ/ø<ﬁS<>Ê(<]·¨ªCfÆª<ñ;r≤ØªËÌSΩ≤¿:|ÍÛª<ç`;yõ;∑(èπnÏ}ª≥#/Ωj∏9=‰≤º⁄ò›ªIÂ∆ª?A*<åπ<·BΩG}¸:ø|<KÚu;‹√íºôRË9’ñﬂªÀS;+)T<N&0ºéƒª∫>§9ÆÓçªq‘√<x-f<$`™ªD±8ºo∫∑ I∫*á¿B*encoder.model.network.0.1.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset14592j
length768p*≥@B*encoder.model.network.0.1.convffn.fc2.biasJÄÒ4ç=ÊÜ≈∫w¿4Ω∂çÈ∫Ü∞Ωt	πº∆Ω˘€<'~ =kFù<IüY<Ilb>8Éﬂ<ˇΩ>…Z∂<√5ΩS/º≤¿Ωæä#*>>˝ =◊∑'ªïÊ–º&…ˇº5ﬁçΩóMD>´=Å=O˚=6:5Ω$hq>ˇXöæ¬%Sæ≈ì>ºaµ•=ù¸tæZπGΩ¶!="¶îºs£Væˆ‡;}…’º<µ1ø€7∫ætœxΩ(=n=1L"ºR”v>vÁœ<Íõ¯=F9=‚ <±:Í>hlÿ<ä°Ωx)ÃªaN>¥òº£ò„ΩnÏ?Ω¿≤w=UÙ≥º–˘º_;äΩµ$ÜΩ*àÄB/encoder.model.network.1.proj.0.lkb_reparam.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset0j
length512p*ãÄB0encoder.model.network.1.proj.1.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset512j
length512p*ÖÄB%encoder.model.network.2.0.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset1024j
length512p*ìÄB7encoder.model.network.2.0.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1536j
length512p*àÄB*encoder.model.network.2.0.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset59392j
length1536p*ÜÄB*encoder.model.network.2.0.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset2048j
length512p*ÖÄB%encoder.model.network.2.1.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset2560j
length512p*ìÄB7encoder.model.network.2.1.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset3072j
length512p*àÄB*encoder.model.network.2.1.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset60928j
length1536p*ÜÄB*encoder.model.network.2.1.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset3584j
length512p*ÖÄB%encoder.model.network.2.2.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset4096j
length512p*ìÄB7encoder.model.network.2.2.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset4608j
length512p*àÄB*encoder.model.network.2.2.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset62464j
length1536p*ÜÄB*encoder.model.network.2.2.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset5120j
length512p*ÖÄB%encoder.model.network.2.3.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset5632j
length512p*ìÄB7encoder.model.network.2.3.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset6144j
length512p*àÄB*encoder.model.network.2.3.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset64000j
length1536p*ÜÄB*encoder.model.network.2.3.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset6656j
length512p*ÖÄB%encoder.model.network.2.4.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset7168j
length512p*ìÄB7encoder.model.network.2.4.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset7680j
length512p*àÄB*encoder.model.network.2.4.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset65536j
length1536p*ÜÄB*encoder.model.network.2.4.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset8192j
length512p*ÖÄB%encoder.model.network.2.5.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset8704j
length512p*ìÄB7encoder.model.network.2.5.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset9216j
length512p*àÄB*encoder.model.network.2.5.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset67072j
length1536p*ÜÄB*encoder.model.network.2.5.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset9728j
length512p*≥@B*encoder.model.network.3.proj.0.se.fc1.biasJÄ£î;=z∏=FR>aMxº3qs=∆0ê>ªúΩ˘ˇ='=qÜ	>CªY> Åè>©;§;Oÿ.>Ÿ„Ω#≤Ü>OßK=æoºë>*ÆÅº=≈T>∏À¶=mÍÄ=Ò>
ë%>§`>*'Q>ªÅ~>ôà¯=E] >
8>?]#>q‡kΩQ	[>πäP=ÃÄ>¸ÈÑ=ö>”‚r=,sÇ>ﬂK>(})>ºnû>gú1>£YS>ÈåÆ;[»ºÁ;ΩûÉ=x†a=ê’πº<@Ú= 3¢>j>d¶>ô®Ó=„≥>Å®ö<FÈ¬=Å7e=Ø€É>ë!§=¥”≤< EØ>*àÄB*encoder.model.network.3.proj.0.se.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset15360j
length1024p*çÄB/encoder.model.network.3.proj.0.lkb_reparam.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset16384j
length1024p*éÄB0encoder.model.network.3.proj.1.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset17408j
length1024p*áÄB%encoder.model.network.4.0.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset18432j
length1024p*ïÄB7encoder.model.network.4.0.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset19456j
length1024p*âÄB*encoder.model.network.4.0.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset120576j
length3072p*àÄB*encoder.model.network.4.0.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset20480j
length1024p*áÄB%encoder.model.network.4.1.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset21504j
length1024p*ïÄB7encoder.model.network.4.1.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset22528j
length1024p*âÄB*encoder.model.network.4.1.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset123648j
length3072p*àÄB*encoder.model.network.4.1.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset23552j
length1024p*áÄB%encoder.model.network.4.2.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset24576j
length1024p*ïÄB7encoder.model.network.4.2.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset25600j
length1024p*âÄB*encoder.model.network.4.2.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset126720j
length3072p*àÄB*encoder.model.network.4.2.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset26624j
length1024p*áÄB%encoder.model.network.4.3.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset27648j
length1024p*ïÄB7encoder.model.network.4.3.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset28672j
length1024p*âÄB*encoder.model.network.4.3.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset129792j
length3072p*àÄB*encoder.model.network.4.3.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset29696j
length1024p*áÄB%encoder.model.network.4.4.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset30720j
length1024p*ïÄB7encoder.model.network.4.4.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset31744j
length1024p*âÄB*encoder.model.network.4.4.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset132864j
length3072p*àÄB*encoder.model.network.4.4.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset32768j
length1024p*áÄB%encoder.model.network.4.5.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset33792j
length1024p*ïÄB7encoder.model.network.4.5.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset34816j
length1024p*âÄB*encoder.model.network.4.5.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset135936j
length3072p*àÄB*encoder.model.network.4.5.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset35840j
length1024p*áÄB%encoder.model.network.4.6.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset36864j
length1024p*ïÄB7encoder.model.network.4.6.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset37888j
length1024p*âÄB*encoder.model.network.4.6.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset139008j
length3072p*àÄB*encoder.model.network.4.6.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset38912j
length1024p*áÄB%encoder.model.network.4.7.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset39936j
length1024p*ïÄB7encoder.model.network.4.7.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset40960j
length1024p*âÄB*encoder.model.network.4.7.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset142080j
length3072p*àÄB*encoder.model.network.4.7.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset41984j
length1024p*áÄB%encoder.model.network.4.8.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset43008j
length1024p*ïÄB7encoder.model.network.4.8.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset44032j
length1024p*âÄB*encoder.model.network.4.8.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset145152j
length3072p*àÄB*encoder.model.network.4.8.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset45056j
length1024p*áÄB%encoder.model.network.4.9.layer_scalej2
location&mobileclip_image_encoder_256.onnx.dataj
offset46080j
length1024p*ïÄB7encoder.model.network.4.9.token_mixer.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset47104j
length1024p*âÄB*encoder.model.network.4.9.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset148224j
length3072p*àÄB*encoder.model.network.4.9.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset48128j
length1024p*áÄB*encoder.model.network.5.proj.0.se.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset10240j
length512p*àÄB*encoder.model.network.5.proj.0.se.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset68608j
length2048p*çÄB/encoder.model.network.5.proj.0.lkb_reparam.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset70656j
length2048p*éÄB0encoder.model.network.5.proj.1.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset72704j
length2048p*áÄB)encoder.model.network.6.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset74752j
length2048p*âÄB'encoder.model.network.7.0.layer_scale_1j2
location&mobileclip_image_encoder_256.onnx.dataj
offset76800j
length2048p*âÄB'encoder.model.network.7.0.layer_scale_2j2
location&mobileclip_image_encoder_256.onnx.dataj
offset78848j
length2048p*ÉÄB%encoder.model.network.7.0.norm.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset80896j
length2048p*ÅÄB#encoder.model.network.7.0.norm.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset82944j
length2048p*çÄB/encoder.model.network.7.0.token_mixer.proj.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset84992j
length2048p*àÄB*encoder.model.network.7.0.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset87040j
length2048p*âÄB'encoder.model.network.7.1.layer_scale_1j2
location&mobileclip_image_encoder_256.onnx.dataj
offset89088j
length2048p*âÄB'encoder.model.network.7.1.layer_scale_2j2
location&mobileclip_image_encoder_256.onnx.dataj
offset91136j
length2048p*ÉÄB%encoder.model.network.7.1.norm.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset93184j
length2048p*ÅÄB#encoder.model.network.7.1.norm.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset95232j
length2048p*çÄB/encoder.model.network.7.1.token_mixer.proj.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset97280j
length2048p*àÄB*encoder.model.network.7.1.convffn.fc2.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset99328j
length2048p*Æ@B%encoder.model.conv_exp.se.reduce.biasJÄ´æ=÷Â> Ñ >±0÷<ÓjÏ<XlÔ<›#≤> ->>©o±=$”t=ﬂ™ü=VΩ<^-='ó> ª>l=òO6=Õ¡®< ‰?Å∫y<£*>§å=<¥U>õI.Ω“=7í!?q|æS%û>±_≠>˚ÿ'=@C´;Ë ÿ=0øÛ>D>’#=
«ê>‚‰Æ=ó>b√>∞ñ<»Ø√>hx<>öuï>Ã√>ïÀﬂ=ŒÈM>ücÉ=tû(>+KK>ˆNe>pÙ=•iÈ=ï¥"?ùÓ¡>e=HΩB¿>è¥ˇ=Ù¡Ö=Ø"ı=&û>»>⁄%ã=YzÃ=πØ¬=*äÄB+encoder.model.network.7.0.norm.running_meanj2
location&mobileclip_image_encoder_256.onnx.dataj
offset101376j
length2048p*âÄB*encoder.model.network.7.0.norm.running_varj2
location&mobileclip_image_encoder_256.onnx.dataj
offset103424j
length2048p*äÄB+encoder.model.network.7.1.norm.running_meanj2
location&mobileclip_image_encoder_256.onnx.dataj
offset105472j
length2048p*âÄB*encoder.model.network.7.1.norm.running_varj2
location&mobileclip_image_encoder_256.onnx.dataj
offset107520j
length2048p*ì@B/encoder.model.patch_embed.0.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset199424j
length6912p*î@@B/encoder.model.patch_embed.2.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset323584j
length16384p*ó@B2encoder.model.network.0.0.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset298496j
length12544p*í¿@B,encoder.model.network.0.0.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset552448j
length49152p*í@¿B,encoder.model.network.0.0.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset601600j
length49152p*ó@B2encoder.model.network.0.1.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset311040j
length12544p*í¿@B,encoder.model.network.0.1.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset650752j
length49152p*í@¿B,encoder.model.network.0.1.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset699904j
length49152p*óÄB1encoder.model.network.1.proj.0.lkb_reparam.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset339968j
length25088p*öÄÄB2encoder.model.network.1.proj.1.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1300992j
length65536p*ûÄB9encoder.model.network.2.0.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset159488j
length4608p*òÄB2encoder.model.network.2.0.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset365056j
length25088p*ïÄÄB,encoder.model.network.2.0.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1899008j
length196608p*ïÄÄB,encoder.model.network.2.0.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset2095616j
length196608p*ûÄB9encoder.model.network.2.1.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset164096j
length4608p*òÄB2encoder.model.network.2.1.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset390144j
length25088p*ïÄÄB,encoder.model.network.2.1.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset2292224j
length196608p*ïÄÄB,encoder.model.network.2.1.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset2488832j
length196608p*ûÄB9encoder.model.network.2.2.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset168704j
length4608p*òÄB2encoder.model.network.2.2.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset415232j
length25088p*ïÄÄB,encoder.model.network.2.2.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset2685440j
length196608p*ïÄÄB,encoder.model.network.2.2.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset2882048j
length196608p*ûÄB9encoder.model.network.2.3.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset173312j
length4608p*òÄB2encoder.model.network.2.3.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset440320j
length25088p*ïÄÄB,encoder.model.network.2.3.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset3078656j
length196608p*ïÄÄB,encoder.model.network.2.3.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset3275264j
length196608p*ûÄB9encoder.model.network.2.4.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset177920j
length4608p*òÄB2encoder.model.network.2.4.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset465408j
length25088p*ïÄÄB,encoder.model.network.2.4.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset3471872j
length196608p*ïÄÄB,encoder.model.network.2.4.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset3668480j
length196608p*ûÄB9encoder.model.network.2.5.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset182528j
length4608p*òÄB2encoder.model.network.2.5.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset490496j
length25088p*ïÄÄB,encoder.model.network.2.5.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset3865088j
length196608p*ïÄÄB,encoder.model.network.2.5.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset4061696j
length196608p*ì@ÄB,encoder.model.network.3.proj.0.se.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1366528j
length65536p*ìÄ@B,encoder.model.network.3.proj.0.se.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1432064j
length65536p*óÄB1encoder.model.network.3.proj.0.lkb_reparam.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset749056j
length50176p*õÄÄB2encoder.model.network.3.proj.1.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset4258304j
length262144p*ûÄB9encoder.model.network.4.0.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset206336j
length9216p*òÄB2encoder.model.network.4.0.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset799232j
length50176p*ïÄÄB,encoder.model.network.4.0.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset5569024j
length786432p*ïÄÄB,encoder.model.network.4.0.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset6355456j
length786432p*ûÄB9encoder.model.network.4.1.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset215552j
length9216p*òÄB2encoder.model.network.4.1.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset849408j
length50176p*ïÄÄB,encoder.model.network.4.1.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset7141888j
length786432p*ïÄÄB,encoder.model.network.4.1.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset7928320j
length786432p*ûÄB9encoder.model.network.4.2.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset224768j
length9216p*òÄB2encoder.model.network.4.2.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset899584j
length50176p*ïÄÄB,encoder.model.network.4.2.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset8714752j
length786432p*ïÄÄB,encoder.model.network.4.2.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset9501184j
length786432p*ûÄB9encoder.model.network.4.3.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset233984j
length9216p*òÄB2encoder.model.network.4.3.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset949760j
length50176p*ñÄÄB,encoder.model.network.4.3.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset10287616j
length786432p*ñÄÄB,encoder.model.network.4.3.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset11074048j
length786432p*ûÄB9encoder.model.network.4.4.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset243200j
length9216p*òÄB2encoder.model.network.4.4.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset999936j
length50176p*ñÄÄB,encoder.model.network.4.4.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset11860480j
length786432p*ñÄÄB,encoder.model.network.4.4.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset12646912j
length786432p*ûÄB9encoder.model.network.4.5.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset252416j
length9216p*ôÄB2encoder.model.network.4.5.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1050112j
length50176p*ñÄÄB,encoder.model.network.4.5.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset13433344j
length786432p*ñÄÄB,encoder.model.network.4.5.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset14219776j
length786432p*ûÄB9encoder.model.network.4.6.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset261632j
length9216p*ôÄB2encoder.model.network.4.6.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1100288j
length50176p*ñÄÄB,encoder.model.network.4.6.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset15006208j
length786432p*ñÄÄB,encoder.model.network.4.6.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset15792640j
length786432p*ûÄB9encoder.model.network.4.7.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset270848j
length9216p*ôÄB2encoder.model.network.4.7.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1150464j
length50176p*ñÄÄB,encoder.model.network.4.7.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset16579072j
length786432p*ñÄÄB,encoder.model.network.4.7.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset17365504j
length786432p*ûÄB9encoder.model.network.4.8.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset280064j
length9216p*ôÄB2encoder.model.network.4.8.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1200640j
length50176p*ñÄÄB,encoder.model.network.4.8.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset18151936j
length786432p*ñÄÄB,encoder.model.network.4.8.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset18938368j
length786432p*ûÄB9encoder.model.network.4.9.token_mixer.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset289280j
length9216p*ôÄB2encoder.model.network.4.9.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1250816j
length50176p*ñÄÄB,encoder.model.network.4.9.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset19724800j
length786432p*ñÄÄB,encoder.model.network.4.9.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset20511232j
length786432p*ïÄÄB,encoder.model.network.5.proj.0.se.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset4520448j
length262144p*ïÄÄB,encoder.model.network.5.proj.0.se.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset4782592j
length262144p*ôÄB1encoder.model.network.5.proj.0.lkb_reparam.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1497600j
length100352p*ùÄÄB2encoder.model.network.5.proj.1.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset21297664j
length1048576p*ìÄB+encoder.model.network.6.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1597952j
length100352p*öÄB2encoder.model.network.7.0.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1698304j
length100352p*óÄÄB,encoder.model.network.7.0.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset26542080j
length3145728p*âÄB*encoder.model.network.7.0.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset187136j
length6144p*óÄÄB,encoder.model.network.7.0.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset29687808j
length3145728p*öÄB2encoder.model.network.7.1.convffn.conv.conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset1798656j
length100352p*óÄÄB,encoder.model.network.7.1.convffn.fc1.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset32833536j
length3145728p*âÄB*encoder.model.network.7.1.convffn.fc1.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset193280j
length6144p*óÄÄB,encoder.model.network.7.1.convffn.fc2.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset35979264j
length3145728p*è@ÄB'encoder.model.conv_exp.se.reduce.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset5044736j
length262144p*èÄ@B'encoder.model.conv_exp.se.expand.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset5306880j
length262144p*ÑÄB%encoder.model.conv_exp.se.expand.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset151296j
length4096p*êÄB*encoder.model.conv_exp.reparam_conv.weightj2
location&mobileclip_image_encoder_256.onnx.dataj
offset515584j
length36864p*áÄB(encoder.model.conv_exp.reparam_conv.biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset155392j
length4096p*~ÄÄBencoder.model.head.projj2
location&mobileclip_image_encoder_256.onnx.dataj
offset24444928j
length2097152p*Bval_168J              *'Bval_367J              @       *nÄÄBval_368j2
location&mobileclip_image_encoder_256.onnx.dataj
offset39124992j
length3145728p*7Bval_375J(       @                             *'Bval_394J       @              *nÄÄBval_395j2
location&mobileclip_image_encoder_256.onnx.dataj
offset22346240j
length1048576p*/Bval_402J                             *nÄÄBval_428j2
location&mobileclip_image_encoder_256.onnx.dataj
offset42270720j
length3145728p*nÄÄBval_453j2
location&mobileclip_image_encoder_256.onnx.dataj
offset23394816j
length1048576p*/Bval_482J ˇˇˇˇˇˇˇˇ                     *Bval_492J˛ˇˇˇˇˇˇˇˇˇˇˇˇˇˇˇ*Bval_495Jˇˇˇˇˇˇˇˇ*¿@B7encoder.model.network.0.0.convffn.conv.conv.weight_biasJÄà‘œ>0— ?*x?⁄8s¿ÉP¿)‘«¿WÒ9¿ên?≠€I@¥‰e?Î•@πÖA>? @–)àæò¡ ZæÕ¢O?`⁄ræC£îB;l>DvÜ>∞≥@ç¿¡ætàæs⁄ øób^¿pÀõøﬂq„æí˝ö?å£¿ŒAï¸∏¿Å€"A‹£»@…Qø÷j¿lyÌ¿ZÁ)¿‹ıø°º ¿Lô?Äc±Ω%6¿∆üAÁòBÏ˛≥¿‚
¡?^
æ–7w?ñâß?ˆ»¡ÉN≈@^ÄÚ@|Kæl‚Z¡~Å˜æègì?ó—#¡º=π¿hˇ@%Ôˇ?A∆l¡Pmc¿*¿@B7encoder.model.network.0.1.convffn.conv.conv.weight_biasJÄÄ}?Êà÷æö‰g>ÆX?Ï˜ÜΩ§∏¿0S≠?‡["ø∞˘L¿Y¶ñ>}5ë>ºÈå=nÛ	?$|=ö—éæÊ∑ø?bøºW@=
@%Ó?9=ÈjU¿d∏¿æ4 Uæ¥@®3?Ã›©øÿàkæŒÌΩg«=oú˛æIå@¿Z±øÛﬂ?∑”˜=Œ›ö>da:ºå˛Æ> h˘æ≥˙>íHú¡\\j>ù]?ıA¡?ªæ∑A¶˚æXµwΩRæ˜?ñÂ?áhç>ÿÖæ4:V?ÿ„8æÎﬂ>ò-øòæè7ù>Àõ@¿Ωk¿{∏æëK@0Ç∫?¢Ê=úø*îÄB7encoder.model.network.2.0.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset10752j
length512p*îÄB7encoder.model.network.2.1.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset11264j
length512p*îÄB7encoder.model.network.2.2.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset11776j
length512p*îÄB7encoder.model.network.2.3.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset12288j
length512p*îÄB7encoder.model.network.2.4.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset12800j
length512p*îÄB7encoder.model.network.2.5.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset13312j
length512p*ïÄB7encoder.model.network.4.0.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset49152j
length1024p*ïÄB7encoder.model.network.4.1.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset50176j
length1024p*ïÄB7encoder.model.network.4.2.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset51200j
length1024p*ïÄB7encoder.model.network.4.3.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset52224j
length1024p*ïÄB7encoder.model.network.4.4.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset53248j
length1024p*ïÄB7encoder.model.network.4.5.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset54272j
length1024p*ïÄB7encoder.model.network.4.6.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset55296j
length1024p*ïÄB7encoder.model.network.4.7.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset56320j
length1024p*ïÄB7encoder.model.network.4.8.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset57344j
length1024p*ïÄB7encoder.model.network.4.9.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset58368j
length1024p*ñÄB7encoder.model.network.7.0.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset109568j
length2048p*ñÄB7encoder.model.network.7.1.convffn.conv.conv.weight_biasj2
location&mobileclip_image_encoder_256.onnx.dataj
offset111616j
length2048p*Bval_0JÛµ?*Bval_3J  Ä?*Bval_5J   ?*Bval_376J        *Bval_377J       *Bval_382J       *Bval_386J       *Bval_389JÛ5>Z«
image



Ä
Ä"=
/pkg.torch.export.graph_signature.InputSpec.kind
USER_INPUT"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"&
!pkg.torch.onnx.original_node_namexbá
	embedding
	

Ä"?
0pkg.torch.export.graph_signature.OutputSpec.kindUSER_OUTPUT"(
!pkg.torch.onnx.original_node_namedivjé
-encoder.model.patch_embed.0.reparam_conv.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"T
!pkg.torch.onnx.original_node_name/p_encoder_model_patch_embed_0_reparam_conv_biasjû
/encoder.model.patch_embed.1.reparam_conv.weight

@


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_encoder_model_patch_embed_1_reparam_conv_weightjé
-encoder.model.patch_embed.1.reparam_conv.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"T
!pkg.torch.onnx.original_node_name/p_encoder_model_patch_embed_1_reparam_conv_biasjé
-encoder.model.patch_embed.2.reparam_conv.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"T
!pkg.torch.onnx.original_node_name/p_encoder_model_patch_embed_2_reparam_conv_biasjÜ
%encoder.model.network.0.0.layer_scale

@

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_0_0_layer_scalej≤
9encoder.model.network.0.0.token_mixer.reparam_conv.weight

@


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_0_0_token_mixer_reparam_conv_weightj¢
7encoder.model.network.0.0.token_mixer.reparam_conv.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_0_0_token_mixer_reparam_conv_biasjâ
*encoder.model.network.0.0.convffn.fc1.bias
	
¿"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_0_0_convffn_fc1_biasjà
*encoder.model.network.0.0.convffn.fc2.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_0_0_convffn_fc2_biasjÜ
%encoder.model.network.0.1.layer_scale

@

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_0_1_layer_scalej≤
9encoder.model.network.0.1.token_mixer.reparam_conv.weight

@


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_0_1_token_mixer_reparam_conv_weightj¢
7encoder.model.network.0.1.token_mixer.reparam_conv.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_0_1_token_mixer_reparam_conv_biasjâ
*encoder.model.network.0.1.convffn.fc1.bias
	
¿"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_0_1_convffn_fc1_biasjà
*encoder.model.network.0.1.convffn.fc2.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_0_1_convffn_fc2_biasjì
/encoder.model.network.1.proj.0.lkb_reparam.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_encoder_model_network_1_proj_0_lkb_reparam_biasjï
0encoder.model.network.1.proj.1.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"W
!pkg.torch.onnx.original_node_name2p_encoder_model_network_1_proj_1_reparam_conv_biasjá
%encoder.model.network.2.0.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_2_0_layer_scalej£
7encoder.model.network.2.0.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_2_0_token_mixer_reparam_conv_biasjâ
*encoder.model.network.2.0.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_2_0_convffn_fc1_biasjâ
*encoder.model.network.2.0.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_2_0_convffn_fc2_biasjá
%encoder.model.network.2.1.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_2_1_layer_scalej£
7encoder.model.network.2.1.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_2_1_token_mixer_reparam_conv_biasjâ
*encoder.model.network.2.1.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_2_1_convffn_fc1_biasjâ
*encoder.model.network.2.1.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_2_1_convffn_fc2_biasjá
%encoder.model.network.2.2.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_2_2_layer_scalej£
7encoder.model.network.2.2.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_2_2_token_mixer_reparam_conv_biasjâ
*encoder.model.network.2.2.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_2_2_convffn_fc1_biasjâ
*encoder.model.network.2.2.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_2_2_convffn_fc2_biasjá
%encoder.model.network.2.3.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_2_3_layer_scalej£
7encoder.model.network.2.3.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_2_3_token_mixer_reparam_conv_biasjâ
*encoder.model.network.2.3.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_2_3_convffn_fc1_biasjâ
*encoder.model.network.2.3.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_2_3_convffn_fc2_biasjá
%encoder.model.network.2.4.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_2_4_layer_scalej£
7encoder.model.network.2.4.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_2_4_token_mixer_reparam_conv_biasjâ
*encoder.model.network.2.4.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_2_4_convffn_fc1_biasjâ
*encoder.model.network.2.4.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_2_4_convffn_fc2_biasjá
%encoder.model.network.2.5.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_2_5_layer_scalej£
7encoder.model.network.2.5.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_2_5_token_mixer_reparam_conv_biasjâ
*encoder.model.network.2.5.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_2_5_convffn_fc1_biasjâ
*encoder.model.network.2.5.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_2_5_convffn_fc2_biasjà
*encoder.model.network.3.proj.0.se.fc1.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_3_proj_0_se_fc1_biasjâ
*encoder.model.network.3.proj.0.se.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_3_proj_0_se_fc2_biasjì
/encoder.model.network.3.proj.0.lkb_reparam.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_encoder_model_network_3_proj_0_lkb_reparam_biasjï
0encoder.model.network.3.proj.1.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"W
!pkg.torch.onnx.original_node_name2p_encoder_model_network_3_proj_1_reparam_conv_biasjá
%encoder.model.network.4.0.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_4_0_layer_scalej£
7encoder.model.network.4.0.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_4_0_token_mixer_reparam_conv_biasjâ
*encoder.model.network.4.0.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_0_convffn_fc1_biasjâ
*encoder.model.network.4.0.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_0_convffn_fc2_biasjá
%encoder.model.network.4.1.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_4_1_layer_scalej£
7encoder.model.network.4.1.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_4_1_token_mixer_reparam_conv_biasjâ
*encoder.model.network.4.1.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_1_convffn_fc1_biasjâ
*encoder.model.network.4.1.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_1_convffn_fc2_biasjá
%encoder.model.network.4.2.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_4_2_layer_scalej£
7encoder.model.network.4.2.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_4_2_token_mixer_reparam_conv_biasjâ
*encoder.model.network.4.2.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_2_convffn_fc1_biasjâ
*encoder.model.network.4.2.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_2_convffn_fc2_biasjá
%encoder.model.network.4.3.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_4_3_layer_scalej£
7encoder.model.network.4.3.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_4_3_token_mixer_reparam_conv_biasjâ
*encoder.model.network.4.3.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_3_convffn_fc1_biasjâ
*encoder.model.network.4.3.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_3_convffn_fc2_biasjá
%encoder.model.network.4.4.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_4_4_layer_scalej£
7encoder.model.network.4.4.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_4_4_token_mixer_reparam_conv_biasjâ
*encoder.model.network.4.4.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_4_convffn_fc1_biasjâ
*encoder.model.network.4.4.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_4_convffn_fc2_biasjá
%encoder.model.network.4.5.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_4_5_layer_scalej£
7encoder.model.network.4.5.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_4_5_token_mixer_reparam_conv_biasjâ
*encoder.model.network.4.5.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_5_convffn_fc1_biasjâ
*encoder.model.network.4.5.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_5_convffn_fc2_biasjá
%encoder.model.network.4.6.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_4_6_layer_scalej£
7encoder.model.network.4.6.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_4_6_token_mixer_reparam_conv_biasjâ
*encoder.model.network.4.6.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_6_convffn_fc1_biasjâ
*encoder.model.network.4.6.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_6_convffn_fc2_biasjá
%encoder.model.network.4.7.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_4_7_layer_scalej£
7encoder.model.network.4.7.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_4_7_token_mixer_reparam_conv_biasjâ
*encoder.model.network.4.7.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_7_convffn_fc1_biasjâ
*encoder.model.network.4.7.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_7_convffn_fc2_biasjá
%encoder.model.network.4.8.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_4_8_layer_scalej£
7encoder.model.network.4.8.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_4_8_token_mixer_reparam_conv_biasjâ
*encoder.model.network.4.8.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_8_convffn_fc1_biasjâ
*encoder.model.network.4.8.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_8_convffn_fc2_biasjá
%encoder.model.network.4.9.layer_scale

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_4_9_layer_scalej£
7encoder.model.network.4.9.token_mixer.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"^
!pkg.torch.onnx.original_node_name9p_encoder_model_network_4_9_token_mixer_reparam_conv_biasjâ
*encoder.model.network.4.9.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_9_convffn_fc1_biasjâ
*encoder.model.network.4.9.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_4_9_convffn_fc2_biasjâ
*encoder.model.network.5.proj.0.se.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_5_proj_0_se_fc1_biasjâ
*encoder.model.network.5.proj.0.se.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_5_proj_0_se_fc2_biasjì
/encoder.model.network.5.proj.0.lkb_reparam.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_encoder_model_network_5_proj_0_lkb_reparam_biasjï
0encoder.model.network.5.proj.1.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"W
!pkg.torch.onnx.original_node_name2p_encoder_model_network_5_proj_1_reparam_conv_biasjá
)encoder.model.network.6.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"P
!pkg.torch.onnx.original_node_name+p_encoder_model_network_6_reparam_conv_biasjã
'encoder.model.network.7.0.layer_scale_1

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"N
!pkg.torch.onnx.original_node_name)p_encoder_model_network_7_0_layer_scale_1jã
'encoder.model.network.7.0.layer_scale_2

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"N
!pkg.torch.onnx.original_node_name)p_encoder_model_network_7_0_layer_scale_2jˇ
%encoder.model.network.7.0.norm.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_7_0_norm_weightj˚
#encoder.model.network.7.0.norm.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"J
!pkg.torch.onnx.original_node_name%p_encoder_model_network_7_0_norm_biasjì
/encoder.model.network.7.0.token_mixer.proj.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_encoder_model_network_7_0_token_mixer_proj_biasjâ
*encoder.model.network.7.0.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_7_0_convffn_fc2_biasjã
'encoder.model.network.7.1.layer_scale_1

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"N
!pkg.torch.onnx.original_node_name)p_encoder_model_network_7_1_layer_scale_1jã
'encoder.model.network.7.1.layer_scale_2

Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"N
!pkg.torch.onnx.original_node_name)p_encoder_model_network_7_1_layer_scale_2jˇ
%encoder.model.network.7.1.norm.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_network_7_1_norm_weightj˚
#encoder.model.network.7.1.norm.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"J
!pkg.torch.onnx.original_node_name%p_encoder_model_network_7_1_norm_biasjì
/encoder.model.network.7.1.token_mixer.proj.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_encoder_model_network_7_1_token_mixer_proj_biasjâ
*encoder.model.network.7.1.convffn.fc2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_7_1_convffn_fc2_biasj˛
%encoder.model.conv_exp.se.reduce.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_conv_exp_se_reduce_biasjà
+encoder.model.network.7.0.norm.running_mean
	
Ä"9
/pkg.torch.export.graph_signature.InputSpec.kindBUFFER"=
5pkg.torch.export.graph_signature.InputSpec.persistentTrue"R
!pkg.torch.onnx.original_node_name-b_encoder_model_network_7_0_norm_running_meanjÜ
*encoder.model.network.7.0.norm.running_var
	
Ä"9
/pkg.torch.export.graph_signature.InputSpec.kindBUFFER"=
5pkg.torch.export.graph_signature.InputSpec.persistentTrue"Q
!pkg.torch.onnx.original_node_name,b_encoder_model_network_7_0_norm_running_varjà
+encoder.model.network.7.1.norm.running_mean
	
Ä"9
/pkg.torch.export.graph_signature.InputSpec.kindBUFFER"=
5pkg.torch.export.graph_signature.InputSpec.persistentTrue"R
!pkg.torch.onnx.original_node_name-b_encoder_model_network_7_1_norm_running_meanjÜ
*encoder.model.network.7.1.norm.running_var
	
Ä"9
/pkg.torch.export.graph_signature.InputSpec.kindBUFFER"=
5pkg.torch.export.graph_signature.InputSpec.persistentTrue"Q
!pkg.torch.onnx.original_node_name,b_encoder_model_network_7_1_norm_running_varjû
/encoder.model.patch_embed.0.reparam_conv.weight

@


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_encoder_model_patch_embed_0_reparam_conv_weightjû
/encoder.model.patch_embed.2.reparam_conv.weight

@
@

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_encoder_model_patch_embed_2_reparam_conv_weightjL
2encoder.model.network.0.0.convffn.conv.conv.weight

@


jô
,encoder.model.network.0.0.convffn.fc1.weight

¿
@

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_0_0_convffn_fc1_weightjô
,encoder.model.network.0.0.convffn.fc2.weight

@
¿

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_0_0_convffn_fc2_weightjL
2encoder.model.network.0.1.convffn.conv.conv.weight

@


jô
,encoder.model.network.0.1.convffn.fc1.weight

¿
@

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_0_1_convffn_fc1_weightjô
,encoder.model.network.0.1.convffn.fc2.weight

@
¿

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_0_1_convffn_fc2_weightj£
1encoder.model.network.1.proj.0.lkb_reparam.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"X
!pkg.torch.onnx.original_node_name3p_encoder_model_network_1_proj_0_lkb_reparam_weightj¶
2encoder.model.network.1.proj.1.reparam_conv.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Y
!pkg.torch.onnx.original_node_name4p_encoder_model_network_1_proj_1_reparam_conv_weightj≥
9encoder.model.network.2.0.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_2_0_token_mixer_reparam_conv_weightjM
2encoder.model.network.2.0.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.2.0.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_2_0_convffn_fc1_weightjö
,encoder.model.network.2.0.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_2_0_convffn_fc2_weightj≥
9encoder.model.network.2.1.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_2_1_token_mixer_reparam_conv_weightjM
2encoder.model.network.2.1.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.2.1.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_2_1_convffn_fc1_weightjö
,encoder.model.network.2.1.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_2_1_convffn_fc2_weightj≥
9encoder.model.network.2.2.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_2_2_token_mixer_reparam_conv_weightjM
2encoder.model.network.2.2.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.2.2.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_2_2_convffn_fc1_weightjö
,encoder.model.network.2.2.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_2_2_convffn_fc2_weightj≥
9encoder.model.network.2.3.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_2_3_token_mixer_reparam_conv_weightjM
2encoder.model.network.2.3.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.2.3.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_2_3_convffn_fc1_weightjö
,encoder.model.network.2.3.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_2_3_convffn_fc2_weightj≥
9encoder.model.network.2.4.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_2_4_token_mixer_reparam_conv_weightjM
2encoder.model.network.2.4.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.2.4.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_2_4_convffn_fc1_weightjö
,encoder.model.network.2.4.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_2_4_convffn_fc2_weightj≥
9encoder.model.network.2.5.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_2_5_token_mixer_reparam_conv_weightjM
2encoder.model.network.2.5.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.2.5.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_2_5_convffn_fc1_weightjö
,encoder.model.network.2.5.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_2_5_convffn_fc2_weightjô
,encoder.model.network.3.proj.0.se.fc1.weight

@
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_3_proj_0_se_fc1_weightjô
,encoder.model.network.3.proj.0.se.fc2.weight

Ä
@

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_3_proj_0_se_fc2_weightj£
1encoder.model.network.3.proj.0.lkb_reparam.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"X
!pkg.torch.onnx.original_node_name3p_encoder_model_network_3_proj_0_lkb_reparam_weightj¶
2encoder.model.network.3.proj.1.reparam_conv.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Y
!pkg.torch.onnx.original_node_name4p_encoder_model_network_3_proj_1_reparam_conv_weightj≥
9encoder.model.network.4.0.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_4_0_token_mixer_reparam_conv_weightjM
2encoder.model.network.4.0.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.4.0.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_0_convffn_fc1_weightjö
,encoder.model.network.4.0.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_0_convffn_fc2_weightj≥
9encoder.model.network.4.1.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_4_1_token_mixer_reparam_conv_weightjM
2encoder.model.network.4.1.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.4.1.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_1_convffn_fc1_weightjö
,encoder.model.network.4.1.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_1_convffn_fc2_weightj≥
9encoder.model.network.4.2.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_4_2_token_mixer_reparam_conv_weightjM
2encoder.model.network.4.2.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.4.2.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_2_convffn_fc1_weightjö
,encoder.model.network.4.2.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_2_convffn_fc2_weightj≥
9encoder.model.network.4.3.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_4_3_token_mixer_reparam_conv_weightjM
2encoder.model.network.4.3.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.4.3.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_3_convffn_fc1_weightjö
,encoder.model.network.4.3.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_3_convffn_fc2_weightj≥
9encoder.model.network.4.4.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_4_4_token_mixer_reparam_conv_weightjM
2encoder.model.network.4.4.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.4.4.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_4_convffn_fc1_weightjö
,encoder.model.network.4.4.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_4_convffn_fc2_weightj≥
9encoder.model.network.4.5.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_4_5_token_mixer_reparam_conv_weightjM
2encoder.model.network.4.5.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.4.5.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_5_convffn_fc1_weightjö
,encoder.model.network.4.5.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_5_convffn_fc2_weightj≥
9encoder.model.network.4.6.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_4_6_token_mixer_reparam_conv_weightjM
2encoder.model.network.4.6.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.4.6.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_6_convffn_fc1_weightjö
,encoder.model.network.4.6.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_6_convffn_fc2_weightj≥
9encoder.model.network.4.7.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_4_7_token_mixer_reparam_conv_weightjM
2encoder.model.network.4.7.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.4.7.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_7_convffn_fc1_weightjö
,encoder.model.network.4.7.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_7_convffn_fc2_weightj≥
9encoder.model.network.4.8.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_4_8_token_mixer_reparam_conv_weightjM
2encoder.model.network.4.8.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.4.8.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_8_convffn_fc1_weightjö
,encoder.model.network.4.8.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_8_convffn_fc2_weightj≥
9encoder.model.network.4.9.token_mixer.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_encoder_model_network_4_9_token_mixer_reparam_conv_weightjM
2encoder.model.network.4.9.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.4.9.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_9_convffn_fc1_weightjö
,encoder.model.network.4.9.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_4_9_convffn_fc2_weightjö
,encoder.model.network.5.proj.0.se.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_5_proj_0_se_fc1_weightjö
,encoder.model.network.5.proj.0.se.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_5_proj_0_se_fc2_weightj£
1encoder.model.network.5.proj.0.lkb_reparam.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"X
!pkg.torch.onnx.original_node_name3p_encoder_model_network_5_proj_0_lkb_reparam_weightj¶
2encoder.model.network.5.proj.1.reparam_conv.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Y
!pkg.torch.onnx.original_node_name4p_encoder_model_network_5_proj_1_reparam_conv_weightjó
+encoder.model.network.6.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"R
!pkg.torch.onnx.original_node_name-p_encoder_model_network_6_reparam_conv_weightjM
2encoder.model.network.7.0.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.7.0.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_7_0_convffn_fc1_weightjâ
*encoder.model.network.7.0.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_7_0_convffn_fc1_biasjö
,encoder.model.network.7.0.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_7_0_convffn_fc2_weightjM
2encoder.model.network.7.1.convffn.conv.conv.weight

Ä


jö
,encoder.model.network.7.1.convffn.fc1.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_7_1_convffn_fc1_weightjâ
*encoder.model.network.7.1.convffn.fc1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_network_7_1_convffn_fc1_biasjö
,encoder.model.network.7.1.convffn.fc2.weight

Ä
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"S
!pkg.torch.onnx.original_node_name.p_encoder_model_network_7_1_convffn_fc2_weightjè
'encoder.model.conv_exp.se.reduce.weight

@
Ä

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"N
!pkg.torch.onnx.original_node_name)p_encoder_model_conv_exp_se_reduce_weightjè
'encoder.model.conv_exp.se.expand.weight

Ä
@

"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"N
!pkg.torch.onnx.original_node_name)p_encoder_model_conv_exp_se_expand_weightjˇ
%encoder.model.conv_exp.se.expand.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"L
!pkg.torch.onnx.original_node_name'p_encoder_model_conv_exp_se_expand_biasjï
*encoder.model.conv_exp.reparam_conv.weight

Ä


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"Q
!pkg.torch.onnx.original_node_name,p_encoder_model_conv_exp_reparam_conv_weightjÖ
(encoder.model.conv_exp.reparam_conv.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"O
!pkg.torch.onnx.original_node_name*p_encoder_model_conv_exp_reparam_conv_biasjË
encoder.model.head.proj


Ä
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone">
!pkg.torch.onnx.original_node_namep_encoder_model_head_projjU
val_168


">
$pkg.onnxscript.optimizer.folded_from['val_166', 'val_167']j`
val_367


"I
$pkg.onnxscript.optimizer.folded_from!['val_364', 'val_365', 'val_366']jy
val_368


Ä
Ä"\
$pkg.onnxscript.optimizer.folded_from4['encoder.model.network.7.0.token_mixer.qkv.weight']jv
val_375


"_
$pkg.onnxscript.optimizer.folded_from7['val_370', 'val_371', 'val_372', 'val_373', 'val_374']j`
val_394


"I
$pkg.onnxscript.optimizer.folded_from!['val_391', 'val_392', 'val_393']jz
val_395


Ä
Ä"]
$pkg.onnxscript.optimizer.folded_from5['encoder.model.network.7.0.token_mixer.proj.weight']jk
val_402


"T
$pkg.onnxscript.optimizer.folded_from,['val_398', 'val_399', 'val_400', 'val_401']jy
val_428


Ä
Ä"\
$pkg.onnxscript.optimizer.folded_from4['encoder.model.network.7.1.token_mixer.qkv.weight']jz
val_453


Ä
Ä"]
$pkg.onnxscript.optimizer.folded_from5['encoder.model.network.7.1.token_mixer.proj.weight']jk
val_482


"T
$pkg.onnxscript.optimizer.folded_from,['val_478', 'val_479', 'val_480', 'val_481']jU
val_492


">
$pkg.onnxscript.optimizer.folded_from['val_490', 'val_491']jU
val_495


">
$pkg.onnxscript.optimizer.folded_from['val_493', 'val_494']jE
7encoder.model.network.0.0.convffn.conv.conv.weight_bias


@jE
7encoder.model.network.0.1.convffn.conv.conv.weight_bias


@jF
7encoder.model.network.2.0.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.2.1.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.2.2.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.2.3.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.2.4.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.2.5.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.4.0.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.4.1.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.4.2.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.4.3.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.4.4.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.4.5.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.4.6.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.4.7.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.4.8.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.4.9.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.7.0.convffn.conv.conv.weight_bias
	
ÄjF
7encoder.model.network.7.1.convffn.conv.conv.weight_bias
	
Äj
val_0
 j
val_3
 j
val_5
 j
val_376


j
val_377


j
val_382


j
val_386


j
val_389
 j"
conv2d


@
Ä
Äj!
val_1


@
Ä
Äj!
val_2


@
Ä
Äj!
val_4


@
Ä
Äj!
val_6


@
Ä
Äj 
gelu


@
Ä
Äj"
conv2d_1


@
@
@j
val_8


@
@
@j
val_9


@
@
@j 
val_11


@
@
@j 
val_13


@
@
@j 
gelu_1


@
@
@j"
conv2d_2


@
@
@j 
val_15


@
@
@j 
val_16


@
@
@j 
val_18


@
@
@j 
val_20


@
@
@j 
gelu_2


@
@
@j"
conv2d_3


@
@
@j!
getitem


@
@
@j#
conv2d_5


¿
@
@j!
val_34


¿
@
@j!
val_35


¿
@
@j!
val_37


¿
@
@j!
val_39


¿
@
@j!
gelu_3


¿
@
@j"
conv2d_6


@
@
@j
mul


@
@
@j
add


@
@
@j"
conv2d_7


@
@
@j#
	getitem_3


@
@
@j#
conv2d_9


¿
@
@j!
val_50


¿
@
@j!
val_51


¿
@
@j!
val_53


¿
@
@j!
val_55


¿
@
@j!
gelu_4


¿
@
@j#
	conv2d_10


@
@
@j
mul_1


@
@
@j
add_1


@
@
@j$
	conv2d_11


Ä
 
 j!
val_57


Ä
 
 j!
val_58


Ä
 
 j!
val_60


Ä
 
 j!
val_62


Ä
 
 j!
gelu_5


Ä
 
 j$
	conv2d_12


Ä
 
 j!
val_64


Ä
 
 j!
val_65


Ä
 
 j!
val_67


Ä
 
 j!
val_69


Ä
 
 j!
gelu_6


Ä
 
 j$
	conv2d_13


Ä
 
 j$
	getitem_6


Ä
 
 j$
	conv2d_15


Ä
 
 j!
val_80


Ä
 
 j!
val_81


Ä
 
 j!
val_83


Ä
 
 j!
val_85


Ä
 
 j!
gelu_7


Ä
 
 j$
	conv2d_16


Ä
 
 j 
mul_2


Ä
 
 j 
add_2


Ä
 
 j$
	conv2d_17


Ä
 
 j$
	getitem_9


Ä
 
 j$
	conv2d_19


Ä
 
 j!
val_96


Ä
 
 j!
val_97


Ä
 
 j!
val_99


Ä
 
 j"
val_101


Ä
 
 j!
gelu_8


Ä
 
 j$
	conv2d_20


Ä
 
 j 
mul_3


Ä
 
 j 
add_3


Ä
 
 j$
	conv2d_21


Ä
 
 j%

getitem_12


Ä
 
 j$
	conv2d_23


Ä
 
 j"
val_112


Ä
 
 j"
val_113


Ä
 
 j"
val_115


Ä
 
 j"
val_117


Ä
 
 j!
gelu_9


Ä
 
 j$
	conv2d_24


Ä
 
 j 
mul_4


Ä
 
 j 
add_4


Ä
 
 j$
	conv2d_25


Ä
 
 j%

getitem_15


Ä
 
 j$
	conv2d_27


Ä
 
 j"
val_128


Ä
 
 j"
val_129


Ä
 
 j"
val_131


Ä
 
 j"
val_133


Ä
 
 j"
gelu_10


Ä
 
 j$
	conv2d_28


Ä
 
 j 
mul_5


Ä
 
 j 
add_5


Ä
 
 j$
	conv2d_29


Ä
 
 j%

getitem_18


Ä
 
 j$
	conv2d_31


Ä
 
 j"
val_144


Ä
 
 j"
val_145


Ä
 
 j"
val_147


Ä
 
 j"
val_149


Ä
 
 j"
gelu_11


Ä
 
 j$
	conv2d_32


Ä
 
 j 
mul_6


Ä
 
 j 
add_6


Ä
 
 j$
	conv2d_33


Ä
 
 j%

getitem_21


Ä
 
 j$
	conv2d_35


Ä
 
 j"
val_160


Ä
 
 j"
val_161


Ä
 
 j"
val_163


Ä
 
 j"
val_165


Ä
 
 j"
gelu_12


Ä
 
 j$
	conv2d_36


Ä
 
 j 
mul_7


Ä
 
 j 
add_7


Ä
 
 j$
	conv2d_37


Ä

j
mean


Ä

j#
	conv2d_38


@

j
relu


@

j$
	conv2d_39


Ä

j"
sigmoid


Ä

j 
mul_8


Ä

j"
val_170


Ä

j"
val_171


Ä

j"
val_173


Ä

j"
val_175


Ä

j"
gelu_13


Ä

j$
	conv2d_40


Ä

j"
val_177


Ä

j"
val_178


Ä

j"
val_180


Ä

j"
val_182


Ä

j"
gelu_14


Ä

j$
	conv2d_41


Ä

j%

getitem_24


Ä

j$
	conv2d_43


Ä

j"
val_193


Ä

j"
val_194


Ä

j"
val_196


Ä

j"
val_198


Ä

j"
gelu_15


Ä

j$
	conv2d_44


Ä

j 
mul_9


Ä

j 
add_8


Ä

j$
	conv2d_45


Ä

j%

getitem_27


Ä

j$
	conv2d_47


Ä

j"
val_209


Ä

j"
val_210


Ä

j"
val_212


Ä

j"
val_214


Ä

j"
gelu_16


Ä

j$
	conv2d_48


Ä

j!
mul_10


Ä

j 
add_9


Ä

j$
	conv2d_49


Ä

j%

getitem_30


Ä

j$
	conv2d_51


Ä

j"
val_225


Ä

j"
val_226


Ä

j"
val_228


Ä

j"
val_230


Ä

j"
gelu_17


Ä

j$
	conv2d_52


Ä

j!
mul_11


Ä

j!
add_10


Ä

j$
	conv2d_53


Ä

j%

getitem_33


Ä

j$
	conv2d_55


Ä

j"
val_241


Ä

j"
val_242


Ä

j"
val_244


Ä

j"
val_246


Ä

j"
gelu_18


Ä

j$
	conv2d_56


Ä

j!
mul_12


Ä

j!
add_11


Ä

j$
	conv2d_57


Ä

j%

getitem_36


Ä

j$
	conv2d_59


Ä

j"
val_257


Ä

j"
val_258


Ä

j"
val_260


Ä

j"
val_262


Ä

j"
gelu_19


Ä

j$
	conv2d_60


Ä

j!
mul_13


Ä

j!
add_12


Ä

j$
	conv2d_61


Ä

j%

getitem_39


Ä

j$
	conv2d_63


Ä

j"
val_273


Ä

j"
val_274


Ä

j"
val_276


Ä

j"
val_278


Ä

j"
gelu_20


Ä

j$
	conv2d_64


Ä

j!
mul_14


Ä

j!
add_13


Ä

j$
	conv2d_65


Ä

j%

getitem_42


Ä

j$
	conv2d_67


Ä

j"
val_289


Ä

j"
val_290


Ä

j"
val_292


Ä

j"
val_294


Ä

j"
gelu_21


Ä

j$
	conv2d_68


Ä

j!
mul_15


Ä

j!
add_14


Ä

j$
	conv2d_69


Ä

j%

getitem_45


Ä

j$
	conv2d_71


Ä

j"
val_305


Ä

j"
val_306


Ä

j"
val_308


Ä

j"
val_310


Ä

j"
gelu_22


Ä

j$
	conv2d_72


Ä

j!
mul_16


Ä

j!
add_15


Ä

j$
	conv2d_73


Ä

j%

getitem_48


Ä

j$
	conv2d_75


Ä

j"
val_321


Ä

j"
val_322


Ä

j"
val_324


Ä

j"
val_326


Ä

j"
gelu_23


Ä

j$
	conv2d_76


Ä

j!
mul_17


Ä

j!
add_16


Ä

j$
	conv2d_77


Ä

j%

getitem_51


Ä

j$
	conv2d_79


Ä

j"
val_337


Ä

j"
val_338


Ä

j"
val_340


Ä

j"
val_342


Ä

j"
gelu_24


Ä

j$
	conv2d_80


Ä

j!
mul_18


Ä

j!
add_17


Ä

j$
	conv2d_81


Ä

j!
mean_1


Ä

j$
	conv2d_82


Ä

j!
relu_1


Ä

j$
	conv2d_83


Ä

j$
	sigmoid_1


Ä

j!
mul_19


Ä

j"
val_346


Ä

j"
val_347


Ä

j"
val_349


Ä

j"
val_351


Ä

j"
gelu_25


Ä

j$
	conv2d_84


Ä

j"
val_353


Ä

j"
val_354


Ä

j"
val_356


Ä

j"
val_358


Ä

j"
gelu_26


Ä

j$
	conv2d_85


Ä

j%

getitem_54


Ä

j
view


Ä
@j 
	transpose


@
Äj
linear


@
Äj$
view_1


@


 j%
permute




@
 j%
val_379




@
 j$

getitem_57



@
 j%
val_384




@
 j$

getitem_58



@
 j%
val_388




@
 j$

getitem_59



@
 j 
mul_20



@
 j%
transpose_1



 
@j 
matmul



@
@j!
softmax



@
@j"
matmul_1



@
 j%
transpose_2


@

 j#
_unsafe_view


@
Äj
val_396


@
Äj
linear_1


@
Äj"
transpose_3


Ä
@j!
view_2


Ä

j!
mul_21


Ä

j!
add_18


Ä

j%

getitem_60


Ä

j$
	conv2d_87


Ä

j"
val_413


Ä

j"
val_414


Ä

j"
val_416


Ä

j"
val_418


Ä

j"
gelu_27


Ä

j$
	conv2d_88


Ä

j!
mul_22


Ä

j!
add_19


Ä

j%

getitem_63


Ä

j
view_3


Ä
@j"
transpose_4


@
Äj
linear_2


@
Äj$
view_4


@


 j'
	permute_1




@
 j%
val_439




@
 j$

getitem_66



@
 j%
val_443




@
 j$

getitem_67



@
 j%
val_447




@
 j$

getitem_68



@
 j 
mul_23



@
 j%
transpose_5



 
@j"
matmul_2



@
@j#
	softmax_1



@
@j"
matmul_3



@
 j%
transpose_6


@

 j%
_unsafe_view_1


@
Äj
val_454


@
Äj
linear_3


@
Äj"
transpose_7


Ä
@j!
view_5


Ä

j!
mul_24


Ä

j!
add_20


Ä

j%

getitem_69


Ä

j$
	conv2d_90


Ä

j"
val_471


Ä

j"
val_472


Ä

j"
val_474


Ä

j"
val_476


Ä

j"
gelu_28


Ä

j$
	conv2d_91


Ä

j!
mul_25


Ä

j!
add_21


Ä

j$
	conv2d_92


Ä

j%

avg_pool2d


Ä

j#
	conv2d_93


@

j 
relu_2


@

j$
	conv2d_94


Ä

j$
	sigmoid_2


Ä

j!
view_6


Ä

j!
mul_26


Ä

j"
val_484


Ä

j"
val_485


Ä

j"
val_487


Ä

j"
val_489


Ä

j"
gelu_29


Ä

j
mean_2
	

Äj
matmul_4
	

Äj$
linalg_vector_norm


Çí®
0pkg.torch.export.ExportedProgram.graph_signature‹ß
# inputs
p_encoder_model_patch_embed_0_reparam_conv_weight: PARAMETER target='encoder.model.patch_embed.0.reparam_conv.weight'
p_encoder_model_patch_embed_0_reparam_conv_bias: PARAMETER target='encoder.model.patch_embed.0.reparam_conv.bias'
p_encoder_model_patch_embed_1_reparam_conv_weight: PARAMETER target='encoder.model.patch_embed.1.reparam_conv.weight'
p_encoder_model_patch_embed_1_reparam_conv_bias: PARAMETER target='encoder.model.patch_embed.1.reparam_conv.bias'
p_encoder_model_patch_embed_2_reparam_conv_weight: PARAMETER target='encoder.model.patch_embed.2.reparam_conv.weight'
p_encoder_model_patch_embed_2_reparam_conv_bias: PARAMETER target='encoder.model.patch_embed.2.reparam_conv.bias'
p_encoder_model_network_0_0_layer_scale: PARAMETER target='encoder.model.network.0.0.layer_scale'
p_encoder_model_network_0_0_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.0.0.token_mixer.reparam_conv.weight'
p_encoder_model_network_0_0_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.0.0.token_mixer.reparam_conv.bias'
p_encoder_model_network_0_0_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.0.0.convffn.conv.conv.weight'
p_encoder_model_network_0_0_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.0.0.convffn.conv.bn.weight'
p_encoder_model_network_0_0_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.0.0.convffn.conv.bn.bias'
p_encoder_model_network_0_0_convffn_fc1_weight: PARAMETER target='encoder.model.network.0.0.convffn.fc1.weight'
p_encoder_model_network_0_0_convffn_fc1_bias: PARAMETER target='encoder.model.network.0.0.convffn.fc1.bias'
p_encoder_model_network_0_0_convffn_fc2_weight: PARAMETER target='encoder.model.network.0.0.convffn.fc2.weight'
p_encoder_model_network_0_0_convffn_fc2_bias: PARAMETER target='encoder.model.network.0.0.convffn.fc2.bias'
p_encoder_model_network_0_1_layer_scale: PARAMETER target='encoder.model.network.0.1.layer_scale'
p_encoder_model_network_0_1_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.0.1.token_mixer.reparam_conv.weight'
p_encoder_model_network_0_1_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.0.1.token_mixer.reparam_conv.bias'
p_encoder_model_network_0_1_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.0.1.convffn.conv.conv.weight'
p_encoder_model_network_0_1_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.0.1.convffn.conv.bn.weight'
p_encoder_model_network_0_1_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.0.1.convffn.conv.bn.bias'
p_encoder_model_network_0_1_convffn_fc1_weight: PARAMETER target='encoder.model.network.0.1.convffn.fc1.weight'
p_encoder_model_network_0_1_convffn_fc1_bias: PARAMETER target='encoder.model.network.0.1.convffn.fc1.bias'
p_encoder_model_network_0_1_convffn_fc2_weight: PARAMETER target='encoder.model.network.0.1.convffn.fc2.weight'
p_encoder_model_network_0_1_convffn_fc2_bias: PARAMETER target='encoder.model.network.0.1.convffn.fc2.bias'
p_encoder_model_network_1_proj_0_lkb_reparam_weight: PARAMETER target='encoder.model.network.1.proj.0.lkb_reparam.weight'
p_encoder_model_network_1_proj_0_lkb_reparam_bias: PARAMETER target='encoder.model.network.1.proj.0.lkb_reparam.bias'
p_encoder_model_network_1_proj_1_reparam_conv_weight: PARAMETER target='encoder.model.network.1.proj.1.reparam_conv.weight'
p_encoder_model_network_1_proj_1_reparam_conv_bias: PARAMETER target='encoder.model.network.1.proj.1.reparam_conv.bias'
p_encoder_model_network_2_0_layer_scale: PARAMETER target='encoder.model.network.2.0.layer_scale'
p_encoder_model_network_2_0_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.2.0.token_mixer.reparam_conv.weight'
p_encoder_model_network_2_0_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.2.0.token_mixer.reparam_conv.bias'
p_encoder_model_network_2_0_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.2.0.convffn.conv.conv.weight'
p_encoder_model_network_2_0_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.2.0.convffn.conv.bn.weight'
p_encoder_model_network_2_0_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.2.0.convffn.conv.bn.bias'
p_encoder_model_network_2_0_convffn_fc1_weight: PARAMETER target='encoder.model.network.2.0.convffn.fc1.weight'
p_encoder_model_network_2_0_convffn_fc1_bias: PARAMETER target='encoder.model.network.2.0.convffn.fc1.bias'
p_encoder_model_network_2_0_convffn_fc2_weight: PARAMETER target='encoder.model.network.2.0.convffn.fc2.weight'
p_encoder_model_network_2_0_convffn_fc2_bias: PARAMETER target='encoder.model.network.2.0.convffn.fc2.bias'
p_encoder_model_network_2_1_layer_scale: PARAMETER target='encoder.model.network.2.1.layer_scale'
p_encoder_model_network_2_1_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.2.1.token_mixer.reparam_conv.weight'
p_encoder_model_network_2_1_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.2.1.token_mixer.reparam_conv.bias'
p_encoder_model_network_2_1_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.2.1.convffn.conv.conv.weight'
p_encoder_model_network_2_1_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.2.1.convffn.conv.bn.weight'
p_encoder_model_network_2_1_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.2.1.convffn.conv.bn.bias'
p_encoder_model_network_2_1_convffn_fc1_weight: PARAMETER target='encoder.model.network.2.1.convffn.fc1.weight'
p_encoder_model_network_2_1_convffn_fc1_bias: PARAMETER target='encoder.model.network.2.1.convffn.fc1.bias'
p_encoder_model_network_2_1_convffn_fc2_weight: PARAMETER target='encoder.model.network.2.1.convffn.fc2.weight'
p_encoder_model_network_2_1_convffn_fc2_bias: PARAMETER target='encoder.model.network.2.1.convffn.fc2.bias'
p_encoder_model_network_2_2_layer_scale: PARAMETER target='encoder.model.network.2.2.layer_scale'
p_encoder_model_network_2_2_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.2.2.token_mixer.reparam_conv.weight'
p_encoder_model_network_2_2_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.2.2.token_mixer.reparam_conv.bias'
p_encoder_model_network_2_2_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.2.2.convffn.conv.conv.weight'
p_encoder_model_network_2_2_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.2.2.convffn.conv.bn.weight'
p_encoder_model_network_2_2_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.2.2.convffn.conv.bn.bias'
p_encoder_model_network_2_2_convffn_fc1_weight: PARAMETER target='encoder.model.network.2.2.convffn.fc1.weight'
p_encoder_model_network_2_2_convffn_fc1_bias: PARAMETER target='encoder.model.network.2.2.convffn.fc1.bias'
p_encoder_model_network_2_2_convffn_fc2_weight: PARAMETER target='encoder.model.network.2.2.convffn.fc2.weight'
p_encoder_model_network_2_2_convffn_fc2_bias: PARAMETER target='encoder.model.network.2.2.convffn.fc2.bias'
p_encoder_model_network_2_3_layer_scale: PARAMETER target='encoder.model.network.2.3.layer_scale'
p_encoder_model_network_2_3_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.2.3.token_mixer.reparam_conv.weight'
p_encoder_model_network_2_3_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.2.3.token_mixer.reparam_conv.bias'
p_encoder_model_network_2_3_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.2.3.convffn.conv.conv.weight'
p_encoder_model_network_2_3_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.2.3.convffn.conv.bn.weight'
p_encoder_model_network_2_3_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.2.3.convffn.conv.bn.bias'
p_encoder_model_network_2_3_convffn_fc1_weight: PARAMETER target='encoder.model.network.2.3.convffn.fc1.weight'
p_encoder_model_network_2_3_convffn_fc1_bias: PARAMETER target='encoder.model.network.2.3.convffn.fc1.bias'
p_encoder_model_network_2_3_convffn_fc2_weight: PARAMETER target='encoder.model.network.2.3.convffn.fc2.weight'
p_encoder_model_network_2_3_convffn_fc2_bias: PARAMETER target='encoder.model.network.2.3.convffn.fc2.bias'
p_encoder_model_network_2_4_layer_scale: PARAMETER target='encoder.model.network.2.4.layer_scale'
p_encoder_model_network_2_4_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.2.4.token_mixer.reparam_conv.weight'
p_encoder_model_network_2_4_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.2.4.token_mixer.reparam_conv.bias'
p_encoder_model_network_2_4_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.2.4.convffn.conv.conv.weight'
p_encoder_model_network_2_4_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.2.4.convffn.conv.bn.weight'
p_encoder_model_network_2_4_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.2.4.convffn.conv.bn.bias'
p_encoder_model_network_2_4_convffn_fc1_weight: PARAMETER target='encoder.model.network.2.4.convffn.fc1.weight'
p_encoder_model_network_2_4_convffn_fc1_bias: PARAMETER target='encoder.model.network.2.4.convffn.fc1.bias'
p_encoder_model_network_2_4_convffn_fc2_weight: PARAMETER target='encoder.model.network.2.4.convffn.fc2.weight'
p_encoder_model_network_2_4_convffn_fc2_bias: PARAMETER target='encoder.model.network.2.4.convffn.fc2.bias'
p_encoder_model_network_2_5_layer_scale: PARAMETER target='encoder.model.network.2.5.layer_scale'
p_encoder_model_network_2_5_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.2.5.token_mixer.reparam_conv.weight'
p_encoder_model_network_2_5_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.2.5.token_mixer.reparam_conv.bias'
p_encoder_model_network_2_5_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.2.5.convffn.conv.conv.weight'
p_encoder_model_network_2_5_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.2.5.convffn.conv.bn.weight'
p_encoder_model_network_2_5_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.2.5.convffn.conv.bn.bias'
p_encoder_model_network_2_5_convffn_fc1_weight: PARAMETER target='encoder.model.network.2.5.convffn.fc1.weight'
p_encoder_model_network_2_5_convffn_fc1_bias: PARAMETER target='encoder.model.network.2.5.convffn.fc1.bias'
p_encoder_model_network_2_5_convffn_fc2_weight: PARAMETER target='encoder.model.network.2.5.convffn.fc2.weight'
p_encoder_model_network_2_5_convffn_fc2_bias: PARAMETER target='encoder.model.network.2.5.convffn.fc2.bias'
p_encoder_model_network_3_proj_0_se_fc1_weight: PARAMETER target='encoder.model.network.3.proj.0.se.fc1.weight'
p_encoder_model_network_3_proj_0_se_fc1_bias: PARAMETER target='encoder.model.network.3.proj.0.se.fc1.bias'
p_encoder_model_network_3_proj_0_se_fc2_weight: PARAMETER target='encoder.model.network.3.proj.0.se.fc2.weight'
p_encoder_model_network_3_proj_0_se_fc2_bias: PARAMETER target='encoder.model.network.3.proj.0.se.fc2.bias'
p_encoder_model_network_3_proj_0_lkb_reparam_weight: PARAMETER target='encoder.model.network.3.proj.0.lkb_reparam.weight'
p_encoder_model_network_3_proj_0_lkb_reparam_bias: PARAMETER target='encoder.model.network.3.proj.0.lkb_reparam.bias'
p_encoder_model_network_3_proj_1_reparam_conv_weight: PARAMETER target='encoder.model.network.3.proj.1.reparam_conv.weight'
p_encoder_model_network_3_proj_1_reparam_conv_bias: PARAMETER target='encoder.model.network.3.proj.1.reparam_conv.bias'
p_encoder_model_network_4_0_layer_scale: PARAMETER target='encoder.model.network.4.0.layer_scale'
p_encoder_model_network_4_0_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.4.0.token_mixer.reparam_conv.weight'
p_encoder_model_network_4_0_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.4.0.token_mixer.reparam_conv.bias'
p_encoder_model_network_4_0_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.4.0.convffn.conv.conv.weight'
p_encoder_model_network_4_0_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.4.0.convffn.conv.bn.weight'
p_encoder_model_network_4_0_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.4.0.convffn.conv.bn.bias'
p_encoder_model_network_4_0_convffn_fc1_weight: PARAMETER target='encoder.model.network.4.0.convffn.fc1.weight'
p_encoder_model_network_4_0_convffn_fc1_bias: PARAMETER target='encoder.model.network.4.0.convffn.fc1.bias'
p_encoder_model_network_4_0_convffn_fc2_weight: PARAMETER target='encoder.model.network.4.0.convffn.fc2.weight'
p_encoder_model_network_4_0_convffn_fc2_bias: PARAMETER target='encoder.model.network.4.0.convffn.fc2.bias'
p_encoder_model_network_4_1_layer_scale: PARAMETER target='encoder.model.network.4.1.layer_scale'
p_encoder_model_network_4_1_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.4.1.token_mixer.reparam_conv.weight'
p_encoder_model_network_4_1_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.4.1.token_mixer.reparam_conv.bias'
p_encoder_model_network_4_1_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.4.1.convffn.conv.conv.weight'
p_encoder_model_network_4_1_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.4.1.convffn.conv.bn.weight'
p_encoder_model_network_4_1_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.4.1.convffn.conv.bn.bias'
p_encoder_model_network_4_1_convffn_fc1_weight: PARAMETER target='encoder.model.network.4.1.convffn.fc1.weight'
p_encoder_model_network_4_1_convffn_fc1_bias: PARAMETER target='encoder.model.network.4.1.convffn.fc1.bias'
p_encoder_model_network_4_1_convffn_fc2_weight: PARAMETER target='encoder.model.network.4.1.convffn.fc2.weight'
p_encoder_model_network_4_1_convffn_fc2_bias: PARAMETER target='encoder.model.network.4.1.convffn.fc2.bias'
p_encoder_model_network_4_2_layer_scale: PARAMETER target='encoder.model.network.4.2.layer_scale'
p_encoder_model_network_4_2_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.4.2.token_mixer.reparam_conv.weight'
p_encoder_model_network_4_2_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.4.2.token_mixer.reparam_conv.bias'
p_encoder_model_network_4_2_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.4.2.convffn.conv.conv.weight'
p_encoder_model_network_4_2_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.4.2.convffn.conv.bn.weight'
p_encoder_model_network_4_2_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.4.2.convffn.conv.bn.bias'
p_encoder_model_network_4_2_convffn_fc1_weight: PARAMETER target='encoder.model.network.4.2.convffn.fc1.weight'
p_encoder_model_network_4_2_convffn_fc1_bias: PARAMETER target='encoder.model.network.4.2.convffn.fc1.bias'
p_encoder_model_network_4_2_convffn_fc2_weight: PARAMETER target='encoder.model.network.4.2.convffn.fc2.weight'
p_encoder_model_network_4_2_convffn_fc2_bias: PARAMETER target='encoder.model.network.4.2.convffn.fc2.bias'
p_encoder_model_network_4_3_layer_scale: PARAMETER target='encoder.model.network.4.3.layer_scale'
p_encoder_model_network_4_3_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.4.3.token_mixer.reparam_conv.weight'
p_encoder_model_network_4_3_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.4.3.token_mixer.reparam_conv.bias'
p_encoder_model_network_4_3_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.4.3.convffn.conv.conv.weight'
p_encoder_model_network_4_3_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.4.3.convffn.conv.bn.weight'
p_encoder_model_network_4_3_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.4.3.convffn.conv.bn.bias'
p_encoder_model_network_4_3_convffn_fc1_weight: PARAMETER target='encoder.model.network.4.3.convffn.fc1.weight'
p_encoder_model_network_4_3_convffn_fc1_bias: PARAMETER target='encoder.model.network.4.3.convffn.fc1.bias'
p_encoder_model_network_4_3_convffn_fc2_weight: PARAMETER target='encoder.model.network.4.3.convffn.fc2.weight'
p_encoder_model_network_4_3_convffn_fc2_bias: PARAMETER target='encoder.model.network.4.3.convffn.fc2.bias'
p_encoder_model_network_4_4_layer_scale: PARAMETER target='encoder.model.network.4.4.layer_scale'
p_encoder_model_network_4_4_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.4.4.token_mixer.reparam_conv.weight'
p_encoder_model_network_4_4_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.4.4.token_mixer.reparam_conv.bias'
p_encoder_model_network_4_4_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.4.4.convffn.conv.conv.weight'
p_encoder_model_network_4_4_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.4.4.convffn.conv.bn.weight'
p_encoder_model_network_4_4_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.4.4.convffn.conv.bn.bias'
p_encoder_model_network_4_4_convffn_fc1_weight: PARAMETER target='encoder.model.network.4.4.convffn.fc1.weight'
p_encoder_model_network_4_4_convffn_fc1_bias: PARAMETER target='encoder.model.network.4.4.convffn.fc1.bias'
p_encoder_model_network_4_4_convffn_fc2_weight: PARAMETER target='encoder.model.network.4.4.convffn.fc2.weight'
p_encoder_model_network_4_4_convffn_fc2_bias: PARAMETER target='encoder.model.network.4.4.convffn.fc2.bias'
p_encoder_model_network_4_5_layer_scale: PARAMETER target='encoder.model.network.4.5.layer_scale'
p_encoder_model_network_4_5_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.4.5.token_mixer.reparam_conv.weight'
p_encoder_model_network_4_5_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.4.5.token_mixer.reparam_conv.bias'
p_encoder_model_network_4_5_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.4.5.convffn.conv.conv.weight'
p_encoder_model_network_4_5_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.4.5.convffn.conv.bn.weight'
p_encoder_model_network_4_5_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.4.5.convffn.conv.bn.bias'
p_encoder_model_network_4_5_convffn_fc1_weight: PARAMETER target='encoder.model.network.4.5.convffn.fc1.weight'
p_encoder_model_network_4_5_convffn_fc1_bias: PARAMETER target='encoder.model.network.4.5.convffn.fc1.bias'
p_encoder_model_network_4_5_convffn_fc2_weight: PARAMETER target='encoder.model.network.4.5.convffn.fc2.weight'
p_encoder_model_network_4_5_convffn_fc2_bias: PARAMETER target='encoder.model.network.4.5.convffn.fc2.bias'
p_encoder_model_network_4_6_layer_scale: PARAMETER target='encoder.model.network.4.6.layer_scale'
p_encoder_model_network_4_6_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.4.6.token_mixer.reparam_conv.weight'
p_encoder_model_network_4_6_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.4.6.token_mixer.reparam_conv.bias'
p_encoder_model_network_4_6_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.4.6.convffn.conv.conv.weight'
p_encoder_model_network_4_6_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.4.6.convffn.conv.bn.weight'
p_encoder_model_network_4_6_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.4.6.convffn.conv.bn.bias'
p_encoder_model_network_4_6_convffn_fc1_weight: PARAMETER target='encoder.model.network.4.6.convffn.fc1.weight'
p_encoder_model_network_4_6_convffn_fc1_bias: PARAMETER target='encoder.model.network.4.6.convffn.fc1.bias'
p_encoder_model_network_4_6_convffn_fc2_weight: PARAMETER target='encoder.model.network.4.6.convffn.fc2.weight'
p_encoder_model_network_4_6_convffn_fc2_bias: PARAMETER target='encoder.model.network.4.6.convffn.fc2.bias'
p_encoder_model_network_4_7_layer_scale: PARAMETER target='encoder.model.network.4.7.layer_scale'
p_encoder_model_network_4_7_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.4.7.token_mixer.reparam_conv.weight'
p_encoder_model_network_4_7_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.4.7.token_mixer.reparam_conv.bias'
p_encoder_model_network_4_7_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.4.7.convffn.conv.conv.weight'
p_encoder_model_network_4_7_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.4.7.convffn.conv.bn.weight'
p_encoder_model_network_4_7_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.4.7.convffn.conv.bn.bias'
p_encoder_model_network_4_7_convffn_fc1_weight: PARAMETER target='encoder.model.network.4.7.convffn.fc1.weight'
p_encoder_model_network_4_7_convffn_fc1_bias: PARAMETER target='encoder.model.network.4.7.convffn.fc1.bias'
p_encoder_model_network_4_7_convffn_fc2_weight: PARAMETER target='encoder.model.network.4.7.convffn.fc2.weight'
p_encoder_model_network_4_7_convffn_fc2_bias: PARAMETER target='encoder.model.network.4.7.convffn.fc2.bias'
p_encoder_model_network_4_8_layer_scale: PARAMETER target='encoder.model.network.4.8.layer_scale'
p_encoder_model_network_4_8_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.4.8.token_mixer.reparam_conv.weight'
p_encoder_model_network_4_8_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.4.8.token_mixer.reparam_conv.bias'
p_encoder_model_network_4_8_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.4.8.convffn.conv.conv.weight'
p_encoder_model_network_4_8_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.4.8.convffn.conv.bn.weight'
p_encoder_model_network_4_8_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.4.8.convffn.conv.bn.bias'
p_encoder_model_network_4_8_convffn_fc1_weight: PARAMETER target='encoder.model.network.4.8.convffn.fc1.weight'
p_encoder_model_network_4_8_convffn_fc1_bias: PARAMETER target='encoder.model.network.4.8.convffn.fc1.bias'
p_encoder_model_network_4_8_convffn_fc2_weight: PARAMETER target='encoder.model.network.4.8.convffn.fc2.weight'
p_encoder_model_network_4_8_convffn_fc2_bias: PARAMETER target='encoder.model.network.4.8.convffn.fc2.bias'
p_encoder_model_network_4_9_layer_scale: PARAMETER target='encoder.model.network.4.9.layer_scale'
p_encoder_model_network_4_9_token_mixer_reparam_conv_weight: PARAMETER target='encoder.model.network.4.9.token_mixer.reparam_conv.weight'
p_encoder_model_network_4_9_token_mixer_reparam_conv_bias: PARAMETER target='encoder.model.network.4.9.token_mixer.reparam_conv.bias'
p_encoder_model_network_4_9_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.4.9.convffn.conv.conv.weight'
p_encoder_model_network_4_9_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.4.9.convffn.conv.bn.weight'
p_encoder_model_network_4_9_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.4.9.convffn.conv.bn.bias'
p_encoder_model_network_4_9_convffn_fc1_weight: PARAMETER target='encoder.model.network.4.9.convffn.fc1.weight'
p_encoder_model_network_4_9_convffn_fc1_bias: PARAMETER target='encoder.model.network.4.9.convffn.fc1.bias'
p_encoder_model_network_4_9_convffn_fc2_weight: PARAMETER target='encoder.model.network.4.9.convffn.fc2.weight'
p_encoder_model_network_4_9_convffn_fc2_bias: PARAMETER target='encoder.model.network.4.9.convffn.fc2.bias'
p_encoder_model_network_5_proj_0_se_fc1_weight: PARAMETER target='encoder.model.network.5.proj.0.se.fc1.weight'
p_encoder_model_network_5_proj_0_se_fc1_bias: PARAMETER target='encoder.model.network.5.proj.0.se.fc1.bias'
p_encoder_model_network_5_proj_0_se_fc2_weight: PARAMETER target='encoder.model.network.5.proj.0.se.fc2.weight'
p_encoder_model_network_5_proj_0_se_fc2_bias: PARAMETER target='encoder.model.network.5.proj.0.se.fc2.bias'
p_encoder_model_network_5_proj_0_lkb_reparam_weight: PARAMETER target='encoder.model.network.5.proj.0.lkb_reparam.weight'
p_encoder_model_network_5_proj_0_lkb_reparam_bias: PARAMETER target='encoder.model.network.5.proj.0.lkb_reparam.bias'
p_encoder_model_network_5_proj_1_reparam_conv_weight: PARAMETER target='encoder.model.network.5.proj.1.reparam_conv.weight'
p_encoder_model_network_5_proj_1_reparam_conv_bias: PARAMETER target='encoder.model.network.5.proj.1.reparam_conv.bias'
p_encoder_model_network_6_reparam_conv_weight: PARAMETER target='encoder.model.network.6.reparam_conv.weight'
p_encoder_model_network_6_reparam_conv_bias: PARAMETER target='encoder.model.network.6.reparam_conv.bias'
p_encoder_model_network_7_0_layer_scale_1: PARAMETER target='encoder.model.network.7.0.layer_scale_1'
p_encoder_model_network_7_0_layer_scale_2: PARAMETER target='encoder.model.network.7.0.layer_scale_2'
p_encoder_model_network_7_0_norm_weight: PARAMETER target='encoder.model.network.7.0.norm.weight'
p_encoder_model_network_7_0_norm_bias: PARAMETER target='encoder.model.network.7.0.norm.bias'
p_encoder_model_network_7_0_token_mixer_qkv_weight: PARAMETER target='encoder.model.network.7.0.token_mixer.qkv.weight'
p_encoder_model_network_7_0_token_mixer_proj_weight: PARAMETER target='encoder.model.network.7.0.token_mixer.proj.weight'
p_encoder_model_network_7_0_token_mixer_proj_bias: PARAMETER target='encoder.model.network.7.0.token_mixer.proj.bias'
p_encoder_model_network_7_0_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.7.0.convffn.conv.conv.weight'
p_encoder_model_network_7_0_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.7.0.convffn.conv.bn.weight'
p_encoder_model_network_7_0_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.7.0.convffn.conv.bn.bias'
p_encoder_model_network_7_0_convffn_fc1_weight: PARAMETER target='encoder.model.network.7.0.convffn.fc1.weight'
p_encoder_model_network_7_0_convffn_fc1_bias: PARAMETER target='encoder.model.network.7.0.convffn.fc1.bias'
p_encoder_model_network_7_0_convffn_fc2_weight: PARAMETER target='encoder.model.network.7.0.convffn.fc2.weight'
p_encoder_model_network_7_0_convffn_fc2_bias: PARAMETER target='encoder.model.network.7.0.convffn.fc2.bias'
p_encoder_model_network_7_1_layer_scale_1: PARAMETER target='encoder.model.network.7.1.layer_scale_1'
p_encoder_model_network_7_1_layer_scale_2: PARAMETER target='encoder.model.network.7.1.layer_scale_2'
p_encoder_model_network_7_1_norm_weight: PARAMETER target='encoder.model.network.7.1.norm.weight'
p_encoder_model_network_7_1_norm_bias: PARAMETER target='encoder.model.network.7.1.norm.bias'
p_encoder_model_network_7_1_token_mixer_qkv_weight: PARAMETER target='encoder.model.network.7.1.token_mixer.qkv.weight'
p_encoder_model_network_7_1_token_mixer_proj_weight: PARAMETER target='encoder.model.network.7.1.token_mixer.proj.weight'
p_encoder_model_network_7_1_token_mixer_proj_bias: PARAMETER target='encoder.model.network.7.1.token_mixer.proj.bias'
p_encoder_model_network_7_1_convffn_conv_conv_weight: PARAMETER target='encoder.model.network.7.1.convffn.conv.conv.weight'
p_encoder_model_network_7_1_convffn_conv_bn_weight: PARAMETER target='encoder.model.network.7.1.convffn.conv.bn.weight'
p_encoder_model_network_7_1_convffn_conv_bn_bias: PARAMETER target='encoder.model.network.7.1.convffn.conv.bn.bias'
p_encoder_model_network_7_1_convffn_fc1_weight: PARAMETER target='encoder.model.network.7.1.convffn.fc1.weight'
p_encoder_model_network_7_1_convffn_fc1_bias: PARAMETER target='encoder.model.network.7.1.convffn.fc1.bias'
p_encoder_model_network_7_1_convffn_fc2_weight: PARAMETER target='encoder.model.network.7.1.convffn.fc2.weight'
p_encoder_model_network_7_1_convffn_fc2_bias: PARAMETER target='encoder.model.network.7.1.convffn.fc2.bias'
p_encoder_model_conv_exp_se_reduce_weight: PARAMETER target='encoder.model.conv_exp.se.reduce.weight'
p_encoder_model_conv_exp_se_reduce_bias: PARAMETER target='encoder.model.conv_exp.se.reduce.bias'
p_encoder_model_conv_exp_se_expand_weight: PARAMETER target='encoder.model.conv_exp.se.expand.weight'
p_encoder_model_conv_exp_se_expand_bias: PARAMETER target='encoder.model.conv_exp.se.expand.bias'
p_encoder_model_conv_exp_reparam_conv_weight: PARAMETER target='encoder.model.conv_exp.reparam_conv.weight'
p_encoder_model_conv_exp_reparam_conv_bias: PARAMETER target='encoder.model.conv_exp.reparam_conv.bias'
p_encoder_model_head_proj: PARAMETER target='encoder.model.head.proj'
b_encoder_model_network_0_0_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.0.0.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_0_0_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.0.0.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_0_0_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.0.0.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_0_1_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.0.1.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_0_1_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.0.1.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_0_1_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.0.1.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_2_0_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.2.0.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_2_0_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.2.0.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_2_0_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.2.0.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_2_1_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.2.1.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_2_1_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.2.1.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_2_1_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.2.1.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_2_2_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.2.2.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_2_2_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.2.2.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_2_2_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.2.2.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_2_3_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.2.3.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_2_3_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.2.3.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_2_3_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.2.3.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_2_4_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.2.4.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_2_4_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.2.4.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_2_4_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.2.4.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_2_5_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.2.5.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_2_5_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.2.5.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_2_5_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.2.5.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_4_0_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.4.0.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_4_0_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.4.0.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_4_0_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.4.0.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_4_1_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.4.1.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_4_1_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.4.1.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_4_1_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.4.1.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_4_2_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.4.2.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_4_2_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.4.2.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_4_2_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.4.2.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_4_3_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.4.3.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_4_3_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.4.3.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_4_3_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.4.3.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_4_4_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.4.4.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_4_4_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.4.4.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_4_4_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.4.4.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_4_5_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.4.5.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_4_5_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.4.5.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_4_5_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.4.5.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_4_6_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.4.6.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_4_6_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.4.6.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_4_6_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.4.6.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_4_7_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.4.7.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_4_7_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.4.7.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_4_7_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.4.7.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_4_8_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.4.8.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_4_8_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.4.8.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_4_8_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.4.8.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_4_9_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.4.9.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_4_9_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.4.9.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_4_9_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.4.9.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_7_0_norm_running_mean: BUFFER target='encoder.model.network.7.0.norm.running_mean' persistent=True
b_encoder_model_network_7_0_norm_running_var: BUFFER target='encoder.model.network.7.0.norm.running_var' persistent=True
b_encoder_model_network_7_0_norm_num_batches_tracked: BUFFER target='encoder.model.network.7.0.norm.num_batches_tracked' persistent=True
b_encoder_model_network_7_0_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.7.0.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_7_0_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.7.0.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_7_0_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.7.0.convffn.conv.bn.num_batches_tracked' persistent=True
b_encoder_model_network_7_1_norm_running_mean: BUFFER target='encoder.model.network.7.1.norm.running_mean' persistent=True
b_encoder_model_network_7_1_norm_running_var: BUFFER target='encoder.model.network.7.1.norm.running_var' persistent=True
b_encoder_model_network_7_1_norm_num_batches_tracked: BUFFER target='encoder.model.network.7.1.norm.num_batches_tracked' persistent=True
b_encoder_model_network_7_1_convffn_conv_bn_running_mean: BUFFER target='encoder.model.network.7.1.convffn.conv.bn.running_mean' persistent=True
b_encoder_model_network_7_1_convffn_conv_bn_running_var: BUFFER target='encoder.model.network.7.1.convffn.conv.bn.running_var' persistent=True
b_encoder_model_network_7_1_convffn_conv_bn_num_batches_tracked: BUFFER target='encoder.model.network.7.1.convffn.conv.bn.num_batches_tracked' persistent=True
x: USER_INPUT

# outputs
div: USER_OUTPUT
Ç8
2pkg.torch.export.ExportedProgram.range_constraints{}B
 